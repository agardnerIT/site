{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"about/","title":"About Me","text":"<p>Travelling and scuba diving are my passions.</p> <p>I'm currently based in Brisbane, Australia and work for Dynatrace as a DevRel focussing on Observability.</p>"},{"location":"about/#non-it-certifications","title":"Non IT Certifications","text":"<ul> <li>PADI Enriched Air (Nitrox) Diver (09/12/18)</li> <li>PADI Rescue Diver (25/11/2018)</li> <li>PADI Advanced Open Water (21/07/2013)</li> <li>PADI Open Water (09/11/2005)</li> </ul>"},{"location":"about/#it-certifications","title":"IT Certifications","text":"<ul> <li>Dynatrace Professional Certification</li> <li>Dynatrace Associate Certification</li> <li>Puppet for the Absolute Beginner</li> <li>Red Hat Accredited Professional \u2013 Delivery Specialist \u2013 Cloud Automation</li> <li>Complete Python Masterclass</li> <li>Mastering Ansible</li> <li>Complete Elasticsearch Masterclass with Kibana and Logstash</li> <li>Learn DevOps: Infrastructure Automation with Terraform</li> <li>Learn DevOps: The Complete Kubernetes Course</li> <li>AWS Certified SysOps Administrator</li> <li>Docker for DevOps and Developers</li> </ul>"},{"location":"about/#disclaimer","title":"Disclaimer","text":"<p>All views, advice (and bugs) dispensed on this site are entirely my own. None of the content on this site has been officially sanctioned by any of the companies mentioned. You use this content entirely at your own risk.</p> <p>If you have an idea for a tutorial or walkthrough, please contact me and I\u2019ll do my best to accommodate you.</p>"},{"location":"contact/","title":"How to Contact Me","text":"<p>Connect with me on LinkedIn, via a comment on one of my videos or email: <code>agardnerit@fastmail.com</code></p>"},{"location":"ansible-101-basics/","title":"Ansible 101 - The Basics","text":"<p>Ansible is an automation and configuration management system. It is incredibly simple to get going and extremely powerful once mastered. The first part in the series aims to get you up and running with Ansible...</p>"},{"location":"ansible-101-basics/#overview","title":"Overview","text":"<p>This is part one of the series, in this post I\u2019ll outline what Ansible is, get Ansible installed, get you setup and run the first few commands to give you a taste of how powerful Ansible can be.</p> <p>Ansible runs on a <code>control</code> machine which is where we\u2019ll write and store all our Ansible code. It\u2019s also where we\u2019ll trigger all the Ansible executions. The <code>target machine(s)</code> do not need anything installed on them.</p> <p>The control machine must be linux, unix or MacOS. Not Windows.</p> <p>I\u2019ll be using MacOS for my control machine but you can easily use Ubuntu Desktop running in a VM (I recommend VirtualBox).</p> <p>STEP 1: SETUP TARGET MACHINE</p> <p>The Ansible control machine communicates with the target machines via SSH. So spin up a VM and make sure you can ping from your host to your target machine.</p> <p>My target IP is <code>192.168.43.237</code> and I can successfully ping it from my host:</p> <pre><code>adam@ubuntu $ ifconfig\n    inet 192.168.43.237...\n</code></pre> <p>Note: My <code>control</code> machine username is <code>adamgardner</code>. My <code>target</code> machine username is <code>adam</code>.</p> <p>Next, create an SSH key and copy it to the target machine:</p> <pre><code># Generate SSH key. Use all default values. I chose not to password protect it\nssh-keygen -t rsa -b 4096\n\n# Copy the SSH key to the target machine. Repeat if you have multiple targets\nssh-copy-id TARGET-MACHINE-ID \n</code></pre> <p>Now ensure you can SSH from the control to the target machine:</p> <pre><code>ssh 192.168.43.237 -l adam\n...\nAdams-MBP: - adamgardner $ ssh 192.168.43.237 -l adam\n...\nWelcome to Ubuntu 18.04...\nadam@ubuntu $\n</code></pre>"},{"location":"ansible-101-basics/#step-2-install-ansible","title":"Step 2: Install Ansible","text":"<p>Now that we have the groundwork covered, it\u2019s time to install Ansible and run our first command to let Ansible ping our node(s).</p> <p>Installing Ansible varies depending on what OS your control machine is running. Since I recommended Ubuntu, here are the Ubuntu and MacOS instructions:</p> <p>Ubuntu</p> <pre><code>sudo apt-get update -y\nsudo apt-get install software-properties-common -y\nsudo apt-add-repository ppa:ansible/ansible -y\nsudo apt-get update -y\nsudo apt-get install ansible -y\n</code></pre> <p>MacOS</p> <pre><code>sudo easy_install pip\nsudo pip install ansible\n</code></pre> <p>Now double check that Ansible has installed correctly by retrieving it\u2019s version number:</p> <pre><code>ansible --version\n</code></pre>"},{"location":"ansible-101-basics/#tell-control-machine-about-targets","title":"Tell Control Machine About Target(s)","text":"<p>Amsible uses a push methodology. This means that commands are propogated out from the control machine to each target machine. Therefore it makes sense that the control machine must know the location of each target machine.</p> <p>Ansible uses a <code>hosts</code> file to achieve this. The hostname or IP of each target is listed in this special file. This hosts file needs to live at <code>/etc/ansible/hosts</code> so first, ensure the folder structure exists and if needed, create that file.</p> <p>Let\u2019s add our target machine (IP: <code>192.168.43.237</code>) to the hosts file.</p> <pre><code>sudo nano /etc/ansible/hosts\n</code></pre> <p></p> <p>Note the two additional parameters: <code>ansible_user</code> denotes the username I wish to use when connecting to this target host (recall that the username of my VM was <code>adam</code>).</p> <p>I\u2019ve included <code>ansible_python_intepreter</code> because Ansible runs the python interpreter at <code>/usr/bin/python</code> but my setup has python 3 installed which lives at <code>/usr/bin/python3</code>.</p> <p>Now that we\u2019ve defined our target(s), we can use the Ansible <code>ping</code> module to ping all target nodes:</p> <pre><code>ansible -m ping all\n</code></pre> <p>Let\u2019s do something more useful. Let\u2019s create a file on each target host. We\u2019ll use the file module and pass a couple of arguments. Namely, the <code>path</code> of the file to create and the desired <code>state</code> of the file.</p> <pre><code>ansible -m file --args 'path=/tmp/test.txt state=touch'\n</code></pre>"},{"location":"ansible-101-basics/#conclusion","title":"Conclusion","text":"<p>Congratulations. If you\u2019ve made it this far, you\u2019re should already be starting to see the potential Ansible gives for automation, repeatability and easy configuration management. Part two of the series will delve deeper into Ansible commands with some real-world demos and introduce the concept of Playbooks and Idempotence.</p>"},{"location":"ansible-101-playbooks/","title":"Ansible 101 - Playbooks","text":"<p>In the previous tutorial we covered Ansible setup and some basic ad-hoc commands. It\u2019s time to get organised and introduce the ability to easily repeat ourselves consistently. This tutorial will focus on playbooks...</p>"},{"location":"ansible-101-playbooks/#overview","title":"Overview","text":"<p>Ansible playbooks are a way to define orchestration, configuration management and settings in an easy to read format. This also means that they can be easily stored in a version control system (such as Git) or shared amongst coworkers.</p>"},{"location":"ansible-101-playbooks/#an-example-playbook","title":"An Example Playbook","text":"<p>AN EXAMPLE PLAYBOOK</p> <p>First, assume that our <code>/etc/ansible/hosts</code> file looks like this:</p> <p><pre><code>[webservers]\n10.0.0.2\n10.0.0.3\n\n[payment-service]\n10.0.0.4\n10.0.0.5\n10.0.0.6\n\n[database]\n10.0.0.7\n10.0.0.8\n</code></pre> See if you can guess what the following playbook does and to which machines. Ansible really is that simple. Scroll down for the full explanation in case you\u2019re stuck.</p> <pre><code>---\n- hosts: webservers\n  become: true\n\n  tasks:\n\n  - name: Install apache2 webserver\n    package:\n      name: apache2\n      state: latest\n\n  - name: Ensure apache2 is started\n    service:\n      name: apache2\n      state: started\n</code></pre> <p>In case you hadn\u2019t guessed, let\u2019s walk through that playbook and explain:</p> <ul> <li>A playbook executes top down, in order.</li> <li>We specify the hosts that will be affected by this playbook. Our webservers group (<code>10.0.0.2</code> and <code>10.0.0.3</code>).</li> <li>We specify that ansible can <code>become</code> another user. By default this means that the script will <code>sudo</code> to \"become\" <code>root</code>.</li> <li>We specify a task. The <code>name</code> of the task is purely for us humans. The first task uses the <code>package</code> module to install latest available version of the <code>apache2</code> module.</li> <li>The next task uses the <code>service</code> module to ensure that <code>apache2</code> is started.</li> </ul> <p>Notice something too, we installed <code>apache2</code> to two webservers with a single playbook. Even if our infrastructure scaled up to hundreds of thousands of webservers \u2013 all we\u2019d need to do is add new IP addresses to our host file. Simple!</p>"},{"location":"ansible-101-playbooks/#indentation-idempotence","title":"Indentation &amp; Idempotence","text":"<p>Indentation is extremely important to ansible because it uses the YAML format. Check your playbooks by adding the <code>--syntax-check</code> flag (eg. <code>ansible-playbook ~/myPlaybook.yml --syntax-check</code>)</p> <p>Idempotence is a key concept in Ansible. If something is idempotent it means the same operation can be executed multiple times without (necessarily) changing the result.</p> <p>What this means in terms of Ansible is that the same playbook should be able to be executed multiple times without necessarily affecting the result.</p> <p>Think about that for a second. It\u2019s extremely powerful. It means you can write a playbook to define the state of your infrastructure then run it continuously (say in a <code>cron</code> job). Ansible will ensure that your infrastructure is always in your desired state. If anything changes, Ansible will fix it for you.</p> <p>Take the example above. We could run that playbook every 30 seconds. In essence, Ansible first checks whether <code>apache2</code> is installed. If it is, Ansible knows not to do anything. Ansible then checks whether <code>apache2</code> is running. If it is, Ansible does nothing. In other words, if everything is running as it should, nothing will happen (as, by definition, nothing needs to happen). After all, you wouldn\u2019t want your web servers being re-installed and restarted every 30 seconds, would you!?</p>"},{"location":"ansible-101-vault/","title":"Ansible 101 - Vault","text":"<p>Ansible Vault is an out-of-the-box encryption mechanism. Use it to store (encrypted) sensitive data for use within playbooks. This tutorial will get you up-and-running with Ansible Vault in under 10 minutes.</p>"},{"location":"ansible-101-vault/#prerequisites","title":"Prerequisites","text":"<p>If you haven\u2019t already got hands-on experience with Ansible, I highly suggest you follow my previous tutorials:</p> <ul> <li>Ansible 101: Ansible Basics</li> <li>Ansible 101: Ansible Playbooks</li> </ul> <p>Needless to say, you\u2019ll need Ansible installed on a <code>control</code> machine. All commands in this article are to be executed on the control machine.</p> <p>Note: For this tutorial, we\u2019ll be running the playbook against the <code>localhost</code> meaning all commands will be executed on and targeted at, the <code>control</code> machine. The files will be copied to directories on the control machine.</p>"},{"location":"ansible-101-vault/#scenario","title":"Scenario","text":"<p>Imagine we want to write a configuration file to a remote host. This configuration file contains sensitive information that should be held not held in plain text. The variable name is <code>foo</code> and the value is <code>bar</code>.</p> <p>Let\u2019s also assume that our origin file (the file on the control node that we want to copy to the target) contains this text and is called <code>myIniFile.ini</code>:</p> <pre><code># Variables\nfoo = bar;\n</code></pre> <p>We could write our playbook like this:</p> <pre><code>---\n- name: Create File\nhosts: localhost\nconnection: local\n\ntasks:\n\n- name: Write File\n  template:\n    src: ~/myIniFile.ini\n    dest: /tmp/myIniFile.ini\n</code></pre> <p>However, this would expose our sensitive data to the world. Dangerous stuff!</p> <p>Go ahead and create that INI file (and insert the contents as above). Also create the playbook (as above).</p> <p>Run the playbook: <code>ansible-playbook myPlaybookFile.playbook</code></p> <p>You should expect to see a copy of the <code>myIniFile.ini</code> in <code>/tmp</code>.</p>"},{"location":"ansible-101-vault/#encrypting-the-sensitive-values","title":"Encrypting the Sensitive Values","text":"<p>Now, let\u2019s solve the problem and use Ansible Vault to encrypt our sensitive data. One of the things I love about Ansible is the great documentation. Take a look at the vault docs page and you\u2019ll see this:</p> <p>If you have existing files that you wish to encrypt, use the ansible-vault encrypt command:<code>ansible-vault encrypt file1 file2 ...</code></p> <p>So let's encrypt our INI file:</p> <pre><code>ansible encrypt myIniFile.ini\n</code></pre> <p>Enter a vault password and confirm. You should a confirmation message: <code>Encryption Successful</code></p> <p>To confirm, view the INI file with your favourite editor and you\u2019ll see the encrypted content. This encrypted file is safe to placed into version control systems.</p> <pre><code>$ cat myIniFile.ini\n$ANSIBLE_VAULT;1.1;AES256\n6166445334280909902380515325321\n0809902525523587890285308235808\n...\n</code></pre>"},{"location":"ansible-101-vault/#viewing-unencrypted-vault-data","title":"Viewing Unencrypted Vault Data","text":"<p>To view the unencrypted contents of the file, use the <code>ansible-vault view</code> command:</p> <pre><code>ansible-vault view myIniFile.ini\n</code></pre>"},{"location":"ansible-101-vault/#vault-password-files","title":"Vault Password Files","text":"<p>As an alternative to the above, you can store your password (plain text) in a file, then pass this file at runtime as a parameter.</p> <p>Let\u2019s say your password is <code>password123</code> and is stored in <code>/tmp/passwordfile</code></p> <p>Encrypt your INI file as such:</p> <pre><code>ansible-vault encrypt ~/myIniFile.ini --vault-id=/tmp/passwordfile\n</code></pre> <p>Then to decrypt, just change <code>encrypt</code> to <code>view</code>:</p> <pre><code>ansible-vault view ~/myIniFile.ini --vault-id=/tmp/passwordfile\n</code></pre> <p>Note that you can specify multiple files (and vault IDs) at once. Imagine <code>test.txt</code> is encrypted with the password in <code>passwordfile</code> and <code>test2.txt</code> is encrypted with the password in <code>passwordfile2</code>.</p> <p>This is a perfectly valid command:</p> <pre><code>ansible-vault view ~/test.txt ~/test2.txt --vault-id=~/passwordfile --vault-id=~/passwordfile2\n</code></pre>"},{"location":"ansible-101-vault/#re-execute-playbook-with-ansible-vault-file","title":"Re-Execute Playbook With Ansible Vault File","text":"<ul> <li>Delete the old INI file in /tmp: <code>rm /tmp/myIniFile.ini</code></li> <li>Re-run the playbook, adding the <code>--ask-vault-pass</code> option: <code>ansible-playbook makeFile.playbook --ask-vault-pass</code></li> <li>Notice that the file is decrypted then copied into the <code>/tmp</code> directory</li> </ul>"},{"location":"apache-reverse-proxy-setup/","title":"Apache Reverse Proxy Setup","text":"<p>I was recently asked to configure a reverse proxy server. It was a stepping stone to a further goal but I hadn\u2019t actually configured one before.</p> <p>Whenever I haven\u2019t attempted something before, I always try and keep a record of what I try so that I know what works &amp; crucially what doesn\u2019t.</p> <p>So here\u2019s my config, dear reader. I hope it helps you!</p>"},{"location":"apache-reverse-proxy-setup/#what-were-building","title":"What We're Building","text":"<p>This simple demo will have one proxy server and one web server. The user will interact with the proxy server and as far as they\u2019re concerned, the requests are returned by the proxy server. The end user never has to know that there\u2019s a different server \u201cbehind the scenes\u201d.</p> <p>I\u2019m building this in AWS but any cloud provider (or VMs) will work.</p>"},{"location":"apache-reverse-proxy-setup/#configuration","title":"Configuration","text":"<p>Create two plain EC2 (or other compute nodes) instances. <code>t2.micro\u2019s</code> are fine for this.</p>"},{"location":"apache-reverse-proxy-setup/#step-1-setup-the-webserver","title":"Step 1: Setup The Webserver","text":"<p>Execute the following on the webserver box.</p> <pre><code>sudo yum update -y\nsudo yum install httpd -y\necho \"this is the web server\" | sudo tee /var/www/html/index.html &gt; /dev/null\nsudo service httpd start\n</code></pre> <p>Visit the web server URL in a browser. You should see the text <code>this is the web server</code>. This is the first and last time you\u2019ll access the webserver directly.</p>"},{"location":"apache-reverse-proxy-setup/#step-2-setup-the-proxy-server","title":"Step 2 Setup The Proxy Server","text":"<p>Execute the following on the proxy server box.</p> <pre><code>sudo yum update -y\nsudo yum install httpd -y\nprintf \"this is the proxy server\" | sudo tee /var/www/html/index.html &gt; /dev/null\nsudo service httpd start\n</code></pre> <p>Visit the proxy server URL in a browser. You should see the text <code>this is the proxy server</code>. This is only a sanity check to ensure the proxy is up and running. We have one more configuration to adjust now. At this point, it is not working as you want (the web server should be invisible to the end user).</p> <p>Let\u2019s configure the proxy to actually do what we want. To do that, we need to add two lines to the end of <code>/etc/httpd/conf/httpd.conf</code></p> <pre><code>ProxyPass / http://WEBSERVERIP\nProxyPassReverse / http://WEBSERVERIP\n</code></pre> <p>For my IPs, I can run the following command.</p> <pre><code>printf \"ProxyPass / http://35.177.55.179\\nProxyPassReverse / http://35.177.55.179\" | sudo tee -a /etc/httpd/conf/httpd.conf &gt; /dev/null\n</code></pre> <p>Now reload the httpd process to pick up these changes</p> <pre><code>sudo service httpd reload\n</code></pre> <p>Refresh the proxy server page and you should now see the text <code>this is the web server</code>.</p>"},{"location":"apache-reverse-proxy-setup/#whats-going-on","title":"What's Going On?","text":"<p>All the magic is due to those two lines <code>ProxyPass</code> and <code>ProxyPassReverse</code>. <code>ProxyPass /</code> is an instruction that this Apache should pass all requests (denoted by /) to that URL (<code>http://WEBSERVERIP</code>)</p> <p><code>ProxyPassReverse</code> instructs apache to overwrite the Location, Content-Location and URI headers on HTTP redirect responses. This is essential to avoid bypassing the reverse proxy when the backend servers may be issuing redirects.</p>"},{"location":"appmon-7-0-web-dashboards-js/","title":"AppMon 7.0 - Web Dashboards and Javascript","text":"<p>One of the coolest features of AppMon 7.0 is the ability to add custom JS and CSS to your dashboards. Let\u2019s see what\u2019s possible\u2026</p> <p>Update: 04/03/2018 The capabilities described in the following post are available in Dynatrace AppMon v7.0.0 \u2013 v7.0.9. After v7.0.9 the javascript and CSS functionality was removed for security reasons.</p> <p>v7.0.9 and onwards include the ability to define variables and perform mathematical operations on variables as an out-of-the-box piece of functionality \u2013 no debug flags or workarounds required.</p> <p>Dynatrace application monitoring (AppMon) 7.0 introduced the ability to include custom Javascript and CSS in your web dashboards. This makes for some pretty powerful visualisation and dashboarding abilities.</p>"},{"location":"appmon-7-0-web-dashboards-js/#prerequisites","title":"Prerequisites","text":"<p>To follow this tutorial you\u2019ll need:</p> <ul> <li>A working and licensed AppMon server and collector.</li> <li>An instrumented Apache server with my wait.php file.</li> <li>Put the client into debug mode (<code>Ctrl + F9</code>) then set the <code>com.dynatrace.diagnostics.webui.tmtAllowJsAndCss</code> debug flag to <code>true</code> (server settings &gt; general tab &gt; debug options).</li> </ul> <p></p>"},{"location":"appmon-7-0-web-dashboards-js/#intelligent-dashboards","title":"Intelligent Dashboards","text":"<p>AppMon 7 allows markdown and measure substitution within that markdown (meaning {1} will be replaced with a response time, for example) but wouldn\u2019t it be great if we could take that a step further and actually do maths in the dashboards themselves?</p> <p>Let\u2019s get the basics set up first. Verify that your Apache server is instrumented correctly, then fire some requests at the wait.php page to generate some purepaths.</p> <p></p> <p>Create a response time measure on the wait.php page.</p> <p></p> <p>Open the web dashboards (https://DT-SERVER:9911), create a new dashboard and chart the measure. You should have something resembling this:</p> <p></p>"},{"location":"appmon-7-0-web-dashboards-js/#text-measurement-dashlet","title":"Text &amp; Measurement Dashlet","text":"<p>Let\u2019s use another new feature of AppMon 7 \u2013 the text replacement feature.</p> <p>Add a new text and measures tile to your dashboard and add the <code>wait.php</code> measure twice (one aggregation set to average and one set to maximum). The {1} and {2} will be dynamically replaced with the average and maximum response times respectively.</p> <pre><code>Average response time of Wait Page: {1}\nMaximum response time of Wait page: {2}\n</code></pre> <p></p> <p>CUSTOM JAVASCRIPT</p> <p>Taking things a step further, we can even use these replacements in custom javascript code.</p> <p>The following code takes the average response time, divides it by 10 the stores it into a JS variable called <code>myVar</code>. I can then use this as standard JavaScript code, so if <code>myVar</code> is above <code>myThreshold</code>, I visually warn the user with new message and a red background:</p> <pre><code>&lt;style&gt;\n#status\n{\n  border: 1px solid black;\n  text-align: center;\n}\n&lt;/style&gt;\n&lt;p id=\"status\"&gt;Status: OK&lt;/p&gt;\n\n&lt;script&gt;\nvar myVar = {1} / 10;\nvar myThreshold = 10;\n\nif (myVar &gt; myThreshold)\n{\n  document.getElementById(\u2018status\u2019).innerHTML=\"Status: Warning!\";\n  document.getElementById(\u2018status\u2019).style.color=\"white\";\n  document.getElementById(\u2018status\u2019).style.backgroundColor=\"red\";\n}\n&lt;/script&gt;\n</code></pre> <p>(click video for full resolution version) </p>"},{"location":"appmon-7-0-web-dashboards-js/#bonus-css-animations","title":"Bonus: CSS Animations","text":"<p>How about combining all of this into a nice, slick CSS animated warning message?</p> <pre><code>&lt;style&gt;\n#status\n{\n  border: 1px solid black;\n  text-align: center;\n}\n&lt;/style&gt;\n&lt;p id=\"status\"&gt;Status: OK&lt;/p&gt;\n\n&lt;script&gt;\nvar myThreshold = 10;\nif (myVar &gt; myThreshold)\n{\n  document.getElementById(\"status\").innerHTML = \"Status: Warning!\";\n  document.getElementById(\"status\").style.animationName = \"warningAnimation\";\n  document.getElementById(\"status\").style.animationDuration = \"2s\";\n  document.getElementById(\"status\").style.animationIterationCount = \"infinite\";\n}\n&lt;/script&gt;\n&lt;style&gt;\n/* Safari 4.0 \u2013 8.0 */\n@-webkit-keyframes warningAnimation {\n  to {background-color: red; color: white;}\n}\n\n/* Standard syntax */\n@keyframes warningAnimation {\n  to {background-color: red; color: white;}\n}\n&lt;/style&gt;\n</code></pre> <p>(click video for full resolution version) </p> <p>Has this sparked your curiosity or creativity? Drop me a line and let me know what you\u2019re thinking of building!</p>"},{"location":"appmon-7-user-data-scripts/","title":"AppMon 7 \u2013 Automated User Data Scripts","text":"<p>Legacy Product</p> <p>This tutorial refers to the legacy application monitoring product from Dynatrace. Not the Dynatrace platform.</p> <p>I often find myself needing to quickly create AppMon setups in AWS for POCs and demos, so I built myself a big green go button in the form of some user data scripts.</p> <p>Yes, Dynatrace offers Ansible, Chef and Puppet scripts for longer term / more robust configurations, but I just want something up and running quickly with fewer dependencies.</p>"},{"location":"appmon-7-user-data-scripts/#server-user-data-script","title":"Server User Data Script","text":"<p>Note the sizing and memory settings on lines #12 and #13 \u2013 you\u2019ll want to set these correctly to your environment.</p> <p>https://gist.github.com/agardnerIT/71623c9e47b178ede8c1b8e810d70117</p>"},{"location":"appmon-7-user-data-scripts/#collector-user-data-script","title":"Collector User Data Script","text":"<p>You\u2019ll need to specify the server IP address on line #12. Also note that this sets the RAM to 512MB which is too small for real-world use and this script should only be used in demo scenarios.</p> <p>This script also presets the collector group (making OOTB resilience possible) and the collector name (otherwise it\u2019ll pick up the AWS IP as the name eg. ip-172-10-12-12).</p> <p>https://gist.github.com/agardnerIT/9ece2319d8c354fff02c03572077c002</p>"},{"location":"appmon-7-user-data-scripts/#apache-with-webserver-agent","title":"Apache with Webserver Agent","text":"<p>This script creates an Apache web server with the AppMon agent preinstalled. You\u2019ll need to create an agent group for this agent and set the relevant name on line #16. You\u2019ll also need to specify the collector IP / DNS on line #17.</p> <p>https://gist.github.com/agardnerIT/c7a83f447e0778f0034e2f4f04424226</p> <p>I hope you find these useful for quickly getting things up and running. Are there any other components you\u2019d like to see user data scripts for? Let me know and I\u2019ll create them.</p>"},{"location":"assured-host-groups-autonomous-monitoring/","title":"Assured Host Groups - Autonomous Monitoring","text":"<p>This is the first in a series of tutorials aimed at ensuring your Dynatrace deployment is (autonomous) cloud-ready. This tutorial will demonstrate how to set &amp; keep your host groups in sync.</p> <p>Dynatrace is already extremely automated. The out-of-the-box behaviour is almost install-and-forget. However, I consider it a best practice to configure host groups. They\u2019ve saved me many times, especially as deployments grow larger and larger.</p> <p>Host Groups are simple to set, just append the <code>--set-host-group=something</code> parameter to the install string.</p> <p>{% raw %} <pre><code>sudo /bin/sh/oneagent.sh ...other params... --set-host-group=something\n</code></pre> {% endraw %}</p> <p>That said, how do you keep thousands of hosts in sync during massive deployments? Moreover, how do you do this automatically?</p>"},{"location":"assured-host-groups-autonomous-monitoring/#the-master-list","title":"The Master List","text":"<p>First, you need some sort of agreed master list. This is the list which will always be in sync with your latest values. As cloud-natives, you\u2019ll most likely be storing this in a Git / SVN repository. Obviously, you could also build this logic into a dynamic inventory system, but we\u2019ll keep things simple for this tutorial \u2013 save the playbook and JSON in the same folder.</p> <p>My JSON looks like this:</p> <p>{% raw %} <pre><code>[{\n  \"hostname\": \"host1\",\n  \"hostGroup\": \"something\"\n}, {\n   \"hostname\": \"IP-ADDRESS\",\n   \"hostGroup\": \"FirstGroup\"\n}, {\n   \"hostname\": \"IP-ADDRESS\",\n   \"hostGroup\": \"SecondGroup\"\n}]\n</code></pre> {% endraw %}</p>"},{"location":"assured-host-groups-autonomous-monitoring/#the-automation-script","title":"The Automation Script","text":"<p>It\u2019s no secret that I love Ansible, so that\u2019s what I\u2019ll use here. However, there\u2019s no reason why the concepts can\u2019t be adapted for any automation / configuration management system.</p> <p>Once configured, I\u2019d have my Ansible playbook running on a schedule (or if your master list is on Git, you could run on each code commit).</p> <p>This script is going to:</p> <ul> <li>Check whether the OneAgent is installed.</li> <li>If it isn\u2019t installed, lookup the host group value from the JSON and install the agent. If the JSON does not contain a host group value, fallback to a default.</li> <li>If it is installed, check the current host group value against the JSON definition. If the JSON is different, reconfigure the agent and restart the OneAgent.</li> <li>Remember to replace <code>***</code> with your own tenant ID and API Key.</li> </ul> <p>{% raw %} <pre><code>---\n- name: Ensure Consistent Host Groups\n  hosts: all\n\n  vars:\n    hostList: \"{{ lookup('file', 'hostList.json') }}\"\n    oneagent_installer_script_url: \"https://***.live.dynatrace.com/api/v1/deployment/installer/agent/unix/default/latest?Api-Token=***&amp;arch=x86&amp;flavor=default\"\n    defaultHostGroup: \"DefaultGroup\"\n\n  tasks:\n  - name: Check if Dynatrace OneAgent is already installed\n    stat:\n      path: /opt/dynatrace/oneagent/agent/lib64/liboneagentos.so\n    register: agent_installed\n\n  - name: Set Host Group Facts\n    set_fact:\n      hostGroup: \"{{ item.hostGroup }}\"\n    loop: \"{{ hostList }}\"\n    when: item.hostname == inventory_hostname\n\n  - name: Download OneAgent\n    get_url:\n      url: \"{{ oneagent_installer_script_url }}\"\n      dest: \"/tmp/dynatrace-oneagent.sh\"\n    when: agent_installed.stat.exists == False\n\n  - name: Install Agent\n    shell: \"sh /tmp/dynatrace-oneagent.sh --set-app-log-content-access=true --set-infra-only=false --set-host-group={{ hostvars[inventory_hostname].hostGroup | default(defaultHostGroup)  }}\"\n    become: yes\n    when: agent_installed.stat.exists == False\n\n  - name: Get Current Host Group\n    shell: \"/opt/dynatrace/oneagent/agent/tools/lib64/oneagentutil --get-host-group\"\n    become: yes\n    when: agent_installed.stat.exists == True\n    register: currentHostGroup\n\n  - name: Update Host Group\n    shell: \"/opt/dynatrace/oneagent/agent/tools/lib64/oneagentutil --set-host-group={{ hostvars[inventory_hostname].hostGroup | default(defaultHostGroup) }} --restart-service\"\n    become: yes\n    when: agent_installed.stat.exists == True and currentHostGroup.stdout != (hostvars[inventory_hostname].hostGroup | default(defaultHostGroup))\n</code></pre> {% endraw %}</p> <p>I hope this tutorial has given you some helpful hints and insight into how you can further automate your Dynatrace deployment. As always, questions, comments, suggestions or corrections are most welcome. Just contact me.</p> <p>The latest playbook and JSON file for this tutorial can always be found on Github.</p>"},{"location":"assured-host-metadata-autonomous-monitoring/","title":"Assured Host Metadata - Autonomous Monitoring","text":"<p>Part two of my Dynatrace Autonomous Monitoring series demonstrates how to specify your host metadata as code &amp; automate any updates.</p> <p>The series is intended to build upon the previous lessons, so I highly suggest you complete part one (automated host groups) first.</p> <p></p>"},{"location":"assured-host-metadata-autonomous-monitoring/#include-metadata-in-json","title":"Include Metadata in JSON","text":"<p>As a reminder, we ended the previous tutorial with a JSON file which looked like this:</p> <pre><code>[{\n  \"hostname\": \"host1\",\n  \"hostGroup\": \"something\"\n}, {\n   \"hostname\": \"IP-ADDRESS\",\n   \"hostGroup\": \"FirstGroup\"\n}, {\n   \"hostname\": \"IP-ADDRESS\",\n   \"hostGroup\": \"SecondGroup\"\n}]\n</code></pre> <p>Now let\u2019s add our metadata variables to the JSON file. You can add as many as you like (or none). Metadata values are simple Key / Value pairs which represent whatever useful data you wish to provide.</p> <pre><code>[{\n   \"hostname\": \"10.0.0.1\",\n   \"hostGroup\": \"staging\",\n   \"metadata\": {\n       \"Location\": \"DE\",\n       \"ConsumingRegion\": \"Global\",\n       \"Owner\": \"Alice\",\n       \"ChargeCode\": \"Central\"\n   }\n}, {\n   \"hostname\": \"10.0.0.2\",\n   \"hostGroup\": \"production\",\n   \"metadata\": {\n       \"Location\": \"FR\",\n       \"ConsumingRegion\": \"EU\",\n       \"Owner\": \"Bob\",\n       \"ChargeCode\": \"GLOBAL123\"\n   }\n}, {\n   \"hostname\": \"10.0.0.3\",\n   \"hostGroup\": \"production\",\n   \"metadata\": {}\n}]\n</code></pre>"},{"location":"assured-host-metadata-autonomous-monitoring/#read-push-metadata","title":"Read &amp; Push Metadata","text":"<p>Finally, we need to add a few new tasks to the playbook. These tasks will:</p> <ul> <li>Use the built-in ansible <code>set_facts</code> capability to store the metadata within Ansible.</li> <li>Remove the <code>hostcustomproperties.conf</code> file if it exists. This ensures any changes to the JSON are pushed immediately on the next execution of the playbook.</li> <li>Create a new <code>hostcustomproperties.conf</code> file and write the metadata as <code>Key=Value</code> pairs.</li> <li>Modify the playlist from the previous tutorial and add 2 new tasks to the end:</li> </ul> <pre><code>- name: \"Set Facts\"\n  set_fact:\n   \"metadata\" : \"{{ item.metadata }}\"\n  loop: \"{{ hostList }}\"\n  when: item.hostname == inventory_hostname\n\n- name: \"Remove File if it Exists\"\n  become: yes\n  file:\n    path: /var/lib/dynatrace/oneagent/agent/config/hostcustomproperties.conf\n    state: absent\n\n- name: \"Write KVs to File\"\n  become: yes\n  lineinfile:\n    path: /var/lib/dynatrace/oneagent/agent/config/hostcustomproperties.conf\n    line: \"{{ item.key }}={{ item.value }}\"\n    create: yes\n  with_dict: \"{{ hostvars[inventory_hostname]['metadata'] }}\"\n</code></pre> <p>Your playbook should look like this:</p> <pre><code>---\n- name: Consistent Host Groups and Metadata\n  hosts: apache\n\n  vars:\n    hostList: \"{{ lookup('file', 'hostList.json') }}\"\n    oneagent_installer_script_url: \"https://***.live.dynatrace.com/api/v1/deployment/installer/agent/unix/default/latest?Api-Token=***&amp;arch=x86&amp;flavor=default\"\n    defaultHostGroup: \"DefaultGroup\"\n\n  tasks:\n  - name: Check if Dynatrace OneAgent is already installed\n    stat:\n      path: /opt/dynatrace/oneagent/agent/lib64/liboneagentos.so\n    register: agent_installed\n\n  - name: Set Host Group Facts\n    set_fact:\n      hostGroup: \"{{ item.hostGroup }}\"\n    loop: \"{{ hostList }}\"\n    when: item.hostname == inventory_hostname\n\n  - name: Download OneAgent\n    get_url:\n      url: \"{{ oneagent_installer_script_url }}\"\n      dest: \"/tmp/dynatrace-oneagent.sh\"\n    when: agent_installed.stat.exists == False\n\n  - name: Install Agent\n    shell: \"sh /tmp/dynatrace-oneagent.sh --set-app-log-content-access=true --set-infra-only=false --set-host-group={{ hostvars[inventory_hostname].hostGroup | default(defaultHostGroup)  }}\"\n    become: yes\n    when: agent_installed.stat.exists == False\n\n  - name: Get Current Host Group\n    shell: \"/opt/dynatrace/oneagent/agent/tools/lib64/oneagentutil --get-host-group\"\n    become: yes\n    when: agent_installed.stat.exists == True\n    register: currentHostGroup\n\n  - name: Update Host Group\n    shell: \"/opt/dynatrace/oneagent/agent/tools/lib64/oneagentutil --set-host-group={{ hostvars[inventory_hostname].hostGroup | default(defaultHostGroup) }} --restart-service\"\n    become: yes\n    when: agent_installed.stat.exists == True and currentHostGroup.stdout != (hostvars[inventory_hostname].hostGroup | default(defaultHostGroup))\n\n  - name: \"Set Facts\"\n    set_fact:\n      \"metadata\" : \"{{ item.metadata }}\"\n    loop: \"{{ hostList }}\"\n    when: item.hostname == inventory_hostname\n\n  - name: \"Remove File if it Exists\"\n    become: yes\n    file:\n      path: /var/lib/dynatrace/oneagent/agent/config/hostcustomproperties.conf\n      state: absent\n\n  - name: \"Write KVs to File\"\n    become: yes\n    lineinfile:\n      path: /var/lib/dynatrace/oneagent/agent/config/hostcustomproperties.conf\n      line: \"{{ item.key }}={{ item.value }}\"\n      create: yes\n    with_dict: \"{{ hostvars[inventory_hostname]['metadata'] }}\"\n</code></pre> <p>Here\u2019s a correctly formatted copy of the playbook and JSON on Github.</p>"},{"location":"atlassian-bamboo-automation/","title":"Atlassian Bamboo Automated Install Script","text":"<p>Atlassian Bamboo is an extremely popular CICD platform. Here's an automation script to spin up an instance on Ubuntu.</p> <p>I tested this on an AWS Ubuntu 18.04 <code>t3.small</code> instance. </p>"},{"location":"atlassian-bamboo-automation/#create-shell-script","title":"Create Shell Script","text":"<p>Spin up an Ubuntu instance then create a new shell script and give it executable permissions:</p> <pre><code>touch bambooSetup.sh &amp;&amp; chmod +x bambooSetup.sh\n</code></pre>"},{"location":"atlassian-bamboo-automation/#script-content","title":"Script Content","text":"<p>Paste this into the script:</p> <pre><code>#!/usr/bin/env bash\n\nsudo apt-get update -y\nsudo apt install openjdk-8-jdk-headless -y\nsudo /usr/sbin/useradd --create-home --home-dir /usr/local/bamboo --shell /bin/bash bamboo\nsudo mkdir /opt/bamboofiles\ncd /opt\nsudo wget https://product-downloads.atlassian.com/software/bamboo/downloads/atlassian-bamboo-7.0.2.tar.gz\nsudo tar -xf /opt/atlassian-bamboo-7.0.2.tar.gz\nsudo mv atlassian-bamboo-7.0.2 bamboo\n# Set Bamboo Home which is NOT the same as the install directory\necho \"bamboo.home=/opt/bamboofiles\" | sudo tee --append /opt/bamboo/atlassian-bamboo/WEB-INF/classes/bamboo-init.properties &gt; /dev/null\nsudo chown -R bamboo: /opt/bamboo\nsudo chown -R bamboo: /opt/bamboofiles\nsudo -H -u bamboo /opt/bamboo/bin/start-bamboo.sh\necho \"\"\necho \"================================================================\"\necho \"Bamboo Started!\"\necho \"Bamboo Available at: http://$(curl -s http://169.254.169.254/latest/meta-data/public-ipv4):8085\"\necho \"================================================================\"\necho \"\"\n</code></pre> <p>Once complete, you'll see the Bamboo address printed to the console where <code>1.2.3.4</code> is your VM public IP address.</p> <pre><code>================================================================\nBamboo Started!\nBamboo Available at: http://1.2.3.4:8085\"\n================================================================\n</code></pre>"},{"location":"atlassian-bamboo-automation/#conclusion","title":"Conclusion","text":"<p>At this point, you should have a fully working Atlassian Bamboo installation. Follow the browser instructions to generate a license key and activate Bamboo.</p>"},{"location":"automatic-application-definitions-autonomous-monitoring/","title":"Automatic Application Definitions - Autonomous Monitoring","text":"<p>Part five of my autonomous cloud management (ACM) tutorial series. In this post we'll use the Dynatrace API to automatically define front-end applications, which are based on URL patterns.</p> <p>This tutorial series builds from previous tutorials. I recommend you complete parts 1 through 4 first:</p> <ul> <li>Part one (Host Group Naming)</li> <li>Part two (Host Metadata)</li> <li>Part three (Service &amp; Process Group Naming)</li> <li>Part four (Automatic Tag Rules)</li> </ul>"},{"location":"automatic-application-definitions-autonomous-monitoring/#recap","title":"Recap","text":"<p>Following previous tutorials has given us 2 EC2 instances, both with a single Apache HTTP server running. One is the <code>production</code> server and the other is the <code>staging</code> server. We have split the process groups and services accordingly so that the Davis AI engine will alert separately for production and staging. We've also applied various tags due to the metadata so that we can create flexible filters.</p>"},{"location":"automatic-application-definitions-autonomous-monitoring/#scenario","title":"Scenario","text":"<p>Imagine that we have two different front-end URLs for our <code>production</code> and <code>staging</code> servers: <code>mysite.com</code> and <code>staging.mysite.com</code> respectively.</p> <p>We need to tell Dynatrace to group the traffic into different \u201cbuckets\u201d or applications, based on these URL patterns. We need to do this automatically as part of our pipeline so that we can easily onboard new environments and commit all the configuration as code.</p>"},{"location":"automatic-application-definitions-autonomous-monitoring/#concepts","title":"Concepts","text":"<p>Dynatrace provides the ability to create Applications. An application is nothing more than a logical bucket, based on a domain or URL pattern. An application can have one or more rules associated with it. All user sessions and user action data (what the user did on your website) will be matched to a specific application.</p> <p>Applications can also exist for mobile (native applications) and a special type of application, called an agentless application. Mobile and agentless applications are out of scope of this post.</p>"},{"location":"automatic-application-definitions-autonomous-monitoring/#my-web-application-a-note","title":"My Web Application - A Note","text":"<p>You may notice an application in Dynatrace called \"My web application\". Think of this as the default \"catch-all\" bucket. If traffic arrives that does not match any other rules, it will fall into this bucket.</p> <p>In a correctly configured system, there should be no traffic in \"My web application\".</p>"},{"location":"automatic-application-definitions-autonomous-monitoring/#application-definition-as-json","title":"Application Definition as JSON","text":"<p>Creating a Dynatrace application is a three step process:</p> <ul> <li>Create the application &amp; configure settings such as the percentage of user traffic to capture or telling Dynatrace which Javascript frameworks your site uses.</li> <li>Create the URL mapping rule. This provides the mapping between the URL (or domain) and the application.</li> <li>Apply tags to the applications.</li> </ul> <p>Create three new JSON files. The first to hold details of the application and configuration details. The second to hold the URL mapping rules for the applications defined in the first JSON. The third to hold the tags you wish to assign to each application. Both the application and rules have unique IDs. The rule maps to the application by using the application ID.</p> <p>Note: The application ID format is <code>APPLICATION-{16 digit uppercase HEX value}</code>. The application rules ID format is any valid <code>8-4-4-4-12</code> GUID.</p>"},{"location":"automatic-application-definitions-autonomous-monitoring/#applistjson","title":"appList.json","text":"<p>{% raw %} <pre><code>[{\n    \"id\": \"APPLICATION-0000000000000001\",\n    \"name\": \"Staging App\"\n},\n{\n    \"id\": \"APPLICATION-0000000000000002\",\n    \"name\": \"Production App\"\n}]\n</code></pre> {% endraw %}</p>"},{"location":"automatic-application-definitions-autonomous-monitoring/#apprulesjson","title":"appRules.json","text":"<p>{% raw %} <pre><code>[{\n    \"id\": \"00000000-0000-0000-0000-000000000100\",\n    \"appId\": \"APPLICATION-0000000000000001\",\n    \"pattern\": \"staging.mysite.com\",\n    \"matchType\": \"CONTAINS\",\n    \"matchTarget\": \"URL\"\n},\n{\n    \"id\": \"00000000-0000-0000-0000-000000000200\",\n    \"appId\": \"APPLICATION-0000000000000002\",\n    \"pattern\": \"mysite.com\",\n    \"matchType\": \"CONTAINS\",\n    \"matchTarget\": \"URL\"\n}]\n</code></pre> {% endraw %}</p> <p>If you're following the previous parts of the tutorial, we obviously don't have domain names setup for our instances. So if you want to see traffic assigned to these applications, use the public IPs of your EC2 instances rather than the domain names.</p> <p>For example, replace <code>staging.mysite.com</code> with the public IP of your <code>staging</code> instance and replace <code>mysite.com</code> with the public IP of your <code>production</code> instance.</p>"},{"location":"automatic-application-definitions-autonomous-monitoring/#apptagsjson","title":"appTags.json","text":"<p>{% raw %} <pre><code>[{\n    \"appId\": \"APPLICATION-0000000000000001\",\n    \"tags\": [ \"mysite-staging\" ]\n},\n{\n    \"appId\": \"APPLICATION-0000000000000002\",\n    \"tags\": [ \"mysite-production\" ]\n}]\n</code></pre> {% endraw %}</p>"},{"location":"automatic-application-definitions-autonomous-monitoring/#additional-token-permissions","title":"Additional Token Permissions","text":"<p>Recall from previous steps in this series that your <code>api_write_token</code> had <code>write configuration</code> permissions.</p> <p>To follow this tutorial, we need to give it one more permission: <code>Access problem and event feed, metrics, and topology.</code></p> <p>Your <code>api_write_token</code> should have two permissions now.</p> <p></p>"},{"location":"automatic-application-definitions-autonomous-monitoring/#leverage-dynatrace-apis","title":"Leverage Dynatrace APIs","text":"<p>Now we need to call the Dynatrace APIs to create our application. As mentioned above, this is a two step process.</p> <p>First, let's create our application defintion using the <code>/config/v1/applications/web</code> endpoint with a JSON payload:</p> <p>{% raw %} <pre><code>PUT https://TENANT-URL/api/config/v1/applications/web/APPID?Api-Token=API-WRITE-TOKEN\"\n\nBody Content:\n{\n  \"name\": \"APPLICATION NAME\",\n  \"realUserMonitoringEnabled\": true,\n  \"costControlUserSessionPercentage\": 100,\n  \"loadActionKeyPerformanceMetric\": \"VISUALLY_COMPLETE\",\n  \"xhrActionKeyPerformanceMetric\": \"ACTION_DURATION\",\n  \"loadActionApdexSettings\": {\n    \"toleratedThreshold\": 3000,\n    \"frustratingThreshold\": 12000,\n    \"toleratedFallbackThreshold\": 3000,\n    \"frustratingFallbackThreshold\": 12000,\n    \"considerJavaScriptErrors\": true\n  },\n  \"xhrActionApdexSettings\": {\n    \"toleratedThreshold\": 2500,\n    \"frustratingThreshold\": 10000,\n    \"toleratedFallbackThreshold\": 3000,\n    \"frustratingFallbackThreshold\": 12000,\n    \"considerJavaScriptErrors\": false\n  },\n  \"customActionApdexSettings\": {\n    \"toleratedThreshold\": 3000,\n    \"frustratingThreshold\": 12000,\n    \"toleratedFallbackThreshold\": 3000,\n    \"frustratingFallbackThreshold\": 12000,\n    \"considerJavaScriptErrors\": true\n  },\n  \"waterfallSettings\": {\n    \"uncompressedResourcesThreshold\": 860,\n    \"resourcesThreshold\": 100000,\n    \"resourceBrowserCachingThreshold\": 50,\n    \"slowFirstPartyResourcesThreshold\": 200000,\n    \"slowThirdPartyResourcesThreshold\": 200000,\n    \"slowCdnResourcesThreshold\": 200000,\n    \"speedIndexVisuallyCompleteRatioThreshold\": 50\n  },\n  \"monitoringSettings\": {\n    \"fetchRequests\": false,\n    \"xmlHttpRequest\": true,\n    \"javaScriptFrameworkSupport\": {\n      \"angular\": true,\n      \"dojo\": false,\n      \"extJS\": false,\n      \"icefaces\": false,\n      \"jQuery\": true,\n      \"mooTools\": false,\n      \"prototype\": true,\n      \"activeXObject\": false\n    },\n    \"contentCapture\": {\n      \"resourceTimingSettings\": {\n        \"w3cResourceTimings\": true,\n        \"nonW3cResourceTimings\": false,\n        \"nonW3cResourceTimingsInstrumentationDelay\": 50,\n        \"resourceTimingCaptureType\": \"CAPTURE_FULL_DETAILS\",\n        \"resourceTimingsDomainLimit\": 10\n      },\n      \"javaScriptErrors\": true,\n      \"timeoutSettings\": {\n        \"timedActionSupport\": false,\n        \"temporaryActionLimit\": 0,\n        \"temporaryActionTotalTimeout\": 100\n      },\n      \"visuallyCompleteAndSpeedIndex\": true\n    },\n    \"excludeXhrRegex\": \"\",\n    \"injectionMode\": \"JAVASCRIPT_TAG\",\n    \"libraryFileLocation\": \"\",\n    \"monitoringDataPath\": \"\",\n    \"customConfigurationProperties\": \"\",\n    \"serverRequestPathId\": \"\",\n    \"secureCookieAttribute\": false,\n    \"cookiePlacementDomain\": \"\",\n    \"cacheControlHeaderOptimizations\": true,\n    \"advancedJavaScriptTagSettings\": {\n      \"syncBeaconFirefox\": false,\n      \"syncBeaconInternetExplorer\": false,\n      \"instrumentUnsupportedAjaxFrameworks\": false,\n      \"specialCharactersToEscape\": \"\",\n      \"maxActionNameLength\": 100,\n      \"maxErrorsToCapture\": 10,\n      \"additionalEventHandlers\": {\n        \"userMouseupEventForClicks\": false,\n        \"clickEventHandler\": false,\n        \"mouseupEventHandler\": false,\n        \"blurEventHandler\": false,\n        \"changeEventHandler\": false,\n        \"toStringMethod\": false,\n        \"maxDomNodesToInstrument\": 5000\n        },\n      \"eventWrapperSettings\": {\n        \"click\": false,\n        \"mouseUp\": false,\n        \"change\": false,\n        \"blur\": false,\n        \"touchStart\": false,\n        \"touchEnd\": false\n      },\n      \"globalEventCaptureSettings\": {\n        \"mouseUp\": true,\n        \"mouseDown\": true,\n        \"click\": true,\n        \"doubleClick\": true,\n        \"keyUp\": true,\n        \"keyDown\": true,\n        \"scroll\": true,\n        \"additionalEventCapturedAsUserInput\": \"\"\n      }\n    }\n  },\n  \"userActionNamingSettings\": {\n    \"placeholders\": [],\n    \"loadActionNamingRules\": [],\n    \"xhrActionNamingRules\": [],\n    \"ignoreCase\": true\n  }\n}\n</code></pre> {% endraw %}</p> <p>Now use the <code>/config/v1/applicationDetectionRules</code> endpoint to create the mapping rule:</p> <p>{% raw %} <pre><code>PUT https://TENANT-URL/api/config/v1/applicationDetectionRules/RULEID?Api-Token=API-WRITE-TOKEN\n\nBody Content:\n{\n  \"applicationIdentifier\": \"APPLICATION-***********\",\n  \"filterConfig\": {\n    \"pattern\": \"myapp.example.com\",\n    \"applicationMatchType\": \"BEGINS_WITH|CONTAINS|ENDS_WITH|EQUALS|MATCHES\",\n    \"applicationMatchTarget\": \"DOMAIN|URL\"\n  }\n}\n</code></pre> {% endraw %}</p>"},{"location":"automatic-application-definitions-autonomous-monitoring/#tagging-applications","title":"Tagging Applications","text":"<p>Finally, we'll tag our applications via the API, using the values held in <code>appTags.json</code>.</p> <p>Use the <code>/api/v1/entity/applications/</code> endpoint. The <code>{APP-ID}</code> corresponds to our <code>APPLICATION-*</code> value and the tags array should be populate from the <code>appTags.json</code> file.</p> <p>{% raw %} <pre><code>POST https://TENANT-URL/api/v1/entity/applications/{APP-ID}\n\nBody:\n{\n  \"tags\": [ // Tags from appTags.json ]\n}\n</code></pre> {% endraw %}</p>"},{"location":"automatic-application-definitions-autonomous-monitoring/#playbook","title":"Playbook","text":"<p>Let's add the above logic into our existing Ansible playbook (from the previous tutorial steps). Ensure all 4 files (<code>appList.json</code>, <code>appRules.json</code>, <code>appTags.json</code> and <code>yourPlaybook.yaml</code>) are all in the same folder.</p> <p>Add three new variables to the vars section near the top of the playlist:</p> <p>{% raw %} <pre><code>    app_list: \"{{ lookup('file', 'appList.json') }}\"\n    app_rules: \"{{ lookup('file', 'appRules.json') }}\"\n    app_tags: \"{{ lookup('file', 'appTags.json') }}\"\n</code></pre> {% endraw %}</p> <p>Now add the following tasks to the end of your playbook:</p> <p>{% raw %} <pre><code>- name: Create Applications\n    run_once: true\n    loop: \"{{ app_list }}\"\n    uri:\n      url: \"https://{{ tenant_url }}/api/config/v1/applications/web/{{ item.id }}?Api-Token={{ api_write_token }}\"\n      method: PUT\n      status_code: 201, 204\n      body_format: json\n      body: |\n        {\n          \"name\": \"{{ item.name }}\",\n          \"realUserMonitoringEnabled\": true,\n          \"costControlUserSessionPercentage\": 100,\n          \"loadActionKeyPerformanceMetric\": \"VISUALLY_COMPLETE\",\n          \"xhrActionKeyPerformanceMetric\": \"ACTION_DURATION\",\n          \"loadActionApdexSettings\": {\n            \"toleratedThreshold\": 3000,\n            \"frustratingThreshold\": 12000,\n            \"toleratedFallbackThreshold\": 3000,\n            \"frustratingFallbackThreshold\": 12000,\n            \"considerJavaScriptErrors\": true\n          },\n          \"xhrActionApdexSettings\": {\n            \"toleratedThreshold\": 2500,\n            \"frustratingThreshold\": 10000,\n            \"toleratedFallbackThreshold\": 3000,\n            \"frustratingFallbackThreshold\": 12000,\n            \"considerJavaScriptErrors\": false\n          },\n          \"customActionApdexSettings\": {\n            \"toleratedThreshold\": 3000,\n            \"frustratingThreshold\": 12000,\n            \"toleratedFallbackThreshold\": 3000,\n            \"frustratingFallbackThreshold\": 12000,\n            \"considerJavaScriptErrors\": true\n          },\n          \"waterfallSettings\": {\n            \"uncompressedResourcesThreshold\": 860,\n            \"resourcesThreshold\": 100000,\n            \"resourceBrowserCachingThreshold\": 50,\n            \"slowFirstPartyResourcesThreshold\": 200000,\n            \"slowThirdPartyResourcesThreshold\": 200000,\n            \"slowCdnResourcesThreshold\": 200000,\n            \"speedIndexVisuallyCompleteRatioThreshold\": 50\n          },\n          \"monitoringSettings\": {\n            \"fetchRequests\": false,\n            \"xmlHttpRequest\": true,\n            \"javaScriptFrameworkSupport\": {\n              \"angular\": true,\n              \"dojo\": false,\n              \"extJS\": false,\n              \"icefaces\": false,\n              \"jQuery\": true,\n              \"mooTools\": false,\n              \"prototype\": true,\n              \"activeXObject\": false\n            },\n            \"contentCapture\": {\n              \"resourceTimingSettings\": {\n                \"w3cResourceTimings\": true,\n                \"nonW3cResourceTimings\": false,\n                \"nonW3cResourceTimingsInstrumentationDelay\": 50,\n                \"resourceTimingCaptureType\": \"CAPTURE_FULL_DETAILS\",\n                \"resourceTimingsDomainLimit\": 10\n              },\n                \"javaScriptErrors\": true,\n              \"timeoutSettings\": {\n                \"timedActionSupport\": false,\n                \"temporaryActionLimit\": 0,\n                \"temporaryActionTotalTimeout\": 100\n              },\n              \"visuallyCompleteAndSpeedIndex\": true\n            },\n            \"excludeXhrRegex\": \"\",\n            \"injectionMode\": \"JAVASCRIPT_TAG\",\n            \"libraryFileLocation\": \"\",\n            \"monitoringDataPath\": \"\",\n            \"customConfigurationProperties\": \"\",\n            \"serverRequestPathId\": \"\",\n                \"secureCookieAttribute\": false,\n            \"cookiePlacementDomain\": \"\",\n            \"cacheControlHeaderOptimizations\": true,\n            \"advancedJavaScriptTagSettings\": {\n              \"syncBeaconFirefox\": false,\n              \"syncBeaconInternetExplorer\": false,\n              \"instrumentUnsupportedAjaxFrameworks\": false,\n              \"specialCharactersToEscape\": \"\",\n              \"maxActionNameLength\": 100,\n              \"maxErrorsToCapture\": 10,\n              \"additionalEventHandlers\": {\n                \"userMouseupEventForClicks\": false,\n                \"clickEventHandler\": false,\n                \"mouseupEventHandler\": false,\n                \"blurEventHandler\": false,\n                \"changeEventHandler\": false,\n                \"toStringMethod\": false,\n                \"maxDomNodesToInstrument\": 5000\n              },\n              \"eventWrapperSettings\": {\n                \"click\": false,\n                \"mouseUp\": false,\n                \"change\": false,\n                \"blur\": false,\n                \"touchStart\": false,\n                \"touchEnd\": false\n                },\n              \"globalEventCaptureSettings\": {\n                \"mouseUp\": true,\n                \"mouseDown\": true,\n                \"click\": true,\n                \"doubleClick\": true,\n                \"keyUp\": true,\n                \"keyDown\": true,\n                \"scroll\": true,\n                \"additionalEventCapturedAsUserInput\": \"\"\n                }\n              }\n            },\n            \"userActionNamingSettings\": {\n            \"placeholders\": [],\n            \"loadActionNamingRules\": [],\n                \"xhrActionNamingRules\": [],\n              \"ignoreCase\": true\n          }\n        }\n\n  - name: Create Application Rules\n    run_once: true\n    loop: \"{{ app_rules }}\"\n    uri:\n      url: \"https://{{ tenant_url }}/api/config/v1/applicationDetectionRules/{{ item.id }}?Api-Token={{ api_write_token }}\"\n      method: PUT\n      status_code: 201, 204\n      body_format: json\n      body: |\n        {\n          \"applicationIdentifier\": \"{{ item.appId }}\",\n          \"filterConfig\": {\n          \"pattern\": \"{{ item.pattern }}\",\n          \"applicationMatchType\": \"{{ item.matchType }}\",\n          \"applicationMatchTarget\": \"{{ item.matchTarget }}\"\n          }\n        }\n\n  - name: Create Application Tags\n    run_once: true\n    loop: \"{{ app_tags }}\"\n    uri:\n      url: \"https://{{ tenant_url }}/api/v1/entity/applications/{{ item.appId }}?Api-Token={{ api_write_token }}\"\n      method: POST\n      status_code: 201, 204\n      body_format: json\n      body: |\n        {\n          \"tags\": {{ item.tags }},\n        }\n</code></pre> {% endraw %}</p> <p>Execute your playbook and view the results in Dynatrace.</p> <p>Go to \"Applications\" and you should see two new applications: <code>Staging App</code> and <code>Production App</code>.</p> <p>Drill into each applications to see the tags and URL rules.</p>"},{"location":"automatic-application-definitions-autonomous-monitoring/#result","title":"Result","text":""},{"location":"automatic-tag-rules-autonomous-monitoring/","title":"Automatic Tag Rules - Autonomous Monitoring","text":"<p>Part four of my autonomous cloud management (ACM) tutorial series. In this article we\u2019ll look at an API driven way to apply tags to all of your entities (hosts, processes, process groups / clusters and services).</p> <p>This tutorial series builds from previous tutorials. I recommend you complete parts 1 through 3 first:</p> <ul> <li>Part one (Host Group Naming)</li> <li>Part two (Host Metadata)</li> <li>Part three (Service &amp; Process Group Naming)</li> </ul>"},{"location":"automatic-tag-rules-autonomous-monitoring/#tutorial-aim","title":"Tutorial Aim","text":"<p>This tutorial will leverage the custom metadata we\u2019ve already applied, we\u2019ll create automatic (dynamic) tag rules so that we can use these values as filters.</p>"},{"location":"automatic-tag-rules-autonomous-monitoring/#recap","title":"Recap","text":"<p>Recall that in the previous tutorials we added the following metadata tags to each host (with unique values for each host):</p> <p>{% raw %} <pre><code>\"metadata\": {\n       \"Location\": \"DE\",\n       \"ConsumingRegion\": \"Global\",\n       \"Owner\": \"Alice\",\n       \"ChargeCode\": \"Central\"\n   }\n</code></pre> {% endraw %}</p> <p></p> <p>It would be extremely powerful to have the ability to use these values as filters:</p> <ul> <li>Show me all hosts with a <code>ChargeCode</code> of <code>Central</code>.</li> <li>Show me all hosts and services for which <code>Alice</code> is responsible.</li> <li>Show me all hosts in <code>France</code>.</li> </ul>"},{"location":"automatic-tag-rules-autonomous-monitoring/#how","title":"How?","text":""},{"location":"automatic-tag-rules-autonomous-monitoring/#api-token","title":"API Token","text":"<p>The first thing we need is an API Token. Go to Settings &gt; Integration &gt; Dynatrace API and create your token.</p> <p>Make sure it has at least <code>Write configuration</code> permissions.</p> <p></p>"},{"location":"automatic-tag-rules-autonomous-monitoring/#call-api-via-ansible","title":"Call API Via Ansible","text":"<p>We\u2019ll use Ansible to call the Dynatrace API. This will create the rule definitions. We\u2019ll create a separate task for each rule.</p> <p>First, add two variables to the <code>vars</code> section near the top of your playlist: <code>tenant_url</code> and <code>api_write_token</code>. Adjust the values as appropriate for your environment.</p> <p>{% raw %} <pre><code>vars:\n  tenant_url: abc123456.live.dynatrace.com\n  api_write_token: xYz123ABCdef678\n</code></pre> {% endraw %}</p> <p>Now create one API call. I\u2019ll define it, explain it below, then copy &amp; paste 3 more times, modifying as we go:</p> <p>{% raw %} <pre><code>- name: Create ChargeCode Auto Tag Rule\n  run_once: true\n  uri:\n    method: PUT\n    status_code: 201, 204\n    body_format: json\n    url: https://{{ tenant_url }}/api/config/v1/autoTags/12345678-1234-1234-1234-123456789012?Api-Token={{api_write_token }}\n    body: |\n      {\n        \"name\": \"ChargeCode\",\n        \"rules\": [\n        {\n          \"type\": \"PROCESS_GROUP\",\n          \"enabled\": true,\n          \"valueFormat\": \"{Host:Environment:ChargeCode}\",\n          \"propagationTypes\": [\n            \"PROCESS_GROUP_TO_HOST\",\n            \"PROCESS_GROUP_TO_SERVICE\"\n          ],\n          \"conditions\": [\n              {\n              \"key\": {\n                \"attribute\": \"HOST_CUSTOM_METADATA\",\n                \"dynamicKey\": {\n                  \"source\": \"ENVIRONMENT\",\n                  \"key\": \"ChargeCode\"\n                },\n                \"type\": \"HOST_CUSTOM_METADATA_KEY\"\n              },\n              \"comparisonInfo\": {\n                \"type\": \"STRING\",\n                \"operator\": \"EXISTS\",\n                \"value\": null,\n                \"negate\": false,\n                \"caseSensitive\": null\n              }\n            }\n            ]\n        }\n        ]\n      }\n</code></pre></p>"},{"location":"automatic-tag-rules-autonomous-monitoring/#task-explanation","title":"Task Explanation","text":"<p>We could improve the Ansible code here in numerous ways. If you\u2019re comfortable enough with Ansible, please feel free to do so. I\u2019ve deliberately left the example verbose for clarity.</p> <p>The task is called <code>Create ChargeCode Auto Tag Rule</code>. Remember that we have two hosts in our inventory. We only need to execute this API call (aka run this task) once. Hence the run_once command.</p> <p>We are executing a <code>PUT</code> call to the API and the URL format is:</p> <p>{% raw %} <pre><code>https://TENANT-URL/api/config/v1/autoTags/TAG-UUID?Api-token=API-WRITE-TOKEN\n</code></pre> {% endraw %}</p> <p>Note that the <code>UUID</code> is something you define and it must be in the standard <code>8-4-4-4-12</code> format. Being a unique ID, it must also be, err, unique, for each tag. Clever huh?</p> <p>Within the <code>PUT</code> call, we will be passing in body content which defines our tag rule. The format of the body is JSON. Hence <code>body_format: json</code>.</p> <p>The Dynatrace API returns one of two HTTP response codes on a successful call. A <code>201</code> response means everything was OK &amp; your tag rule was successfully created. A <code>204</code> code means that your tag was updated successfully. This means you can re-execute your Ansible playbook multiple times with no ill effects. The <code>status_code</code> rule tells Ansible to consider <code>201</code> and <code>204</code> as successful calls.</p> <p><code>body</code> is obviously the <code>PUT</code> body content. Every tag is a <code>Key=Value</code> pair. The Key is represented by the name parameter. This tag rule will be enabled and the ruleset will match against to process groups.</p> <p>Any tags applied to the process group will be propagated down to the hosts that they run on. The tags will also be propagated up to the services offered by those processes / process groups. This is denoted within the <code>propagationTypes</code> parameter.</p> <p>The <code>conditions</code> and <code>comparisonInfo</code> work together to denote that this tag rule will match whenever the String value of <code>ChargeCode</code> exists. If found, the value of this tag will be pulled from the value of the host metadata (see the <code>valueFormat</code> parameter).</p> <p>All together, this means that whenever the <code>ChargeCode</code> host metadata tag is found, Dynatrace will create a tag with the <code>Key=ChargeCode</code> and a dynamic value.</p> <p>CREATE OTHER TAG API CALLS</p> <p>Copy and paste the above Ansible tasks 3 more times. Change the UUID value for each and change ChargeCode to match the other Keys (ConsumingRegion, Location and Owner).</p> <p>Your final playbook (including previous tutorials) should look something like this:</p> <p>{% raw %} <pre><code>---\n- name: ACM Tutorial Playbook\n  hosts: apache\n\n  vars:\n    hostList: \"{{ lookup('file', 'hostList.json') }}\"\n    oneagent_installer_script_url: \"https://{{ tenant_url}}/api/v1/deployment/installer/agent/unix/default/latest?Api-Token=***&amp;arch=x86&amp;flavor=default\"\n    defaultHostGroup: \"DefaultGroup\"\n    tenant_url: \"***.live.dynatrace.com\"\n    api_write_token: \"***\"\n\n  tasks:\n  - name: Check if Dynatrace OneAgent is already installed\n    stat:\n      path: /opt/dynatrace/oneagent/agent/lib64/liboneagentos.so\n    register: agent_installed\n\n  - name: Set Host Group Facts\n    set_fact:\n      hostGroup: \"{{ item.hostGroup }}\"\n    loop: \"{{ hostList }}\"\n    when: item.hostname == inventory_hostname\n\n  - name: Download OneAgent\n    get_url:\n      url: \"{{ oneagent_installer_script_url }}\"\n      dest: \"/tmp/dynatrace-oneagent.sh\"\n    when: agent_installed.stat.exists == False\n\n  - name: Install Agent\n    shell: \"sh /tmp/dynatrace-oneagent.sh --set-app-log-content-access=true --set-host-group={{ hostvars[inventory_hostname].hostGroup | default(defaultHostGroup)  }}\"\n    become: yes\n    when: agent_installed.stat.exists == False\n\n  - name: Get Current Host Group\n    shell: \"/opt/dynatrace/oneagent/agent/tools/lib64/oneagentutil --get-host-group\"\n    become: yes\n    when: agent_installed.stat.exists == True\n    register: currentHostGroup\n\n  - name: Update Host Group\n    shell: \"/opt/dynatrace/oneagent/agent/tools/lib64/oneagentutil --set-host-group={{ hostvars[inventory_hostname].hostGroup | default(defaultHostGroup) }} --restart-service\"\n    become: yes\n    when: agent_installed.stat.exists == True and currentHostGroup.stdout != (hostvars[inventory_hostname].hostGroup | default(defaultHostGroup))\n\n  - name: \"Set Facts\"\n    set_fact:\n     \"metadata\" : \"{{ item.metadata }}\"\n    loop: \"{{ hostList }}\"\n    when: item.hostname == inventory_hostname\n\n  - name: \"Remove File if it Exists\"\n    become: yes\n    file:\n      path: /var/lib/dynatrace/oneagent/agent/config/hostcustomproperties.conf\n      state: absent\n\n  - name: \"Write KVs to File\"\n    become: yes\n    lineinfile:\n      path: /var/lib/dynatrace/oneagent/agent/config/hostcustomproperties.conf\n      line: \"{{ item.key }}={{ item.value }}\"\n      create: yes\n    with_dict: \"{{ hostvars[inventory_hostname]['metadata'] }}\"\n\n  - name: Install HTTPD (Apache)\n    become: yes\n    package:\n      name: httpd\n      state: present\n\n  - name: Start HTTPD (Apache)\n    become: yes\n    service:\n      name: httpd\n      state: started\n\n  - name: Create ChargeCode Auto Tag Rule\n    run_once: true\n    uri:\n      method: PUT\n      status_code: 201, 204\n      body_format: json\n      url: https://{{ tenant_url }}/api/config/v1/autoTags/12345678-1234-1234-1234-123456789012?Api-Token={{ api_write_token }}\n      body: |\n        {\n          \"name\": \"ChargeCode\",\n          \"rules\": [\n          {\n            \"type\": \"PROCESS_GROUP\",\n            \"enabled\": true,\n            \"valueFormat\": \"{Host:Environment:ChargeCode}\",\n            \"propagationTypes\": [\n            \"PROCESS_GROUP_TO_HOST\",\n            \"PROCESS_GROUP_TO_SERVICE\"\n          ],\n          \"conditions\": [\n              {\n              \"key\": {\n                \"attribute\": \"HOST_CUSTOM_METADATA\",\n                \"dynamicKey\": {\n                  \"source\": \"ENVIRONMENT\",\n                  \"key\": \"ChargeCode\"\n                },\n                \"type\": \"HOST_CUSTOM_METADATA_KEY\"\n              },\n              \"comparisonInfo\": {\n                \"type\": \"STRING\",\n                \"operator\": \"EXISTS\",\n                \"value\": null,\n                \"negate\": false,\n                \"caseSensitive\": null\n              }\n            }\n            ]\n          }\n        ]\n        }\n\n  - name: Create ConsumingRegion Auto Tag Rule\n    run_once: true\n    uri:\n      method: PUT\n      status_code: 201, 204\n      body_format: json\n      url: https://{{ tenant_url }}/api/config/v1/autoTags/23456789-2345-2345-2345-234567890123?Api-Token={{ api_write_token }}\n      body: |\n        {\n          \"name\": \"ConsumingRegion\",\n          \"rules\": [\n          {\n            \"type\": \"PROCESS_GROUP\",\n            \"enabled\": true,\n            \"valueFormat\": \"{Host:Environment:ConsumingRegion}\",\n            \"propagationTypes\": [\n            \"PROCESS_GROUP_TO_HOST\",\n            \"PROCESS_GROUP_TO_SERVICE\"\n          ],\n          \"conditions\": [\n              {\n              \"key\": {\n                \"attribute\": \"HOST_CUSTOM_METADATA\",\n                \"dynamicKey\": {\n                  \"source\": \"ENVIRONMENT\",\n                  \"key\": \"ConsumingRegion\"\n                },\n                \"type\": \"HOST_CUSTOM_METADATA_KEY\"\n              },\n              \"comparisonInfo\": {\n                \"type\": \"STRING\",\n                \"operator\": \"EXISTS\",\n                \"value\": null,\n                \"negate\": false,\n                \"caseSensitive\": null\n              }\n            }\n            ]\n          }\n        ]\n        }\n\n  - name: Create Location Auto Tag Rule\n    run_once: true\n    uri:\n      method: PUT\n      status_code: 201, 204\n      body_format: json\n      url: https://{{ tenant_url }}/api/config/v1/autoTags/34567890-3456-3456-3456-345678901234?Api-Token={{ api_write_token }}\n      body: |\n        {\n          \"name\": \"Location\",\n          \"rules\": [\n          {\n            \"type\": \"PROCESS_GROUP\",\n            \"enabled\": true,\n            \"valueFormat\": \"{Host:Environment:Location}\",\n            \"propagationTypes\": [\n            \"PROCESS_GROUP_TO_HOST\",\n            \"PROCESS_GROUP_TO_SERVICE\"\n          ],\n          \"conditions\": [\n              {\n              \"key\": {\n                \"attribute\": \"HOST_CUSTOM_METADATA\",\n                \"dynamicKey\": {\n                  \"source\": \"ENVIRONMENT\",\n                  \"key\": \"Location\"\n                },\n                \"type\": \"HOST_CUSTOM_METADATA_KEY\"\n              },\n              \"comparisonInfo\": {\n                \"type\": \"STRING\",\n                \"operator\": \"EXISTS\",\n                \"value\": null,\n                \"negate\": false,\n                \"caseSensitive\": null\n              }\n            }\n            ]\n          }\n        ]\n        }\n\n  - name: Create Owner Auto Tag Rule\n    run_once: true\n    uri:\n      method: PUT\n      status_code: 201, 204\n      body_format: json\n      url: https://{{ tenant_url }}/api/config/v1/autoTags/45678901-4567-4567-4567-456789012345?Api-Token={{ api_write_token }}\n      body: |\n        {\n          \"name\": \"Owner\",\n          \"rules\": [\n          {\n            \"type\": \"PROCESS_GROUP\",\n            \"enabled\": true,\n            \"valueFormat\": \"{Host:Environment:Owner}\",\n            \"propagationTypes\": [\n            \"PROCESS_GROUP_TO_HOST\",\n            \"PROCESS_GROUP_TO_SERVICE\"\n          ],\n          \"conditions\": [\n              {\n              \"key\": {\n                \"attribute\": \"HOST_CUSTOM_METADATA\",\n                \"dynamicKey\": {\n                  \"source\": \"ENVIRONMENT\",\n                  \"key\": \"Owner\"\n                },\n                \"type\": \"HOST_CUSTOM_METADATA_KEY\"\n              },\n              \"comparisonInfo\": {\n                \"type\": \"STRING\",\n                \"operator\": \"EXISTS\",\n                \"value\": null,\n                \"negate\": false,\n                \"caseSensitive\": null\n              }\n            }\n            ]\n          }\n        ]\n        }\n</code></pre> {% endraw %}</p> <p>We now have hosts, processes, process groups and services all automatically tagged based on the content of the host metadata. The host metadata is, in itself, dynamic because it\u2019s based on a JSON file.</p> <p></p> <p>As always, the latest version of the host list JSON and playbook is available on Github.</p>"},{"location":"awx-on-amazon-ansible-playbook/","title":"AWX on Amazon Ansible Playbook","text":"<p>Today I spent a good few hours trying to get the Geerling Guy AWX role to work. AWX is the open source, free version of Ansible Tower. Suffice to say, the combination of dependencies is sufficiently broken to be (currently) useless. Here\u2019s the fix...</p> <p>However, I have put together a handy playbook which does work for Amazon AMIs (specifically, I used the 2018.03.0 AMI).</p> <p>Here\u2019s the playbook (latest version will always be on GitHub):</p> <p>{% raw %} <pre><code>---\n- name: Install AWX\n  hosts: awx\n  become: yes\n\n  vars:\n    - tmpdir: \"/tmp/ansible\"\n\n  tasks:\n\n  - name: Update yum\n    yum:\n      name: '*'\n      state: latest\n\n  - name: Install Ansible\n    pip:\n      name: ansible\n      state: latest\n\n  - name: Install Repositories\n    yum:\n      name: \"{{ item }}\"\n    loop:\n      - git\n      - docker\n\n  - name: Start Docker\n    service:\n      name: docker\n      state: started\n\n  - name: Clone awx\n    git:\n      repo: 'https://github.com/ansible/awx.git'\n      dest: \"{{ tmpdir }}\"\n      clone: yes\n\n  - name: Install Docker Compose\n    pip:\n      name: docker-compose\n      state: latest\n\n  - name: Install AWX\n    command: /usr/local/bin/ansible-playbook -i {{ tmpdir }}/installer/inventory {{ tmpdir }}/installer/install.yml\n</code></pre> {% endraw %}</p> <p>AWX runs on port 80. The login is <code>admin</code> / <code>password</code>.</p> <p></p> <p>If you have any questions, comments or suggestions. Please contact me or raise a GitHub issue.</p>"},{"location":"batch-job-monitoring-dynatrace/","title":"Batch Job Monitoring with Dynatrace","text":"<p>Legacy Product</p> <p>This tutorial refers to the legacy application monitoring product from Dynatrace. Not the Dynatrace platform.</p> <p>Dynatrace ships with many out of the box sensor packs but for anything non-standard, you\u2019ll need a little bit more configuration to get up and running. This post will show you how to monitor any custom batch jobs within Dynatrace AppMon and retrieve the full Purepath data for the job.</p> <p>This post will give a Java-based example, but the principles are identical for other technology types.</p>"},{"location":"batch-job-monitoring-dynatrace/#prerequisites","title":"Prerequisites","text":"<p>Ensure you have a System profile and agent group created, ready for your agent to connect to.</p>"},{"location":"batch-job-monitoring-dynatrace/#code","title":"Code","text":"<p>Here is my batch job. The script is packaged as an executable JAR file which runs in an endless loop. Every 2 seconds the <code>startBatch</code> method is called. The <code>startBatch</code> method calls the <code>doThis</code> method which simply prints a message to the console. The batch then sleeps until the next loop execution.</p> <p>To test: Download the executable JAR file or view the source code on Github.</p> <p>To execute: <code>java -jar SimpleBatchJob.jar</code></p> <pre><code>package com.adamgardner.code;\n\npublic class EntryPoint {\n  public static void main(String[] args) {\n    while (true)\n    {\n      try\n      {\n        startBatch();\n        Thread.sleep(2000);\n      }\n      catch (Exception e) {}\n    }\n  }\n\n  private static void startBatch() {\n    System.out.println(\"Starting Batch Job\");\n    doThis();\n  }\n\n  private static void doThis() {\n    System.out.println(\"Now doing this\u2026\");\n    System.out.println(\"\u2014\u2014\u2014\u2014\u2014\u2014\");\n  }\n}\n</code></pre>"},{"location":"batch-job-monitoring-dynatrace/#instrument-with-appmon","title":"Instrument with AppMon","text":"<p>When you first created your tier, you AppMon will have given you a string that you need to include in the JVM startup parameters. It will begin with <code>-agentpath:...</code></p> <p>Mine is:</p> <pre><code>-agentpath:\"C:/Program Files/Dynatrace/Dynatrace 7.0/agent/lib64/dtagent.dll\"=name=BatchTier_Monitoring,collector=localhost:9998\n</code></pre> <p>So rather than executing my JAR as a standard JAR (<code>java -jar SimpleBatchJob.jar</code>), I\u2019ll use:</p> <pre><code>java -jar -agentpath:\"C:/Program Files/Dynatrace/Dynatrace 7.0/agent/lib64/dtagent.dll\"=name=BatchTier_Monitoring,collector=localhost:9998 SimpleBatchJob.jar\n</code></pre> <p>Note that the name portion on the above will be unique to your setup.</p> <p>In the agents overview dashlet, you should now see a happily connected agent (look for the green ring).</p> <p></p>"},{"location":"batch-job-monitoring-dynatrace/#adding-sensors-creating-purepaths","title":"Adding Sensors &amp; Creating Purepaths","text":"<p>At this point, you won\u2019t have any purepaths. Remember, I told you that we\u2019d need to create some custom sensor rules? Time to do that. This will tell Dynatrace where to begin the purepath.</p> <p>We are therefore going to want to place the sensor on the <code>startBatch</code> method. In AppMon terms, this is called the Purepath Entry Point.</p> <ul> <li>Open the system profile &gt; sensors &gt; Add a sensor group</li> </ul> <p></p> <ul> <li>Create a Java Method sensor group. Give it any name you desire.</li> </ul> <p></p> <ul> <li>Browse the class cache, highlight the BatchTier agent group and click the yellow arrow icon.</li> </ul> <p></p> <ul> <li>Browse (or use <code>Ctrl+F</code> to search) until you find the <code>startBatch</code> method. Place a sensor on it. You will be warned that changed sensor rules will only take effect after you application is restarted, that\u2019s OK. We\u2019ll restart the JAR soon so you can OK this dialog.</li> </ul> <p></p> <ul> <li>The final step is to make sure you\u2019ve placed the sensor group (mine was called My BatchJob Sensor Group) on the tier containing the agents (my tier was called BatchTier). Navigate to the Sensor Placement section of the relevant agent group and ensure it's placed and set to Active and Start Purepaths (sensor configuration settings).</li> </ul> <p></p>"},{"location":"batch-job-monitoring-dynatrace/#restart-app-see-your-batch-job","title":"Restart App &amp; See Your Batch Job","text":"<p>Restart the JAR file then open the Purepaths dashlet to see your purepaths.</p> <p></p> <p>In future tutorials we\u2019ll expand this batch job to cover more complex batch processing scenarios.</p>"},{"location":"canary-releases-istio/","title":"Canary Releases in Istio & Kubernetes","text":"<p>Recently I have been playing with Istio on Kubernetes &amp; one of the things I wanted to achieve was seamless canary releases. Turns out it is really easy with Istio. Let's dive in...</p>"},{"location":"canary-releases-istio/#prerequisites","title":"Prerequisites","text":"<p>To follow along, you will need: - A kubernetes cluster (localhost Docker for Desktop is fine). - Istio installed (Installation takes 5 mins - see getting started guide). - Your <code>default</code> namespace labelled with <code>istio-injection=enabled</code>.</p>"},{"location":"canary-releases-istio/#overview","title":"Overview","text":"<p>We will deploy two versions of a website to our k8s cluster as two seperate Services: <code>v1</code> and <code>v2</code>.</p> <p>We will then use Istio to control the traffic routing between the two versions of our service.</p> <p>We start at 100% of traffic on <code>v1</code> and gradually switch over to <code>v2</code>.</p> <p>This is <code>v1</code>. </p> <p>This is <code>v2</code>. </p>"},{"location":"canary-releases-istio/#deployments","title":"Deployments","text":"<p>Save and apply the <code>v1</code> deployment YAML file:</p> <p>web-v1-deployment.yaml <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web-v1\n  labels:\n    app: web\n    version: v1\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: web\n      version: v1\n  template:\n    metadata:\n      labels:\n        app: web\n        version: v1\n    spec:\n      containers:\n      - image: gardnera/canarytester:v1\n        name: web-v1\n        ports:\n        - containerPort: 80\n        imagePullPolicy: Always\n      restartPolicy: Always\n</code></pre></p> <p>Then apply with:</p> <pre><code>kubectl apply -f web-v1-deployment.yaml\n</code></pre> <p>Repeat for <code>v2</code>:</p> <p>web-v2-deployment.yaml <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web-v2\n  labels:\n    app: web\n    version: v2\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: web\n      version: v2\n  template:\n    metadata:\n      labels:\n        app: web\n        version: v2\n    spec:\n      containers:\n      - image: gardnera/canarytester:v2\n        name: web-v2\n        ports:\n        - containerPort: 80\n        imagePullPolicy: Always\n      restartPolicy: Always\n</code></pre></p> <p>Apply with:</p> <pre><code>kubectl apply -f web-v2-deployment.yaml\n</code></pre> <p>Verify that both pods are running and they both show <code>2/2</code> under \"READY\":</p> <pre><code>kubectl get pods\nNAME             READY   STATUS    RESTARTS   AGE\nweb-v1-***-***   2/2     Running   0          1m\nweb-v2-***-***   2/2     Running   0          1m\n</code></pre>"},{"location":"canary-releases-istio/#services","title":"Services","text":"<p>The pods are running and Istio has been injected (proven by the <code>2/2</code> shown above).</p> <p>Now we will create a service for each version. Save and apply these YAML files:</p> <p>web-v1-service.yaml</p> <p><pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: web-v1-service\nspec:\n  ports:\n  - name: \"80\"\n    port: 80\n  selector:\n    app: web\n    version: v1\n</code></pre> web-v2-service.yaml</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: web-v2-service\nspec:\n  ports:\n  - name: \"80\"\n    port: 80\n  selector:\n    app: web\n    version: v2\n</code></pre> <p>Apply both with:</p> <pre><code>kubectl apply -f web-v1-service.yaml\nkubectl apply -f web-v2-service.yaml\n</code></pre> <p>Verify the services have been created: <pre><code>% kubectl get services\nNAME             TYPE        CLUSTER-IP  EXTERNAL-IP   PORT(S)   AGE\nkubernetes       ClusterIP   10.*.*.*    &lt;none&gt;        443/TCP   2d19h\nweb-v1-service   ClusterIP   10.*.*.*    &lt;none&gt;        80/TCP    11s\nweb-v2-service   ClusterIP   10.*.*.*    &lt;none&gt;        80/TCP    4s\n</code></pre></p>"},{"location":"canary-releases-istio/#expose-app-outside-cluster","title":"Expose App Outside Cluster","text":"<p>The application runs on port 80 so you would think that going to <code>http://127.0.0.1</code> in a browser would show something.</p> <p>Istio has actually already given us a way into the cluster on <code>localhost</code> on (among others) port <code>80</code>:</p> <pre><code>% kubectl get services -n istio-system\nNAME              ...    EXTERNAL-IP   PORT(S) ...\nistio-ingressgateway     localhost     80:32481/TCP,443:31558/TCP\n</code></pre>"},{"location":"canary-releases-istio/#whats-missing","title":"What's Missing?","text":"<p>The workload is up, running, connected to a service. We have traffic allowed to enter the cluster. What is missing?</p> <p>In a nutshell, the internal routing which tells Istio: This traffic should be sent to service X, Y or Z.</p>"},{"location":"canary-releases-istio/#istio-gateway-and-virtualservice","title":"Istio Gateway and VirtualService","text":"<p>To resolve this, we need two more components, a <code>Gateway</code> and a <code>VirtualService</code>.</p> <p>These two components work in tandem to get your traffic from the \"front door\" of the cluster back to the correct service.</p> <p>Save this file &amp; then I will explain:</p> <p>web-gateway-virtualservice.yaml</p> <pre><code>apiVersion: networking.istio.io/v1alpha3\nkind: Gateway\nmetadata:\n  name: web-gateway\nspec:\n  selector:\n    istio: ingressgateway # use Istio default gateway implementation\n  servers:\n  - port:\n      number: 80\n      name: http\n      protocol: HTTP\n    hosts:\n    - \"*\"\n---\napiVersion: networking.istio.io/v1alpha3\nkind: VirtualService\nmetadata:\n  name: web-virtualservice\nspec:\n  hosts:\n  - \"*\"\n  gateways:\n  - web-gateway\n  http:\n  - match:\n    - uri:\n        exact: /\n    route:\n    - destination:\n        host: web-v1-service\n        port:\n          number: 80\n</code></pre> <p>Reading the YAML from top to bottom: - We create a <code>Gateway</code> called <code>web-gateway</code>. - This gateway accepts traffic from the <code>istio-ingressgateway</code>. - This gateway accepts <code>HTTP</code> traffic from any host (<code>hosts: \"*\"</code>) on port <code>80</code>. - We create a <code>VirtualService</code> called <code>web-virtualservice</code>. - This <code>VirtualService</code> also accepts incoming traffic from any hosts. - This <code>VirtualService</code> is linked to the <code>web-gateway</code> ie. handles traffic from <code>web-gateway</code>. - For all requests, the <code>VirtualService</code> does a lookup. If the URI path is exactly <code>/</code> (ie. the homepage) then traffic is routed to the <code>web-v1-service</code> Service on port <code>80</code>.</p> <p>Apply this file to your cluster:</p> <pre><code>kubectl apply -f web-gateway-virtualservice.yaml\n</code></pre> <p>Now try to hit your website in a browser on <code>http://127.0.0.1</code>. You should see <code>v1</code>.</p> <p></p>"},{"location":"canary-releases-istio/#canary-releases-introducing-v2","title":"Canary Releases: Introducing v2","text":"<p>The origin of the term \"canary release\" refers to the canaries that miners carried into the coalmines. These birds would be the early warning sign of dangerous gases for miners. The birds would become ill (or dead) and thus give the miners a chance to get out before they succumbed. This was still happening within my lifetime!</p> <p>In the same way, we can release software to a small portion of our users which, if problems occur, gives us time to rollback safely without affecting all users.</p> <p>Remember that both <code>v1</code> and <code>v2</code> are actually deployed to the cluster, but due to the <code>VirtualService</code> rule, no-one can access <code>v2</code>.</p> <p>We will gradually introduce traffic to <code>v2</code> of the service until we're using <code>v2</code> exclusively.</p> <p>Modify the web-gateway-virtualservice.yaml as such:</p> <pre><code>apiVersion: networking.istio.io/v1alpha3\nkind: Gateway\nmetadata:\n  name: web-gateway\nspec:\n  selector:\n    istio: ingressgateway # use Istio default gateway implementation\n  servers:\n  - port:\n      number: 80\n      name: http\n      protocol: HTTP\n    hosts:\n    - \"*\"\n---\napiVersion: networking.istio.io/v1alpha3\nkind: VirtualService\nmetadata:\n  name: web-virtualservice\nspec:\n  hosts:\n  - \"*\"\n  gateways:\n  - web-gateway\n  http:\n  - match:\n    - uri:\n        exact: /\n    route:\n    - destination:\n        host: web-v1-service\n        port:\n          number: 80\n      weight: 90\n    - destination:\n        host: web-v2-service\n        port:\n          number: 80\n      weight: 10\n</code></pre> <p>Apply this new configuration:</p> <pre><code>kubectl apply -f web-gateway-virtualservice.yaml\n</code></pre> <p>You'll see the the <code>Gateway</code> remains unchanged, but the <code>VirtualService</code> configuration has been updated:</p> <pre><code>% kubectl apply -f web-gateway-virtualservice.yaml\ngateway.networking.istio.io/web-gateway unchanged\nvirtualservice.networking.istio.io/web-virtualservice configured\n</code></pre> <p>Notice the new <code>- destination</code> section and the <code>weight</code> values.</p> <p>As you may have guessed, we are sending 90% traffic to <code>v1</code> and 10% to <code>v2</code>.</p> <p>Keep refreshing your page and you'll eventually see <code>v2</code>:</p> <p></p>"},{"location":"canary-releases-istio/#adjust-weights","title":"Adjust Weights","text":"<p>Alter the weights and ultimately try setting <code>v1</code> weight to <code>0</code> and <code>v2</code> weight to <code>100</code>. At that point, you've fully migrated to version 2 of your application.</p>"},{"location":"canary-releases-istio/#conclusion","title":"Conclusion","text":"<p>This is powerful stuff. Link this into a delivery pipeline, automation workflow &amp; throw in decent monitoring and you have a formidable capability.</p> <p>Imagine gradually adjusting the weighting, monitoring the results (failure rate &amp; response time being just two metrics), then automatically increasing the weighting or automating the rollback to a previous version.</p>"},{"location":"chatgpt-write-article-about-keptn/","title":"ChatGPT: Write me an Article about Keptn","text":"<p>I asked ChatGPT to write an article about Keptn...</p>"},{"location":"chatgpt-write-article-about-keptn/#do-it-for-me-chatgpt","title":"Do it for me ChatGPT...","text":"<p>Here is ChatGPT's response:</p> <p>The world of software development is constantly evolving, and with it, the tools and processes that teams use to deliver software. One of the most promising developments in recent years has been the rise of cloud-native technologies such as Kubernetes and Istio, which have enabled teams to build, deploy and operate their applications in a more efficient and scalable way. However, as teams adopt these new technologies, they also face new challenges when it comes to automating their software delivery process.</p> <p>This is where Keptn comes in. Keptn is an open-source project that provides a platform for continuous delivery and automated operations of cloud-native applications. It enables teams to automate their software delivery process, from development to production, by providing a set of automation capabilities that can be integrated with existing tools and processes.</p> <p>One of the key features of Keptn is its ability to automate the deployment and rollback of applications. This means that teams can deploy new versions of their applications with the click of a button, and quickly roll back to a previous version if something goes wrong. This can save teams a lot of time and effort, and help them to deliver software faster and more reliably.</p> <p>Another important feature of Keptn is its ability to automate testing and validation of deployments. With Keptn, teams can define automated tests that are run automatically after a deployment, ensuring that the new version of the application is working as expected. This can help teams to catch issues early on, before they become a problem in production.</p> <p>Keptn also provides automated remediation of issues, which means that if something goes wrong, the platform can automatically take action to fix the problem. This can save teams a lot of time and effort, and help them to keep their applications running smoothly.</p> <p>In addition to these features, Keptn also provides automated root-cause analysis, which helps teams to quickly identify the cause of an issue. This can be especially helpful in complex systems where the root cause of a problem is not always obvious.</p> <p>Finally, Keptn provides automated monitoring and observability, which enables teams to see how their applications are performing in real-time. This can help teams to identify issues early on, and to make informed decisions about how to improve the performance of their applications.</p> <p>Keptn is built on top of popular open-source projects such as Prometheus, Grafana, and kubectl, and it integrates well with other tools and platforms such as Jenkins, GitHub, and Prometheus. This means that teams can easily integrate Keptn into their existing workflow, making it easy to start automating their software delivery process.</p> <p>In conclusion, Keptn is a powerful open-source platform that can help teams to automate their software delivery process, from development to production. With its ability to automate deployment, testing, remediation, and monitoring, Keptn can help teams to deliver software faster and more reliably, and to keep their applications running smoothly. If you're looking for a way to automate your software delivery process, Keptn is definitely worth checking out.</p>"},{"location":"chatgpt-write-article-about-keptn/#back-to-the-human","title":"Back to the Human","text":"<p>Not bad really, but to me, it doesn't \"feel\" human. Remember at school or uni where you'd look over someone's work and just \"know\" it wasn't them - sure, they hadn't \"plagiarised\" but it the sentences were \"heavily inspired\" by other content and they'd wrapped some nice conjunctions around it to make it feel human.</p> <p>Then again, maybe I'm jealous, all humans are soon to be useless and we're just watching our usefulness clock tick away...</p> <p>Don't get me wrong, we ignore ChatGPT at our peril - it is a gamechanger - we would be foolish to dismiss it outright. The combination of deep fakes, ChatGPT and vocal synthesis is something we simply aren't ready for. The spam and phishing resulting from that will make those \"you have an unpaid Amazon bill\" emails and SMS's feel antiquated.</p> <p>No doubt ChatGPT has lots of smarts and the time-savings can be immense. I just don't think that we are redundant, yet...</p>"},{"location":"cloud-events-eat-the-world/","title":"Why CloudEvents will Eat The World","text":"<p>YAML and JSON have won the war for data exchange formats, but we still haven't standardised on what we say to each other. CloudEvents solve that problem.</p>"},{"location":"cloud-events-eat-the-world/#what-is-said-not-how-it-is-said","title":"What Is Said, Not How It Is Said","text":"<p>Sending messages between tools using JSON is almost standard practice by now. But that is only half the equation. Take these two JSON documents. Each document contains the same information, they say the same thing, but in entirely different ways.</p> <p>Document 1: <pre><code>{\n  \"generator\": \"vendor A\",\n  \"issue\": {\n    \"id\": \"1\",\n    \"impacted\": [{\n      \"system\": \"system 1\",\n      \"application\": \"appA\" \n    }],\n    \"alert_type\": \"email\",\n    \"alert_destination\": \"john.smith@example.com\"\n    \"alert_when\": \"now\"\n  }\n}\n</code></pre></p> <p>Document 2: <pre><code>{\n  \"issue_id\": \"1\",\n  \"systems_involved\": [{\n    \"name\": \"system 1\"\n  }],\n  \"apps_involved\": [{\n    \"name\": \"appA\"\n  }],\n  \"notification\": \"E\",\n  \"notify_endpoint\": \"john.smith@example.com\",\n  \"notify_delay\": 0,\n  \"tool\": \"vendor B\"\n}\n</code></pre></p> <p>We humans have no issue understanding that these are ultimately conveying the same information, but that is harder for a computer to ascertain.</p> <p>If a system wants to consume these alerts, the system needs to understand the content and context of the sender. In essence, the receiving system needs to know how to parse and translate the input into a format usable for its purposes.</p> <p>This problem does not scale well. If I'm writing an alert ingestion system (which can send emails in response to incoming alerts, for example) I need to understand every possible input JSON format and write \"decoders\" for each tool.</p>"},{"location":"cloud-events-eat-the-world/#the-core-problem","title":"The Core Problem","text":"<p>No one has yet sat down and decided: If you want to send an alert (to continue the above example): - Here is how you say it - Here are the fields you must include (eg. the minimal information to make this understandable as an alert for a consuming system) - We don't want to be too draconian and opinionated so here is a space for any custom or vendor-specific fields.</p>"},{"location":"cloud-events-eat-the-world/#cloudevents-a-solution","title":"CloudEvents: A Solution","text":"<p>The CloudEvents specification tackles this exact problem. The specification defines the basic outline of a message:</p> <ul> <li>The fields that MUST be included</li> <li>The fields that MAY be included</li> <li>A \"spare\" area for anything truly vendor specific</li> </ul> <p>Transforming the above event to a cloudevent gives:</p> <pre><code>{\n  \"specversion\": \"1.0\",\n  \"id\": \"1\",\n  \"source\": \"vendor A\",\n  \"type\": \"com.vendora.alert.triggered\",\n  \"data\": {\n    \"issue\": {\n      \"id\": \"1\",\n      \"impacted\": [{\n        \"system\": \"system 1\",\n        \"application\": \"appA\" \n      }],\n      \"alert_type\": \"email\",\n      \"alert_destination\": \"john.smith@example.com\"\n      \"alert_when\": \"now\"\n    }\n  }\n}\n</code></pre> <p>An alert consumer can now easily understand that this is an <code>alert.triggered</code> event and that the <code>source</code> was <code>vendor A</code>. <code>vendor A</code> has included their custom fields inside the <code>data</code> block but even without that data, the consumer still knows this is an alert.</p> <p>CloudEvents define a standard syntax for describing events as they flow around, through and across system boundaries.</p>"},{"location":"cloud-events-eat-the-world/#what-types-of-cloudevent-are-available","title":"What Types of CloudEvent are available?","text":"<p>CloudEvents define the specification, but are deliberately not descriptive on what types of events are created - so you're free to create your standards.</p> <p>Contributors and companies I've seen utilising CloudEvents:</p> <ul> <li>Google</li> <li>Jenkins</li> <li>Keptn</li> <li>Alibaba</li> <li>Confluent</li> <li>Huawei</li> <li>IBM</li> <li>Oracle</li> <li>PayPal</li> <li>SAP</li> <li>RedHat</li> <li>and many many more. You'll be in good company.</li> </ul>"},{"location":"cloud-events-eat-the-world/#use-cloudevents","title":"Use CloudEvents?","text":"<p>Does your product use CloudEvents? Have I missed anything in the article? Want your name linked above? Drop me a line and let me know.</p>"},{"location":"crewai-observability/","title":"CrewAI Observability","text":"<p>In this post &amp; video I'll demonstrate how to get full end-to-end Observability of your multi-agent CrewAI crews using Traceloop OpenLLMetry and the OpenTelemetry collector.</p> <p>Watch</p> <p>If you'd prefer to watch rather than read, I have the following content as a YouTube video:</p> <p></p> <p>CrewAI bills itself as a multi agent framework and orchestration tool.</p> <p>There are two main concepts:</p> <ul> <li>Crews: Loosely think of this as a \"team\" of agents. Each agent has a \"role\" or \"job\" and they work together - much like a human team would, to achieve a goal. The key here is that when each agent in the crew does something is entirely driven by the crew itself. In other words, they handoff between each other.</li> <li>Flows: These are workflows that you design. Being prescriptive workflows, they're much more \"designed\" and therefore deterministic. They are event-driven so again, you control everything about flows.</li> </ul> <p>It's worth saying that the two concepts can interact and work together: Crews can call workflows at defined points and / or flows can trigger crews. So - the marketing text says - you can get the best of both worlds.</p> <p>In either case, Observability of both crews and flows is critical. If you allow a crew to do whatever (and whenever) it likes, without knowing what's happening within the crew, you're really asking for trouble. Likewise, if a (work)flow that you've triggered fails, you need to know which step failed and the reason so that you can fix it.</p> <p>Enter OpenLLMetry and the OpenTelemetry collector.</p> <p>OpenLLMetry is a framework that \"wraps\" calls that go to LLMs and this enables us to deeply observe and monitor what and how the LLMs are interacting with one another, when handoffs occur, what data is being passed back and forth - basically everything happening within the crew. OpenLLMetry is based on OpenTelemetry and this also provides the means to instrument your workflows (aka flows).</p>"},{"location":"crewai-observability/#install-crewai-and-create-a-crew","title":"Install CrewAI and Create a Crew","text":"<p>Start by installing CrewAI following their guide.</p> <p>Now create a CrewAI crew. This is the scaffolding that defines your crew. I'll follow their quickstart guide to create a first crew:</p> <pre><code>crewai create crew my_first_crew\n</code></pre>"},{"location":"crewai-observability/#add-openllmetry","title":"Add OpenLLMetry","text":"<p>Next add traceloop as a dependency in your project. This will trace the goings-on of the LLMs in the crew.</p> <ul> <li>Open this file: <code>my_first_crew/pyproject.toml</code>.</li> <li>In the dependencies section, add a new dependency on <code>\"traceloop-sdk\"</code></li> </ul> <p>It should look something like this:</p> <pre><code>...\n\"dependencies\" = [\n  \"crewai[tools]==1.1.0\",\n  \"traceloop-sdk\"\n]\n...\n</code></pre> <ul> <li>Open <code>src/main.py</code></li> <li>Add the following two lines to the file:</li> </ul> <pre><code>from traceloop.sdk import Traceloop\nTraceloop.init(api_endpoint=\"http://localhost:4318\", app_name=\"crewAI\")\n</code></pre> <p>Finally, install the crewai packages:</p> <pre><code>pip install traceloop-sdk\n</code></pre> <p>Collector</p> <p>The localhost:4318 address above should point to an OpenTelemetry Collector. We haven't spun that up yet, but we will in the following step.</p>"},{"location":"crewai-observability/#configure-and-start-collector","title":"Configure and Start Collector","text":"<p>The OpenTelemetry Collector is the component that the application will send telemetry data to. The collector has lots of uses (I have lots more content on the collector on this blog &amp; my YouTube channel) and is such a useful component, I consider it a mandatory piece of a modern Observability setup. In short, think of it as an ETL pipeline - you can transform, batch, redact, filter and drop data - plus you can transform one signal type to another (eg. take in logs and transform them to metrics).</p> <p>OpenTelemetry Collector</p> <p>The OpenTelemetry collector is a mandatory part of a modern Observability platform.</p> <p>First you'll need to download the collector binary (or run it as a docker container - take your pick). You can download it from here.</p> <p>The collector requires a configuration file on startup. Save this as a YAML file called <code>collector-config.yaml</code>:</p> <pre><code>receivers:\n  otlp:\n    protocols:\n      http:\n        endpoint: localhost:4318\n\nexporters:\n  debug:\n    verbosity: detailed\n\nservice:\n  pipelines:\n    metrics:\n      receivers: [otlp]\n      processors: []\n      exporters: [debug]\n    traces:\n      receivers: [otlp]\n      processors: []\n      exporters: [debug]\n    logs:\n      receivers: [otlp]\n      processors: []\n      exporters: [debug]\n</code></pre> <p>If this is the first time you've seen a collector config, the above may look daunting, but here's what it does:</p> <ul> <li>The top sections define the configuration of the potential components and their configuration</li> <li>The bottom section defines how those components are \"wired\" together (think about the flow of data coming into the collector, how it is processed and how data will leave the collector)</li> <li>Data will be received into the collector via the OTLP protocol (traceloop will send this)</li> <li>Metrics, traces and logs will all be sent out of the collector to the <code>debug</code> exporter (which means the data will be dumped to the collectors stdout)</li> </ul> <p>If you have a \"proper\" Observability backend you can also add additional exporters - just follow the instructions given by your Observability vendor / tool.</p>"},{"location":"crewai-observability/#start-collector","title":"Start Collector","text":"<p>Start the collector (assuming you've downloaded and extracted the <code>contrib</code> collector binary):</p> <pre><code>./otelcol-contrib --config=collector-config.yaml\n</code></pre>"},{"location":"crewai-observability/#start-application","title":"Start Application","text":"<p>The collector is now running and ready to accept telemetry. It is time to start the crew:</p> <pre><code>cd my_first_crew\ncrewai run\n</code></pre>"},{"location":"crewai-observability/#collector-output","title":"Collector Output","text":"<p>Recall that a \"crew\" is a set of agents. Look at <code>my_first_crew/src/my_first_crew/config/agents.yaml</code> to understand which agents you have in your crew (ie. on your team). There are two: A reporting analyst and a senior data researcher. Both are specified to be experts in a certain <code>{topic}</code>. They will work together to produce a final report on the given topic (provided as a variable).</p> <p>The <code>{topic}</code> variable is defined in <code>my_first_crew/src/my_first_crew/main.py</code> in the <code>inputs</code> variable defined in the <code>run</code> function:</p> <pre><code>inputs = {\n        'topic': 'AI LLMs',\n        'current_year': str(datetime.now().year)\n    }\n</code></pre> <p>Think about it, the agents will each \"do something\" then potentially hand off to the other agent. The second agent will \"do something\" then potentially hand back to the first - and so on - until they determine the task is complete. Each of these \"doing something\" tasks will be represented in OpenTelemetry as a single unit of work (a span). All spans will share a common ID which \"ties\" all of this work together (think about all of the work being done against a single JIRA ticket ID).</p> <p>So, back to the collector output. You should see output in the collector like this:</p> <pre><code>Span #0\n    Trace ID       : e838675910b9a2a2ba5b0506220fe163\n    Parent ID      : 0ab44914113b77a6\n    ID             : faa3fa3de19d1f1c\n    Name           : AI LLMs Reporting Analyst\n.agent\n    Kind           : Client\n    Start time     : 2025-10-23 22:42:55.271987 +0000 UTC\n    End time       : 2025-10-23 22:43:17.058684 +0000 UTC\n    Status code    : Ok\n    Status message : \nAttributes:\n     -&gt; traceloop.span.kind: Str(agent)\n     -&gt; crewai.agent.id: Str(1ff74c04-0910-4872-ad6d-0022f18b8d20)\n     -&gt; crewai.agent.role: Str(AI LLMs Reporting Analyst\n)\n     -&gt; crewai.agent.goal: Str(Create detailed reports based on AI LLMs data analysis and research findings\n)\n     -&gt; crewai.agent.backstory: Str(You're a meticulous analyst with a keen eye for detail. You're known for your ability to turn complex data into clear and concise reports, making it easy for others to understand and act on the information you provide.)\n     -&gt; crewai.agent.cache: Str(True)\n     -&gt; crewai.agent.verbose: Str(True)\n     -&gt; crewai.agent.allow_delegation: Str(False)\n     -&gt; crewai.agent.tools: Str([])\n     -&gt; crewai.agent.max_iter: Str(25)\n     -&gt; crewai.agent.agent_executor: Str(&lt;crewai.agents.crew_agent_executor.CrewAgentExecutor object at 0x112669810&gt;)\n     -&gt; crewai.agent.llm: Str(&lt;crewai.llms.providers.openai.completion.OpenAICompletion object at 0x112651d10&gt;)\n     -&gt; crewai.agent.crew: Str(parent_flow=None name='crew' cache=True tasks=[Task(description=Conduct a thorough research about AI LLMs Make sure you find any interesting and relevant information given the current year is 2025.\n, expected_output=A list with 10 bullet points of the most relevant information about AI LLMs\n), Task(description=Review the context you got and expand each topic into a full section for a report. Make sure the report is detailed and contains any and all relevant information.\n, expected_output=A fully fledged report with the main topics, each with a full section of information. Formatted as markdown without '```'\n)] agents=[Agent(role=AI LLMs Senior Data Researcher\n, goal=Uncover cutting-edge developments in AI LLMs\n, backstory=You're a seasoned researcher with a knack for uncovering the latest developments in AI LLMs. Known for your ability to find the most relevant information and present it in a clear and concise manner.\n), Agent(role=AI LLMs Reporting Analyst\n, goal=Create detailed reports based on AI LLMs data analysis and research findings\n, backstory=You're a meticulous analyst with a keen eye for detail. You're known for your ability to turn complex data into clear and concise reports, making it easy for others to understand and act on the information you provide.)] process=&lt;Process.sequential: 'sequential'&gt; verbose=True memory=False short_term_memory=None long_term_memory=None entity_memory=None external_memory=None embedder=None usage_metrics=None manager_llm=None manager_agent=None function_calling_llm=None config=None id=UUID('b52329b4-2247-421b-b868-3274d932741a') share_crew=False step_callback=None task_callback=None before_kickoff_callbacks=[] after_kickoff_callbacks=[&lt;function crew.&lt;locals&gt;.wrapper.&lt;locals&gt;.callback_wrapper.&lt;locals&gt;.bound_callback at 0x102cd87c0&gt;] max_rpm=None prompt_file=None output_log_file=None planning=False planning_llm=None task_execution_output_json_files=None execution_logs=[] knowledge_sources=None chat_llm=None knowledge=None security_config=SecurityConfig(fingerprint=Fingerprint(metadata={})) token_usage=None tracing=False)\n     -&gt; crewai.agent.i18n: Str(prompt_file=None)\n     -&gt; crewai.agent.tools_handler: Str(&lt;crewai.agents.tools_handler.ToolsHandler object at 0x1125c2310&gt;)\n     -&gt; crewai.agent.tools_results: Str([])\n     -&gt; crewai.agent.security_config: Str(fingerprint=Fingerprint(metadata={}))\n     -&gt; crewai.agent.callbacks: Str([])\n     -&gt; crewai.agent.adapted_agent: Str(False)\n     -&gt; crewai.agent.use_system_prompt: Str(True)\n     -&gt; crewai.agent.allow_code_execution: Str(False)\n     -&gt; crewai.agent.respect_context_window: Str(True)\n     -&gt; crewai.agent.max_retry_limit: Str(2)\n     -&gt; crewai.agent.multimodal: Str(False)\n     -&gt; crewai.agent.inject_date: Str(False)\n     -&gt; crewai.agent.date_format: Str(%Y-%m-%d)\n     -&gt; crewai.agent.code_execution_mode: Str(safe)\n     -&gt; crewai.agent.reasoning: Str(False)\n     -&gt; crewai.agent.guardrail_max_retries: Str(3)\n     -&gt; gen_ai.request.model: Str(gpt-4)\n     -&gt; gen_ai.response.model: Str(gpt-4)\n</code></pre> <p>This is the textual representation of a single span. It is a single \"task\" being done and, as you can see, you have all of the information about this task. The inputs, parameters, attributes and outputs.</p> <p>This span represents one of the blue bars in the diagram below.</p> <p></p> <p>Look more closely at the collector output. Notice that all spans share the same trace ID. This is how the Observability system understands that all of these spans are part of the same \"unit of work\" and thus visualise them together as you see above.</p> <pre><code>Span #0\n    Trace ID       : e838675910b9a2a2ba5b0506220fe163\nSpan #1\n    Trace ID       : e838675910b9a2a2ba5b0506220fe163\n...\n</code></pre>"},{"location":"crewai-observability/#metrics","title":"Metrics","text":"<p>OpenLLMetry also emits metrics and you can see these in teh collector logs too. In this case, a histogram of token usage.</p> <pre><code>ScopeMetrics #1\nScopeMetrics SchemaURL: \nInstrumentationScope opentelemetry.instrumentation.crewai.instrumentation 0.36.0\nMetric #0\nDescriptor:\n     -&gt; Name: gen_ai.client.token.usage\n     -&gt; Description: Measures number of input and output tokens used\n     -&gt; Unit: token\n     -&gt; DataType: Histogram\n     -&gt; AggregationTemporality: Cumulative\nHistogramDataPoints #0\nData point attributes:\n     -&gt; gen_ai.system: Str(crewai)\n     -&gt; gen_ai.token.type: Str(input)\n     -&gt; gen_ai.response.model: Str(gpt-4)\nStartTimestamp: 2025-10-23 22:42:55.267706 +0000 UTC\nTimestamp: 2025-10-23 22:43:17.074056 +0000 UTC\nCount: 2\nSum: 0.000000\nMin: 0.000000\nMax: 0.000000\nExplicitBounds #0: 1.000000\nExplicitBounds #1: 4.000000\nExplicitBounds #2: 16.000000\nExplicitBounds #3: 64.000000\nExplicitBounds #4: 256.000000\nExplicitBounds #5: 1024.000000\nExplicitBounds #6: 4096.000000\nExplicitBounds #7: 16384.000000\nExplicitBounds #8: 65536.000000\nExplicitBounds #9: 262144.000000\nExplicitBounds #10: 1048576.000000\nExplicitBounds #11: 4194304.000000\nExplicitBounds #12: 16777216.000000\nExplicitBounds #13: 67108864.000000\nBuckets #0, Count: 2\nBuckets #1, Count: 0\nBuckets #2, Count: 0\nBuckets #3, Count: 0\nBuckets #4, Count: 0\nBuckets #5, Count: 0\nBuckets #6, Count: 0\nBuckets #7, Count: 0\nBuckets #8, Count: 0\nBuckets #9, Count: 0\nBuckets #10, Count: 0\nBuckets #11, Count: 0\nBuckets #12, Count: 0\nBuckets #13, Count: 0\nBuckets #14, Count: 0\nExemplars:\nExemplar #0\n     -&gt; Trace ID: e838675910b9a2a2ba5b0506220fe163\n     -&gt; Span ID: faa3fa3de19d1f1c\n     -&gt; Timestamp: 2025-10-23 22:43:17.058611 +0000 UTC\n     -&gt; Value: 0\n</code></pre> <p>Limits of OpenLLMetry</p> <p>OpenLLMetry only instruments the calls to/from AI providers.</p> <p>You will still need either auto-instrumentation, manual instrumentation or a vendor's agent to fully instrument the remainder of your application logic (eg. calls to / from APIs or other systems).</p>"},{"location":"crewai-observability/#summary","title":"Summary","text":"<p>CrewAI is a relatively easy way to build teams of agents that collaborate towards a common goal.</p> <p>Adding OpenLLMetry provides the essential visibility into how, why and when they're doing work. Plus, it enables you to track the all important key metrics like token usage - so you don't get a shock on your AI bill!</p>"},{"location":"csv-log-enrichment/","title":"Enrich Logs using CSV Files and the OpenTelemetry Collector","text":"<p>You can enrich your metrics, logs and traces in REALTIME using CSV data and the OpenTelemetry collector. This video shows you how to build a custom collector using the OpenTelemetry Collector Builder (ocb) and how to process, match and enrich log lines with CSV data.</p>"},{"location":"dive-summary/","title":"Dive: Explore and Optimise your Container Images","text":"<p>This is a text-based version of this video so if you prefer reading to watching, read on!</p> <p>Want to jump straight to the Dive hands-on? </p>"},{"location":"dive-summary/#thank-you-to-dnsmichi","title":"Thank you to @dnsmichi","text":"<p>Thank you to @dnsmichi for posting about this tool. Go give him a follow on Masterdon, LinkedIn or your platform of choice.</p> <ul> <li>Apologies about the pronounciation. It's \"mee-ch-ee\" \"free-dree-ck\" - so I got that wrong within 30 seconds. Sorry!</li> </ul>"},{"location":"dive-summary/#the-dive-tool","title":"The Dive Tool","text":"<p>Dive is a tool for exploring and optimising your OCI container images (works with Docker too).</p> <p>The tool will, layer by layer, analyse a given image and provide a report of where the space is used and thus potentially what you could remove to make it smaller.</p> <p>Shrinking images has many advantages:</p> <ul> <li>Faster build times</li> <li>Faster download and startup times</li> <li>Lower storage cost</li> <li>Potentially a more secure image (anything not in the image is one less thing that can be become vulnerable and hacked)</li> </ul>"},{"location":"dive-summary/#thresholds","title":"Thresholds","text":"<p>Dive can also be executed in <code>ci</code> mode whereby you provide some acceptable thresholds and the result will be a failure if those numbers aren't met. In this way, you can use Dive as a basic quality gate in your build process to prevent large images from creeping into production.</p>"},{"location":"dive-summary/#dive-hands-on","title":"Dive Hands On","text":"<p>If this sounds good, go and get hands-on with Dive in your browser with nothing to install - then check out the official repository.</p>"},{"location":"dive-summary/#new-project-each-week","title":"New Project Each Week","text":"<p>Each week in 2023 I will pick a new open source project to review. The entire list is here.</p> <p>The series is designed to be interactive - if you have a project you'd like me to review - please do get in contact and suggest a project!</p> <p>I hope you enjoy Dive!</p>"},{"location":"docker-export-merge-kubernetes-contexts/","title":"Export Kube Config From Docker Container and Merge","text":"<p>How to export a kubernetes config context file from inside a docker container, to the host and merge.</p>"},{"location":"docker-export-merge-kubernetes-contexts/#introduction","title":"Introduction","text":"<p>If you use Docker in Docker (<code>dind</code>) or Kubernetes in Docker (<code>kind</code>) you will end up with a <code>kubectl</code> running inside a docker container but you probably also want to interact using <code>kubectl</code> from localhost.</p>"},{"location":"docker-export-merge-kubernetes-contexts/#export-contexts","title":"Export Contexts","text":"<p>Use the <code>docker cp</code> command to copy the kube config file from the container to the localmachine:</p> <p><pre><code>docker cp YourContainerName:/UsernameTheContainerIsUsing/.kube/config ./config\n</code></pre> For example:</p> <pre><code>docker cp mycontainer:/root/.kube/config ./config\n</code></pre> <p>Ensure the config points to the correct endpoint.  Modify the <code>config</code> file and adjust the <code>server</code> field. You might have:</p> <pre><code>server: https://host.docker.internal:39155\n</code></pre> <p>Whereas <code>host.docker.internal</code> won't resolve from the host machine so that needs to be:</p> <pre><code>server: https://127.0.0.1:39155\n</code></pre> <p>Trial and error is probably required here.</p>"},{"location":"docker-export-merge-kubernetes-contexts/#merge-contexts","title":"Merge Contexts","text":"<p>Now merge any existing <code>kubectl</code> config file and this new one. The syntax is:</p> <pre><code>export KUBECONFIG=ExistingFile:NewFile\n</code></pre> <p>For example:</p> <pre><code>export KUBECONFIG=~/.kube/config:~/config\n</code></pre> <p>Finally use kubectl to merge and flatten the two configs:</p> <pre><code>kubectl config view --merge --flatten &gt; ./mergedconfigs &amp;&amp; mv ./mergedconfigs ~/.kube/config\n</code></pre> <p>Verify that configs are merged:</p> <pre><code>kubectl config get-contexts\n</code></pre> <p>For example:</p> <pre><code>CURRENT   NAME             CLUSTER          AUTHINFO            NAMESPACE\n*         docker-desktop   docker-desktop   docker-desktop      \n          k3d-mykeptn      k3d-mykeptn      admin@k3d-mykeptn  \n</code></pre>"},{"location":"docker-export-merge-kubernetes-contexts/#try-it-out","title":"Try It Out","text":"<p>Switch to your new context: <pre><code>kubectl config use-context k3d-mykeptn\n</code></pre></p> <p>Now try it. From your localhost:</p> <pre><code>kubectl get ns --context=YourNameContext\n</code></pre>"},{"location":"dynatrace-apache-solr-monitoring/","title":"Dynatrace Apache SOLR Monitoring in Minutes","text":"<p>Legacy Product</p> <p>This tutorial refers to the legacy application monitoring product from Dynatrace. Not the Dynatrace platform.</p> <p>Having Apache Solr performance issues or just want to make sure it keeps running like a dream? Let\u2019s get your Solr instances monitored in minutes with Dynatrace Application Monitoring.</p> <p>I promise, this will take less than 15 minutes.</p>"},{"location":"dynatrace-apache-solr-monitoring/#prerequisites","title":"Prerequisites","text":"<ol> <li>A licensed Dynatrace AppMon server &amp; collector.</li> <li>Apache Solr installed.</li> </ol>"},{"location":"dynatrace-apache-solr-monitoring/#lets-go","title":"Let's Go","text":"<ol> <li>Download the Apache Solr Fastpack from the Dynatrace Community.</li> <li>Upload the <code>.dtp</code> fastpack file, via the client, to the Dynatrace server.</li> </ol>"},{"location":"dynatrace-apache-solr-monitoring/#install-web-server-agent","title":"Install Web Server Agent","text":"<p>The following instructions simply specify how to install the standard Dynatrace web server agent. Skip if you\u2019ve already got the agent installed on the Solr box.</p> <ol> <li>Download the Dynatrace agent JAR file to the /tmp directory of your Solr server.</li> </ol> <pre><code>cd /tmp\nwget https://files.dynatrace.com/downloads/OnPrem/dynaTrace/6.5/6.5.0.1289/dynatrace-wsagent-6.5.0.1289-linux-x86-64.tar\n</code></pre> <ol> <li>Extract the agent, copy the <code>.sh</code> file to <code>/opt</code> and execute.</li> </ol> <pre><code>sudo tar -xvf dynatrace-wsagent-6.5.0.1289-linux-x86-64.tar\nsudo cp dynatrace-wsagent-6.5.0.1289-linux-x64.sh /opt\ncd /opt\nsudo ./dynatrace-wsagent-6.5.0.1289-linux-x64.sh\n</code></pre> <p></p> <ol> <li>Add the Dynatrace master agent to the init.d startup scripts &amp; add to automatic startup.</li> </ol> <pre><code>sudo cp /opt/dynatrace-6.5/init.d/dynaTraceWebServerAgent /etc/init.d\ncd /etc/init.d\nsudo update-rc.d dynaTraceWebServerAgent defaults\n</code></pre> <ol> <li>Edit the master web server agent config to have the correct name (starts with <code>SOLR</code>) and ensure it points to your collector. Then start the <code>dynaTraceWebServerAgent</code> service.</li> </ol> <pre><code>sudo nano /opt/dynatrace-6.5/agent/conf/dtwsagent.ini\n// CHANGE NAME PARAMETER TO BE SOLR_AGENT\n// CHANGE SERVER PARAMETER TO POINT TO THE COLLECTOR IP ADDRESS\nservice dynaTraceWebServerAgent start\n</code></pre> <p>If it worked, great! We\u2019re halfway there! You should see this in the Agent Overview dashlet.</p> <p></p>"},{"location":"dynatrace-apache-solr-monitoring/#instrument-solr","title":"Instrument SOLR","text":"<p>Good news \u2013 this bit is really easy!</p> <p>Open the <code>/opt/solr-6.5.0/bin/solr.in.sh</code> file (or <code>solr.in.cmd</code> on Windows) and add the following line (be sure to substitute your collector IP rather than my <code>192.168.1.1</code> address!)</p> <p></p> <pre><code>sudo nano /opt/solr-6.5.0/bin/solr.in.sh\n// ADD FOLLOWING LINE ABOVE THE OTHER SOLR_OPTS LINES\nSOLR_OPTS=\u201d$SOLR_OPTS -agentpath:/opt/dynatrace-6.5/agent/lib64/libdtagent.so=name=Solr_Agent,collector=192.167.1.1:9998\u2033\n</code></pre>"},{"location":"dynatrace-apache-solr-monitoring/#start-solr","title":"Start SOLR","text":"<p>Run <code>/opt/solr-6.5.0/bin/solr start</code> - You should see both agents in the Agents Overview, the preconfigured dashboards will work and you\u2019ll have Purepaths flowing into the system.</p> <p></p>"},{"location":"dynatrace-basics-creating-measures/","title":"Dynatrace Basics: Creating Measures","text":"<p>Legacy Product</p> <p>This tutorial refers to the legacy application monitoring product from Dynatrace. Not the Dynatrace platform.</p> <p>This tutorial will how to setup long term trending of your applications and key transactions in Dynatrace AppMon. It will use a concept called measures.</p>"},{"location":"dynatrace-basics-creating-measures/#what-is-a-measure","title":"What is a Measure?","text":"<p>Put simply, it\u2019s a piece of your application that you wish to measure. Dynatrace will automatically create Purepaths for every transaction (Dynatrace does not sample data) but for long term trending, you\u2019ll want to explicitly set up a measure on the transaction. Once done, the data will be stored forever in the Dynatrace database (performance warehouse).</p> <p>Measures are the way to track long term trends in metrics.</p>"},{"location":"dynatrace-basics-creating-measures/#the-business-requirement","title":"The Business Requirement","text":"<p>Imagine that the business has given us 3 requirements:</p> <ol> <li>Track the response time of the batch job.</li> <li>Track the response time of the batch job when the batch type is Type B.</li> <li>Track the response time of the batch job per execution type (ie. response time of Type A vs. Type B).</li> </ol>"},{"location":"dynatrace-basics-creating-measures/#demo-code","title":"Demo Code","text":"<p>Here\u2019s the demo JAR file we\u2019ll be using in this tutorial (and here\u2019s the source code on Github if you want it).</p> <p>The JAR file randomly calculates a boolean and if it\u2019s true, the batch is said to be \u201cType A\u201d. If false, the batch is \u201cType B\u201d. If the batch is \u201cType B\u201d, the process pauses for an extra 3 seconds (the reason for this will become clear later).</p> <pre><code>/* Randomly pick A or B\n* Arbitrarily, if true it\u2019s \u201cType A\u201d if false it\u2019s \u201cType B\u201d\n* Note: We use Strings to make it clearer to the reader.\n* Dynatrace AppMon works perfectly well with booleans or numbers or anything else.\n*/\nString strBatchType = \"\";\nRandom oRandom = new Random();\nboolean bSwitch = oRandom.nextBoolean();\n\nif (bSwitch) strBatchType = \"Type A\";\nelse strBatchType = \"Type B\";\n\nstartBatch(strBatchType);\nThread.sleep(2000);\n\n// Fake an issue when Batch Type is \u201cType B\u201d, pause for 3s.\nprivate static void doThis(String strBatchType)\n{\n  System.out.println(\"Now doing this...\");\n  if (strBatchType.equals(\"Type B\"))\n\ntry\n{\n  Thread.sleep(3000);\n}\ncatch (Exception e){}\n\nSystem.out.println(\"---\");\n}\n</code></pre> <p>First, let\u2019s run the JAR without Dynatrace to prove it works: <code>java -jar DTBasicsCreatingMeasures.jar</code></p> <p>Now instrument it with the <code>-agentpath</code> parameter you received when you setup your agent tier.</p> <p>No idea what I\u2019m talking about? Read this tutorial to understand how to instrument your JAR.</p>"},{"location":"dynatrace-basics-creating-measures/#sensor-configuration","title":"Sensor Configuration","text":"<p>You should now have your agent connected (view the Agent Overview dashlet to verify) but you won\u2019t be getting any Purepaths yet. Let\u2019s correct that now.</p> <ol> <li> <p>Open the system profile &gt; sensors and create a new sensor group. Called BatchStartPurepath (the name is arbitrary but it\u2019ll help you understand what\u2019s going on later).</p> </li> <li> <p>Make sure your sensor group is deployed to the relevant agent group. Next, in the sensor configuration, make sure you\u2019ve set the sensor pack to Active and Start Purepaths.</p> </li> </ol> <p></p> <ol> <li>Restart the JAR file and open the Purepaths dashlet. You should now see purepaths of varying lengths.</li> </ol> <p></p>"},{"location":"dynatrace-basics-creating-measures/#satisfy-business-requirement-1","title":"Satisfy Business Requirement #1","text":"<p>To track the overall response time, simply create a measure on the purepath as a whole.</p> <ol> <li>Right click the first purepath node (in the Purepath Tree view). Select Create Measure.</li> <li>Choose the Time measure and give it a good name.</li> <li>Click OK.</li> </ol> <p></p> <p>Your batch job is now being monitored and the data is being saved into the Performance Warehouse Database for long term trending.</p>"},{"location":"dynatrace-basics-creating-measures/#viewing-the-results-charting","title":"Viewing the Results - Charting","text":"<ol> <li>Press Ctrl + N to create a new dashboard. Select Use for Analysis.</li> <li>Add a Chart dashlet.</li> <li>In the measure dashlet, search for (hint: <code>Ctrl + F</code>) the Batch Job Response Time measure.</li> <li>Chart it.</li> </ol> <ol> <li>Hit the refresh icon a few times (the circular arrow highlighted below) and you\u2019ll see data start to flow into the system.</li> </ol> <p>For business requirements #2 and #3 we\u2019ll need to introduce a new concept called Business Transactions. Stay tuned for the Business Transactions tutorial.</p>"},{"location":"dynatrace-business-transactions/","title":"Dynatrace Basics: Business Transactions","text":"<p>Legacy Product</p> <p>This tutorial refers to the legacy application monitoring product from Dynatrace. Not the Dynatrace platform.</p> <p>As a follow up to the previous Creating Measures tutorial, let\u2019s introduce another key Dynatrace Application Monitoring (AppMon) concept: Business Transactions.</p> <p>In the last tutorial, we saw how to create basic datapoints for long term trending purposes. These are perfect as they\u2019re extremely low overhead in terms of the processing and storage requirements on the AppMon system. However, they do have limitations.</p> <p>A Business Transaction is a way to combine multiple measures to form more complex conditions.</p>"},{"location":"dynatrace-business-transactions/#measures-vs-business-transactions","title":"Measures vs. Business Transactions","text":"Measures Business Transactions (BTs) Simple More Complex Low Overhead Higher Overhead Purepath Data: Not Linked Purepath Data: Linked Handles Simple Logic Handles Complex Logic. <p>Measures are great for simple business requirements such as response time or SLA tracking. Business Transactions are the best fit for more complex business-logic modelling.</p> <p>Let\u2019s make the above statement a bit more concrete. Here are some scenarios and an indication of which measurement concept I\u2019d use.</p> <ul> <li>Response time of the Contact Page \u2013 Measure</li> <li>Response time of the \u201cClick on Login\u201d button \u2013 Measure</li> <li>404 Count for the About Us page \u2013 Measure</li> <li>Count exception for net.agardner.ConnectionException \u2013 Measure</li> <li>Response time of contact page when browser is Chrome and the user\u2019s country is France \u2013 Business Transaction</li> <li>Response time of a batch job. Report the response times separately per batch job type \u2013 Business Transaction</li> <li>Response time of category pages in an eCommerce store, reported separately per category (eg. <code>/category/bags</code> vs. <code>/category/shoes</code>) \u2013 Business Transaction</li> </ul> <p>If you ever need to ultimately drill down to the method level data (Purepath data), you must use a business transaction.</p>"},{"location":"dynatrace-business-transactions/#business-requirements","title":"Business Requirements","text":"<p>So, let\u2019s satisfy the two outstanding requirements from the previous tutorial.</p> <ol> <li>Track the response time of the batch job when the batch type is Type B.</li> <li>Track the response time of the batch job per execution type (ie. response time of Type A vs. Type B).</li> </ol>"},{"location":"dynatrace-business-transactions/#the-solution","title":"The Solution","text":"<p>We can actually tackle both of these requirements with a single solution: Satisfy #2 and ignoring Type A results when those results are not needed.</p> <p></p>"},{"location":"dynatrace-business-transactions/#implementing-solution-capturing-batch-job-type","title":"Implementing Solution \u2013 Capturing Batch Job Type","text":"<p>Following on from the previous tutorial, start your instrumented JAR file. You should already have the <code>startBatch</code> sensor placed and be seeing purepaths.</p> <p>Edit the system profile &gt; Sensors &gt; BatchStartPP</p> <p>Double click the startBatch method and enable the method capture tickbox. Kill the JAR file and restart it. This will tell AppMon to grab the actual string value as the code is executing.</p> <p></p> <p>Finally, let\u2019s create a measure which will capture the String value and push the data into the Performance Warehouse. We\u2019ll need this later \u2013 don\u2019t worry, it\u2019ll become clear!</p> <p>Open the purepaths view, right click the top node (line) in Purepath Tree and select Create Measure</p> <p></p> <p>Choose the Methods \u2013 Argument Value method type and give it a good, memorable name (overwrite the default shown here).</p> <p></p>"},{"location":"dynatrace-business-transactions/#building-the-business-transaction","title":"Building The Business Transaction","text":"<p>Time to build the business transaction which will result in the statistics split by the batch type:</p> <p></p> <p>Right click the Business Transactions cockpit item and choose New Business Transaction</p> <p></p> <p>I\u2019ll explain what each of these settings mean in a future post. For now, add the measure you\u2019ve just created to the Splitting section.</p> <p></p> <p>Now refresh the client (circular arrow at the top of the screen) and you\u2019ll see your business transaction appear in the cockpit (left hand panel). Double click the BT and you\u2019ll see something similar to this:</p> <p></p> <p>Congratulations! You\u2019ve done it. You can now use these results in charts and in a future post, I\u2019ll explain how to enable alerting based on the separate Batch job types (nobody likes false positives, do they?)</p>"},{"location":"dynatrace-host-groups/","title":"Dynatrace Host Groups","text":"<p>Host groups are a powerful concept in Dynatrace. This tutorial shows how to utilise host groupings to properly define and baseline a set of Apache services.  </p>"},{"location":"dynatrace-host-groups/#architecture-deployment-reference","title":"Architecture / Deployment Reference","text":"<p>First, let\u2019s describe the basic architecture of what we\u2019re deploying. Assume we have a set of hosts all running Apache HTTPD processes. These hosts serve 3 distinct functions within our estate:</p> <ul> <li>One set of hosts is a set of inbound proxies.</li> <li>Another is a set of outbound proxies.</li> <li>The final group of hosts are reverse proxies.</li> </ul> <p>Logically, we need to separate these hosts by their function \u2013 we don\u2019t want Dynatrace to group them all together. More crucially, we want Dynatrace to baseline and alert on them seperately.</p> <p>The official documentation is great, so I\u2019m not going to repeat things, aside from a few keys points:</p> <p>Host groups are defined when the OneAgent is installed. Hosts can only be a member of a single host group. Host groups form the boundary for process groups and services.</p>"},{"location":"dynatrace-host-groups/#why-are-host-groups-necessary","title":"Why Are Host Groups Necessary?","text":"<p>Host groups are optional but should be considered necessary for an optimal deployment. Utilising host groups should be considered a best practice.</p> <p>Due to the way Dynatrace creates process groups. By default, Dynatrace will see that the hosts are all running Apache HTTPD servers and thus logically group them into a single process group.</p> <p></p> <p>We can use the 3rd fact above to split these hosts into different process groups, based on their host group.</p>"},{"location":"dynatrace-host-groups/#setup","title":"Setup","text":"<p>Spin up 3x VMs and install the latest Apache <code>httpd</code> on them:</p> <pre><code>sudo yum install httpd24 php72 -y\n</code></pre> <p>Do not start the Apache process yet!</p> <p>I\u2019m assuming you have a Dynatrace environment. If not, get a free 15 day trial here.</p>"},{"location":"dynatrace-host-groups/#inbound-proxy-server","title":"Inbound Proxy Server","text":"<p>Install the Dynatrace OneAgent but be sure to append the <code>--set-host-group=inbound-proxy</code> to your shell script installation.</p> <pre><code>sudo /bin/sh Dynatrace-OneAgent...sh APP_LOG_CONTENT_ACCESS=1 --set-host-group=inbound-proxy\n</code></pre>"},{"location":"dynatrace-host-groups/#outbound-proxy-server","title":"Outbound Proxy Server","text":"<p>Repeat for your outbound proxy server, appending <code>--set-host-group=outbound-proxy</code></p> <pre><code>sudo /bin/sh Dynatrace-OneAgent...sh APP_LOG_CONTENT_ACCESS=1 --set-host-group=outbound-proxy\n</code></pre>"},{"location":"dynatrace-host-groups/#reverse-proxy-server","title":"Reverse Proxy Server","text":"<p>Finally for your reverse proxy server, appending <code>--set-host-group=reverse-proxy</code></p> <pre><code>sudo /bin/sh Dynatrace-OneAgent...sh APP_LOG_CONTENT_ACCESS=1 --set-host-group=reverse-proxy\n</code></pre>"},{"location":"dynatrace-host-groups/#process-group-host-group","title":"Process Group &amp; Host Group","text":"<p>Due to the host groups, once the Apache processes are started, you will have 3 distinct groups. However they\u2019ll all be called the same default name.</p> <p>Make things easy for yourself and append the host group name to the process group. You\u2019ll be able to instantly tell which set of processes is having issues.</p> <p>Do this via a process group naming rule:</p> <p></p>"},{"location":"dynatrace-host-groups/#start-your-processes","title":"Start Your Processes","text":"<p>Start your apache processes on each box. You\u2019ll see that you have 3 distinct process groups:</p> <ul> <li>inbound-proxy - Apache Web Server httpd</li> <li>outbound-proxy - Apache Web Server httpd</li> <li>reverse-proxy - Apache Web Server httpd</li> </ul> <p></p>"},{"location":"dynatrace-host-groups/#conclusion","title":"Conclusion","text":"<p>As already mentioned, host groups are extremely powerful and this use case is just one example. Stay tuned for more use cases...</p>"},{"location":"dynatrace-oneagent-same-binary-different-tenants/","title":"Dynatrace OneAgent - Same Binary, Different Tenants","text":"<p>Can we use the same Dynatrace OneAgent installer in two different environments? That was the question posed by a customer recently.</p> <p>Let\u2019s see how it\u2019s done...</p>"},{"location":"dynatrace-oneagent-same-binary-different-tenants/#prerequisites","title":"Prerequisites","text":"<ul> <li>A Dynatrace account. Sign up for a free 15 day trial here.</li> </ul>"},{"location":"dynatrace-oneagent-same-binary-different-tenants/#why","title":"Why?","text":"<p>By default, the OneAgent installer you download from your tenant is predefined to point to that tenant. In most cases this works perfectly and is the best way to install the OneAgent (lowest effort).</p> <p>However, your organisational policy or workflow may demand that you use the same OneAgent installer for two (or more tenants). One example is using the same binary in a pipeline where the OneAgent will point at a preproduction tenant or a production tenant.</p>"},{"location":"dynatrace-oneagent-same-binary-different-tenants/#how","title":"How?","text":"<p>The default OneAgent install commands look like this:</p> <pre><code>wget -O Dynatrace-OneAgent-Linux-VERSION.sh \"https://abc123.live.dynatrace.com/api/v1/deployment/installer/agent/unix/default/latest?Api-Token=def456&amp;amp;arch=x86&amp;amp;flavor=default\nsudo /bin/sh Dynatrace-OneAgent-Linux-VERSION.sh APP_LOG_CONTENT_ACCESS=1\n</code></pre> <p>To reconfigure the OneAgent, there are 3 additional (mandatory) parameters to pass during the install. <code>SERVER</code>, <code>TENANT</code> and <code>TENANT_TOKEN</code>.</p> <ul> <li>The <code>SERVER</code> variable is the full URL to the tenant eg. <code>https://abc123.live.dynatrace.com</code></li> <li>The <code>TENANT</code> variable is the tenant part of the URL eg. <code>abc123</code></li> <li>The <code>TENANT_TOKEN</code> is a variable which should be considered secret and private. Encrypt as you would with any other password.</li> </ul>"},{"location":"dynatrace-oneagent-same-binary-different-tenants/#retrieve-your-tenant_token","title":"Retrieve Your TENANT_TOKEN","text":"<p>A REST call is necessary to retrieve your <code>TENANT_TOKEN</code>.</p> <p><code>GET https://TENANT.live.dynatrace.com/api/v1/deployment/installer/agent/connectioninfo?Api-Token=YOURAPITOKEN</code></p> <p>The <code>Api-Token</code> needs to have <code>InstallerDownload</code> permission.</p> <p>To generate one, go to Settings &gt; Integration &gt; Platform as a Service</p>"},{"location":"dynatrace-oneagent-same-binary-different-tenants/#put-it-all-together","title":"Put It All Together","text":"<p>So, assuming the following:</p> <pre><code>SERVER=https://abc123.live.dynatrace.com\nTENANT=abc123\nTENANT_TOKEN=9876hjk\n</code></pre> <p>All you need to do is append them to the installer line:</p> <pre><code>wget -O Dynatrace-OneAgent-Linux-VERSION.sh \"https://abc123.live.dynatrace.com/api/v1/deployment/installer/agent/unix/default/latest?Api-Token=def456&amp;amp;arch=x86&amp;amp;flavor=default\nsudo /bin/sh Dynatrace-OneAgent-Linux-VERSION.sh SERVER=https://abc123.live.dynatrace.com TENANT=abc123 TENANT_TOKEN=9876hjk APP_LOG_CONTENT_ACCESS=1\n</code></pre>"},{"location":"dynatrace-releases/","title":"Using Dynatrace Releases","text":"<p>This post will cover how to use the Dynatrace releases functionality...</p>"},{"location":"dynatrace-releases/#assumptions","title":"Assumptions","text":"<ul> <li>You have a Dynatrace environment available</li> <li>You have a host (or Kubernetes cluster) with a OneAgent already installed</li> </ul>"},{"location":"dynatrace-releases/#dynatrace-release-tracking","title":"Dynatrace Release Tracking","text":"<p>Using the Dynatrace release functionality is extremely easy. The method differs slightly whether you're using a VM based deployment for your application or Kubernetes.</p> <p>Fundamentally though, there are three things you must set:</p> <ol> <li>The product eg. <code>frontend</code></li> <li>The stage eg. <code>dev</code></li> <li>The version eg. <code>v0.0.1</code></li> </ol>"},{"location":"dynatrace-releases/#kubernetes-method","title":"Kubernetes Method","text":"<p>For Kubernetes, simply add Kubernetes labels to your pods:</p> <pre><code>app.kubernetes.io/part-of\napp.kubernetes.io/version\n</code></pre> <p>The <code>product</code> value is taken from the <code>app.kubernetes.io/part-of</code> label. The <code>version</code> is taken from the <code>app.kubernetes.io/version</code> label and the <code>stage</code> is taken from either the <code>namespace</code> or the Dynatrace <code>host group</code> (if set).</p> <p>That's all there is to it &amp; you'll see releases begin to appear when new deployments are pushed.</p> <p></p>"},{"location":"dynatrace-releases/#vm-method","title":"VM Method","text":"<p>Assuming you have a process on a VM, make the following environment variables available to the process during startup.</p> <pre><code>export DT_RELEASE_PRODUCT=&lt;PRODUCT-NAME&gt;\nexport DT_RELEASE_STAGE=&lt;STAGE-NAME&gt;\nexport DT_RELEASE_VERSION=&lt;VERSION-NUMBER&gt;\n</code></pre> <p>For example, to track <code>apache httpd</code> releases we add these variables to <code>/etc/apache2/envvars</code> and restart <code>apache2</code>:</p> <pre><code>echo 'export DT_RELEASE_VERSION=v0.1' | sudo tee -a /etc/apache2/envvars\necho 'export DT_RELEASE_STAGE=dev' | sudo tee -a /etc/apache2/envvars\necho 'export DT_RELEASE_PRODUCT=frontend' | sudo tee -a /etc/apache2/envvars\nsudo service apache2 restart\n</code></pre> <p></p>"},{"location":"dynatrace-releases/#summary","title":"Summary","text":"<p>This is a relatively new feature in Dynatrace but is so easy to do, I now consider it a best practice.</p>"},{"location":"dynatrace-sla-management/","title":"SLA Management - Keeping 3rd Parties Honest","text":"<p>Legacy Product</p> <p>This tutorial refers to the legacy application monitoring product from Dynatrace. Not the Dynatrace platform.</p> <p>Comprehensive SLA management and tracking is absolutely critical to the success of a project. In this post I'll investigate how Dynatrace AppMon can keep all your third parties honest.</p> <p>To follow this tutorial you'll need:</p> <ul> <li>A working and licensed AppMon server and collector. (Here's a free forever license for you.)</li> <li>A PHP-enabled web server (I recommend XAMPP)</li> <li>Save the <code>endpoint.php</code> (find it here) code into the htdocs folder of your XAMPP server. Download the <code>DTSLAManagement.jar</code> file (from here)</li> </ul>"},{"location":"dynatrace-sla-management/#scenario","title":"Scenario","text":"<p>Our organisation uses third party API endpoints (who doesn't?). These endpoints could be internally developed services or provided entirely by a third party \u2013 we want to ensure they're performing to advertised or agreed standards \u2013 our application depends on it.</p> <p>An acceptable level of performance for this endpoint is 2 seconds. Any requests over this threshold count against the third party provider and financial penalties are incurred. Any requests above 4 seconds automatically trigger an investigation from their support department and a heavier penalty is incurred.</p>"},{"location":"dynatrace-sla-management/#business-outcomes","title":"Business Outcomes","text":"<p>As a business owner, I want the following:</p> <ul> <li>How many calls have I had over 2 seconds.</li> <li>How many calls have I had over 4 seconds.</li> <li>An automated way of alerting their support department for &gt;4s calls.</li> <li>A weekly report of the above.</li> </ul>"},{"location":"dynatrace-sla-management/#test-setup","title":"Test Setup","text":"<ol> <li>Place the <code>endpoint.php</code> file into the <code>htdocs</code> folder of XAMPP and start XAMPP.</li> <li>Create a new Java based agent tier in AppMon.</li> <li>Create a sensor group and attach it to the Java tier you've just created. Set this sensor group to Active &amp; Start Purepaths.</li> <li>Instrument and run the <code>DTSLAManagement.jar</code> file.</li> </ol> <p>{% raw %} <pre><code>java -jar -agentpath:\"C:/Program Files/Dynatrace/Dynatrace 7.0/agent/lib64/dtagent.dll\"=name=YOUR-AGENT-NAME,server=localhost:9998 DTSLAManagement.jar\n</code></pre> {% endraw %}</p> <p>It should execute and look something like this:</p> <p></p> <p>If you see <code>Connection refused: connect</code>, make sure you've started XAMPP!</p> <p>Using the JAR is easy, just pass the number of seconds you want the endpoint to wait, as the only parameter to the JAR file.</p>"},{"location":"dynatrace-sla-management/#wait-1-second","title":"Wait 1 Second","text":"<pre><code>java -jar -agentpath:\"C:/Program Files/Dynatrace/Dynatrace 7.0/agent/lib64/dtagent.dll\"=name=YOUR-AGENT-NAME,server=localhost:9998 DTSLAManagement.jar 1\n</code></pre>"},{"location":"dynatrace-sla-management/#wait-4-seconds","title":"Wait 4 Seconds","text":"<p>{% raw %} <pre><code>java -jar -agentpath:\"C:/Program Files/Dynatrace/Dynatrace 7.0/agent/lib64/dtagent.dll\"=name=YOUR-AGENT-NAME,server=localhost:9998 DTSLAManagement.jar 4\n</code></pre> {% endraw %}</p>"},{"location":"dynatrace-sla-management/#create-purepaths","title":"Create Purepaths","text":"<p>Add a sensor to the <code>main</code> method of the JAR file. You'll find it in the <code>co.uk.adamgardner.code</code> package.</p> <p></p>"},{"location":"dynatrace-sla-management/#see-the-purepaths","title":"See The Purepaths","text":"<p>If you open the Purepaths dashlet, you should now be seeing a single Purepath created each time you run of the JAR file.</p> <p>I've ran the JAR file 3 times and crucially, I can see the main method and the call out to <code>http://localhost/endpoint.php</code></p> <p></p>"},{"location":"dynatrace-sla-management/#track-it","title":"Track It","text":"<p>Tracking these calls is now easy. Right click the node in the Purepath tree with the call out to the <code>endpoint.php</code> &gt; Create Measure &gt; Time Measure (remove the query parameter and give it a good name).</p> <p></p> <p>Now let's finalise the setup. Edit the system profile and go to the Measures section. Edit your measure and set your Upper Warning threshold to 2 seconds and your Upper Severe threshold to 4 seconds.</p> <p></p> <p>Create two new threshold violation measures. Both should have the source measure set to the your endpoint measure. One should have the threshold set to warning and the other should have the threshold set to severe.</p> <p></p>"},{"location":"dynatrace-sla-management/#visualisation","title":"Visualisation","text":"<p>Let's put it all together on a dashboard. You can see that I've had one warning alert and two severe alerts.</p> <p></p> <p>Save this dashboard to the server and create an automated report and start accurately tracking your third party calls.</p>"},{"location":"enriching-spans-opentelemetry-python/","title":"Enriching OpenTelemetry Span Metadata in Python","text":"<p>In the second of the series, the spans are enriched with useful metadata: Setting the span status, adding span attributes and span events.</p> <p>This is the text version for those who prefer to read instead. If you got here and want to watch, just click the image below.</p>"},{"location":"enriching-spans-opentelemetry-python/#understanding-span_id-and-trace_id","title":"Understanding span_id and trace_id","text":"<p>Each span has a name, which you defined in the decorator on line 19. Each span has a unique <code>span_id</code> and a (possibly non unique) <code>trace_id</code>. The difference between these two fields is that the <code>span_id</code> provides an ID for the small unit of work (the <code>add</code> method).</p> <p>Realise though that in a realistic program, you will have multiple functions and many microservices. Ie. you will have many spans. The <code>trace_id</code>.</p> <p>To put it another way, imagine using OpenTelemetry to track the tasks in your day (yes, you could do that). Each task (breakfast, cleaning teeth, shower, walking the dog...) would be a span. The commonality (trace) would be the day / date. So you know that each of these discrete activities was related and can then reconstitute them into a relationship graph like this:</p> <p></p>"},{"location":"enriching-spans-opentelemetry-python/#span-kind-field","title":"Span kind field","text":"<p>Every Span has a kind:</p> <ul> <li>CLIENT</li> <li>SERVER</li> <li>PRODUCER</li> <li>CONSUMER</li> <li>INTERNAL (default)</li> </ul> <p>The specification (linked above) describes the differences in detail, but <code>UNSET</code> (the default) means: \"Indicates that the span represents an internal operation within an application...\" so that is actually appropriate for our usecase.</p>"},{"location":"enriching-spans-opentelemetry-python/#parent-id-start-and-end-time","title":"Parent ID, start and end time","text":"<p>The parent ID of our span is <code>null</code> this simply means that \"this span had no parent span\" (nothing came before it).</p> <p>The <code>start_time</code> and <code>end_time</code> should be self explanatory. Subtracting one from the other gives us the duration of this span.</p>"},{"location":"enriching-spans-opentelemetry-python/#span-status","title":"Span Status","text":"<p>Every span has a status which is one of:</p> <ul> <li>Unset (default)</li> <li>Ok</li> <li>Error</li> </ul> <p>Obviously, you (the developer) use the status field to denote success of failure of whatever action this span was trying to achieve.</p> <p>We will set the Span status soon.</p>"},{"location":"enriching-spans-opentelemetry-python/#span-attributes","title":"Span Attributes","text":"<p>Span attributes are Key / Value pairs of additional metadata that you have control over. You can add whatever additional metadata you like.</p> <p>We will add attributes to the span soon. Please also see the bottom of this post for a discussion / warning about attributes.</p>"},{"location":"enriching-spans-opentelemetry-python/#span-events","title":"Span Events","text":"<p>Sometimes, things happen during a span which may not rise to the level of having their own span - but you still want to \"put a dot on the timeline\" to inform viewers of this telemetry that \"something happened here\". That's what span events are used for. They denote that \"something happened during this span that I'd like you to know about\".</p> <p>We will add a span event to the span soon.</p>"},{"location":"fastapi-opentelemetry-autoinstrumentation/","title":"Autoinstrumentation of Python FastAPI with OpenTelemetry","text":"<p>In this video I demonstrate how to autoinstrument a Python-based FastAPI application with OpenTelemetry in 2 lines of code, using auto instrumentation.</p>"},{"location":"fastapi-opentelemetry/","title":"OpenTelemetry Instrumented Python FastAPI","text":"<p>I expand a previous tutorial, adding OpenTelemetry tracing both automatically and manually.</p>"},{"location":"fastapi-opentelemetry/#1-instrument-automatically","title":"1: Instrument Automatically","text":""},{"location":"fastapi-opentelemetry/#2-discuss-the-limitations-of-auto-instrumentation","title":"2: Discuss the limitations of Auto-Instrumentation","text":""},{"location":"fastapi-opentelemetry/#3-add-manual-instrumentation","title":"3: Add manual instrumentation","text":""},{"location":"first-steps-prometheus/","title":"Take your first steps with Prometheus","text":"<p>If you are brand new to Prometheus, this video will guide you through your first steps with the CNCF monitoring tool. From what Prometheus is, how it works, the types of metrics it supports. It is, as usual, a hands-on exercise so by the end of the video you will have Prometheus up and running collecting monitoring data.</p>"},{"location":"fluent-bit-2mins/","title":"Get Started with Fluent Bit in 2 Minutes","text":"<p>Get started with Fluent Bit in about 2 minutes. Stream log files to any log storage backend.</p> <p>Links:</p> <ul> <li>Fluent Bit</li> </ul>"},{"location":"free-website-checks-opentelemetry/","title":"Check your Website for Free using OpenTelemetry","text":"<p>Here's how to perform basic up/down checks of your websites using the OpenTelemetry Collector.</p> <p>Watch</p> <p>If you'd prefer to watch rather than read, I have the following content as a YouTube video:</p> <p></p> <p>Did you know it is possible to use the OpenTelemetry Collector to perform basic HTTP checks of your webpages?</p>"},{"location":"free-website-checks-opentelemetry/#collector-configuration","title":"Collector Configuration","text":"<p>It's actually really quite easy. For this blog post I'll assume you're already familiar with the collector and its YAML configuration file (if not, I suggest watching the video above as it has a full end-to-end runthrough with no assumptions).</p> <p>First, you'll need a collector distribution that includes the httpcheck receiver (the contrib and k8s versions both do).</p> <p>Here's the magic snippet you need:</p> <pre><code>receivers:\n  httpcheck:\n    collection_interval: 10s\n    targets:\n      - endpoint: http://example.com\n        method: GET\n      - endpoint: https://agardner.net\n        method: GET\n      ...\nexporters:\n  debug:\n    verbosity: detailed\n  otlphttp:\n     endpoint: ...\n\nservice:\n  pipelines:\n    metrics:\n      receivers: [httpcheck]\n      exporters: [otlphttp, debug etc...]\n</code></pre> <p>The <code>collection_interval</code> defines how often the collector will check each endpoint. Note: 10 seconds is far too frequent for production. Probably once a minute is good enough (<code>1m</code>).</p> <p>That's it, start the collector and as long as you've configured the <code>exporters</code> section correctly for your backend, you should see metrics in your Observability tool. How Green is your Observability? How the OpenTelemetry collector can help</p>"},{"location":"gemini-cli-monitoring/","title":"How to monitor Google's Gemini CLI using OpenTelemetry","text":"<p>Track token counts, model usage, tool usage and \"developer idle time\" (the time you sit waiting for an answer when using Google's Gemini CLI and the OpenTelemetry collector. In this video, I show you how...</p> <p>I configure the Gemini CLI to send metrics and logs. I spin up the OpenTelemetry collector to receive that telemetry and I leverage OpenObserve as the backend to visualise the data.</p> <p>Now you can have full, runtime and realtime usage statistics of your Gemini CLI token usage split by model, tool usage and perhaps most importantly, you can track your \"idle time\".</p>"},{"location":"getting-started-with-keptn/","title":"Getting Started with Keptn","text":"<p>Keptn is a flexible workflow and automation engine. It is extremely easy to get started with Keptn, you can be up, running and modelling your own workflows in under 10 minutes...</p>"},{"location":"getting-started-with-keptn/#keptn-101-getting-started-in-3-steps","title":"Keptn 101: Getting Started in 3 Steps","text":"<ol> <li>Define what you want to achieve</li> <li>Model that intention in a Shipyard file</li> <li>Define the tooling that you want to use to achieve those tasks</li> </ol>"},{"location":"getting-started-with-keptn/#1-define-tasks","title":"1. Define Tasks","text":"<p>Start by deciding what you want to achieve. In Keptn's world, the high level requirement is a sequence and the individual tasks are, ummm tasks.</p> <p>The most simple example would be a desire to create a report. Most likely this is a very simple workflow consisting of a single \"create report\" task.</p> <p>A more complex example may be that you wish to build an entire demo system including infrastructure, user onboarding and notifications when completed. In which case, the tasks you must run are:</p> <ol> <li>Create infrastructure</li> <li>Install application</li> <li>Onboard a user</li> <li>Notify user that they now have access to the demo system</li> <li>Report to a third party system to record that the demo system is available</li> </ol>"},{"location":"getting-started-with-keptn/#2-model-your-world","title":"2. Model Your World","text":"<p>Keptn models the world in a file called a Shipyard YAML file.</p> <p>Some quick FYIs:</p> <ol> <li>Sequences (by default) are standalone</li> <li>Sequences and tasks can be named anything you want</li> <li>When a sequence is triggered to run, Keptn will automatically execute each task sequentially</li> <li>Sequences can be linked together to build complex workflows</li> <li>Metadata can be provided to each task either in the Shipyard or via the Keptn API when triggering a sequence</li> </ol> <p>Define the basics of your shipyard:</p> <pre><code>---\napiVersion: \"spec.keptn.sh/0.2.2\"\nkind: \"Shipyard\"\nmetadata:\n  name: \"my-first-shipyard\"\n</code></pre> <p>Now decide how many <code>stages</code> you want to model. In our case, we only need one as we're not modelling anything pipeline related (where things \"move\" from one \"stage\" to the next). So we'll call our stage <code>main</code></p> <pre><code>---\napiVersion: \"spec.keptn.sh/0.2.2\"\nkind: \"Shipyard\"\nmetadata:\n  name: \"my-first-shipyard\"\nspec:\n  stages:\n    - name: \"main\"\n</code></pre> <p>Define your sequences and tasks. Modelling the simple \"create report\" sequence might look like this, where we have a single sequence and a matching single task.</p> <p>Remember that the name values can be any string you want, so you're free to make up your own sequence and task names.</p> <pre><code>---\napiVersion: \"spec.keptn.sh/0.2.2\"\nkind: \"Shipyard\"\nmetadata:\n  name: \"my-first-shipyard\"\nspec:\n  stages:\n    - name: \"main\"\n    sequences:\n      - name: \"makereport\"\n        tasks:\n          - name: \"createreport\"\n</code></pre> <p>The more complex \"create a demo system\" workflow might look like this:</p> <pre><code>---\napiVersion: \"spec.keptn.sh/0.2.2\"\nkind: \"Shipyard\"\nmetadata:\n  name: \"my-first-shipyard\"\nspec:\n  stages:\n    - name: \"main\"\n    sequences:\n      - name: \"createdemosystem\"\n        tasks:\n          - name: \"createinfra\"\n          - name: \"installapp\"\n          - name: \"onboarduser\"\n          - name: \"notify\"\n          - name: \"report\"\n</code></pre>"},{"location":"getting-started-with-keptn/#3-define-your-tooling","title":"3. Define Your Tooling","text":"<p>It is now time to define the tooling you wish to execute each task. This tooling can take a number of forms:</p> <ol> <li>An HTTP request to an endpoint, shell script or Python script</li> <li>A container image that should run for a particular task</li> <li>A more complex or custom workflow that's modelled by a \"full Keptn service\" (example: JIRA Service)</li> </ol> <p>You may decide that:</p> <ul> <li><code>createreport</code> task is best handled by running a Python file</li> <li><code>createinfra</code> task is best handled by the <code>aws-service</code></li> <li><code>installapp</code> task is best handled by running a shell script</li> <li><code>onboarduser</code> task is best handled by running the <code>example/someimage:1.0.2</code> container</li> <li><code>notify</code> best handled by <code>slack-service</code></li> <li><code>report</code> best handled by <code>jira-service</code></li> </ul> <p>Tooling in Keptn listens for and reacts to <code>sh.keptn.event.{taskname}.*</code> events. Usually the <code>taskname.triggered</code> event. The exact events your service listens for is easily set via an environment variable when installing the service.</p> <p>In the above example:</p> <ul> <li>Install the generic executor service and configure the environment variable to listen for the <code>sh.keptn.event.buildreport.triggered</code> event</li> <li>Install the <code>aws-service</code> and configure the env var to listen for the <code>sh.keptn.event.createinfra.triggered</code> event</li> <li>Configure the generic executor service to listen for a second event: <code>sh.keptn.event.installapp.triggered</code> event and run the shell script</li> <li>Install the job executor service and configure it to listen for the <code>sh.keptn.event.onboarduser.triggered</code> event</li> <li>Install the Slack service and configure it to listen for the <code>sh.keptn.event.notify.triggered</code> event</li> <li>Install the JIRA Service and configure it to listen for the <code>sh.keptn.event.report.triggered</code> event</li> </ul>"},{"location":"getting-started-with-keptn/#step-4-trigger-sequence","title":"Step 4: Trigger Sequence","text":"<p>Your system is set up and ready to go! Trigger the sequence and Keptn will handle the lifecycle of the tasks for you. Your tooling will respond and your tasks will be achieved.</p> <pre><code>curl -X POST https://keptn/...\n{\n  ...\n  \"type\": \"sh.keptn.event.main.createdemosystem.triggered\"\n  ...\n}\n</code></pre>"},{"location":"getting-started-with-keptn/#summary","title":"Summary","text":"<p>Modelling workflows and tasks in Keptn is relatively painless. Keptn is designed to split the workflow from tooling. You decide the best tool for the job and Keptn handles the rest.</p> <p>Check out the Keptn public demo system to see how sequences are executed.</p> <p>Is there a service you'd like to use with Keptn? Join the Keptn community on Slack and suggest an integration.</p>"},{"location":"hands-on-keptn-metrics-server/","title":"Retrieve metrics from anywhere on Kubernetes","text":"<p>The Keptn metrics server is a tool which enables developers to retrieve metrics from any / many sources, store them in an on-cluster cache and treat them completely generically. You no longer need to worry about where they came from, just \"get me the value of metric <code>foo</code>\" regardless of the metric source.</p> <p>I've wrote this as a hands-on blog over on <code>dev.to</code>: Hands on with Keptn Metrics Server: Pull Metrics from anywhere and treat them generically.</p>"},{"location":"hands-on-with-pitometer/","title":"Hands On With Pitometer","text":"<p>Build an automated quality gate in under 5 minutes with this hands on demo of Pitometer...</p>"},{"location":"hands-on-with-pitometer/#prerequisites","title":"Prerequisites","text":"<ul> <li>A Dynatrace SaaS Tenant (free trial here).</li> <li>An Amazon EC2 instance (free tier is fine).</li> <li>Ansible installed on a control machine.</li> <li>Python <code>netaddr</code> installed on ansible control machine (<code>pip install netaddr</code>)</li> </ul> <p>Need some background info? Need to know why you should use Pitometer?Read this overview post first.</p>"},{"location":"hands-on-with-pitometer/#scenario","title":"Scenario","text":"<p>We\u2019ll build a system which runs one of two versions of a web server. This web server will be placed under artificial load by a load generator installed on the EC2 instance. The EC2 instance will be monitored with the Dynatrace OneAgent.</p> <ul> <li>Web server version 1 has a wait time of zero seconds.</li> <li>Web server version 2 has a wait time of 4 seconds.</li> <li>Pitometer is installed onto the same EC2 instance and is configured to have a <code>warning threshold</code> of <code>3 seconds</code> and a <code>severe threshold</code> of <code>5 seconds</code>.</li> </ul> <p>When Pitometer is run, it will read a <code>perfspec</code> file. This perfspec file contains our quality criteria and thresholds.</p> <p>Pitometer will retrieve response time statistics from Dynatrace and evaluate against the criteria from the perfspec file.</p> <p>Pitometer will provide a <code>pass</code>, <code>warning</code> or <code>fail</code> decision based on the execution.</p> <p>Your pipeline technology can then utilise this decision to stop or progress the build.</p> <p> </p>"},{"location":"hands-on-with-pitometer/#expected-output","title":"Expected Output","text":"<p>We\u2019re expecting a <code>pass</code> decision for version 1 since it is under the 3 second response time threshold (see above).</p> <p>We\u2019re expecting a <code>warning</code> decision for version 2 since response time is over 3 seconds but under 5 seconds (see above).</p>"},{"location":"hands-on-with-pitometer/#installation","title":"Installation","text":"<ul> <li>Add your EC2 instance to the <code>pitometer</code> group of your <code>/etc/ansible/hosts</code> file.</li> <li>Ensure you can ping your node with: <code>ansible -m ping pitometer</code></li> <li>If you want to access the web server UI, ensure port 80 is open to your IP address.</li> <li>Save this Ansible playbook to your control node.</li> <li>Modify the <code>vars</code> section of the playbook to reflect your values.</li> <li>Install the system with: {% raw %} <pre><code>ansible-playbook ~/pitometerDemo.playbook.yml --tags install\n</code></pre> {% endraw %}</li> </ul> <p>Unsupported parameters for (yum) module warning? Either upgrade your ansible to &gt;2.8 or comment out the <code>lock_timeout</code> line of the <code>Install Docker and Git Packages</code> task.</p>"},{"location":"hands-on-with-pitometer/#recap","title":"Recap","text":"<p>The Ansible playbook has installed the system for us. It has spun up version 1 of the web server and started the load generator. You should now see statistics in Dynatrace:</p> <p></p>"},{"location":"hands-on-with-pitometer/#execute-pitometer","title":"Execute Pitometer","text":"<p>Now that you have some statistics in Dynatrace, let\u2019s run Pitometer and see the output.</p> <p>Remember that we\u2019re running version 1 so we\u2019re expecting a <code>\"result\": \"pass\"</code> output.</p> <p>{% raw %} <pre><code>ansible-playbook ~/pitometerDemo.playbook.yml --tags runPitometer\n</code></pre> {% endraw %}</p> <p></p>"},{"location":"hands-on-with-pitometer/#switch-to-version-2","title":"Switch To Version 2","text":"<p>Now let\u2019s switch to version 2 of the web server. Remember that this version includes a 4 second wait.</p> <p>{% raw %} <pre><code>ansible-playbook ~/pitometerDemo.playbook.yml --tags runWaiterV2\n</code></pre> {% endraw %}</p> <p>Wait a minute or so until the new longer response time statistics are reflected in Dynatrace. Notice the increased response time and the drop in throughput.</p> <p></p>"},{"location":"hands-on-with-pitometer/#re-run-pitometer","title":"Re-Run Pitometer","text":"<p>This time, we expect Pitometer to produce a <code>\"result\": \"warning\"</code> output. Our response time is 4 seconds and our warning threshold is set at 3 seconds:</p> <p>{% raw %} <pre><code>ansible-playbook ~/pitometerDemo.playbook.yml --tags runPitometer\n</code></pre> {% endraw %}</p> <p></p>"},{"location":"hands-on-with-pitometer/#toggle-between-v1-v2","title":"Toggle Between v1 &amp; v2","text":"<p>If you\u2019d like to toggle between v1 and v2 of the web server, just run the following commands:</p> <p>{% raw %} <pre><code>ansible-playbook ~/pitometerDemo.playbook.yml --tags runWaiterV1\nansible-playbook ~/pitometerDemo.playbook.yml --tags runWaiterV2\n</code></pre> {% endraw %}</p>"},{"location":"hands-on-with-pitometer/#summary","title":"Summary","text":"<p>This tutorial demonstrated how to create a simple automatic quality gate using the open source Pitometer tool &amp; provides a repeatable demo system you can use.</p>"},{"location":"how-distributed-tracing-really-works/","title":"How OpenTelemetry tracing really works","text":"<p>A technical deep dive explanation of how OpenTelemetry distributed tracing really works under the hood with examples using Python, FastAPI and the requests module and Jaeger. I discuss the traceparent header and its importance in correctly stitching a trace together end-to-end.</p> <p>GitHub Repo for code: <code>https://github.com/agardnerIT/python-fastapi-requests-traceparent</code></p>"},{"location":"introduction-to-feature-flags-cncf/","title":"An Introduction to Feature Flagging and OpenFeature","text":"<p>I recently did an introduction to feature flagging &amp; OpenFeature CNCF webinar. Here it is...</p>"},{"location":"introduction-to-feature-flags-cncf/#links","title":"Links","text":"<ul> <li>OpenFeature hands-on tutorials on Killercoda</li> <li>flagd</li> <li>OpenFeature Operator</li> </ul>"},{"location":"invalid-dylib-load-error/","title":"Invalid dylib load Python Error","text":"<p>Here is how I fixed the <code>Invalid dylib load. Clients should not load the unversioned libcrypto dylib as it does not have a stable ABI</code> error on MacOS.</p> <p>Recently I moved from the MacOS standard (old) terminal to <code>zsh</code>. I also updated all my <code>brew</code> packages. I'll be honest, I do not know which of those two things caused the issue, but the end result was that most of my Python-based programs were broken with the error report:</p> <pre><code>Invalid dylib load. Clients should not load the unversioned libcrypto dylib as it does not have a stable ABI\n</code></pre> <p>After some hunting, I found the solution.</p> <ol> <li>Open a new terminal window and <code>cd</code> to <code>/usr/local/opt</code></li> <li>List all directories which begin with 'openssl': <code>ls | grep '^openssl*'</code>. I get <code>openssl</code> and <code>openssl@1.1</code></li> <li>Set the <code>DYLD_FALLBACK_LIBRARY_PATH</code> variable to point to the <code>lib</code> folder of the <code>openssl</code> directory. Note: I chose the <code>openssl@1.1</code> directory. Frankly that was just a guess (based on the fact that it has a version number and that is what the error was complaining about). I am not claiming that it's correct. <pre><code>export DYLD_FALLBACK_LIBRARY_PATH=/usr/local/opt/openssl@1.1/lib\n</code></pre></li> <li>Open a new terminal window and retry your script. It should now work.</li> </ol>"},{"location":"invalid-dylib-load-error/#persisting-the-parameter","title":"Persisting The Parameter","text":"<p>To persist this parameter, write it into <code>~/.bashrc</code> then run <code>source ~/.bashrc</code></p> <pre><code>sudo nano ~/.bashrc\n</code></pre> <p>Paste this line into the file:</p> <pre><code>export DYLD_FALLBACK_LIBRARY_PATH=/usr/local/opt/openssl@1.1/lib\n</code></pre> <p>Now run <code>source ~/.bashrc</code></p> <p>As always, if you have any comments or suggestions, please do not hesitate to contact me.</p>"},{"location":"keptn-hello-world/","title":"Keptn Hello World","text":"<p>The Keptn \"Hello World\" example in under 10 minutes.</p>"},{"location":"keptn-hello-world/#introduction","title":"Introduction","text":"<p>There are lots of excellent tutorials on the Keptn website but this post aims to get you up and running with Keptn and a \"Hello World\" example in under 10 minutes.</p>"},{"location":"keptn-hello-world/#what-you-will-need","title":"What You Will Need","text":"<p>To follow along, you will need:</p> <ol> <li>A GitHub account</li> <li>A Kubernetes Cluster</li> <li>Helm installed</li> </ol>"},{"location":"keptn-hello-world/#create-github-stuff","title":"Create GitHub Stuff","text":"<ol> <li>Create a GitHub PAT with full <code>repo</code> scope. Keptn will use this token to ensure all files and changes are synced to the upstream repo.</li> <li>Create a blank (uninitialised) repository for Keptn to work with. Do not add any files (not even a readme)</li> <li>Set some environment variables like below</li> </ol> <pre><code>export GIT_USER=&lt;YourGitUsername&gt;\nexport GIT_REPO=https://github.com/&lt;YourGitUserName&gt;/&lt;YourRepo&gt;\nexport GIT_TOKEN=ghp_****\n</code></pre>"},{"location":"keptn-hello-world/#install-and-expose-keptn","title":"Install and Expose Keptn","text":"<pre><code>curl -sL https://get.keptn.sh | KEPTN_VERSION=0.12.0 bash\nhelm install keptn https://github.com/keptn/keptn/releases/download/0.12.0/keptn-0.12.0.tgz -n keptn --create-namespace --wait\n</code></pre> <p>All pods should now be up and running: <code>kubectl get pods -n keptn</code></p> <pre><code>NAME                         READY   STATUS\nbridge-*                     1/1     Running\napproval-service-*           2/2     Running\napi-gateway-nginx-*          1/1     Running\nwebhook-service-*            2/2     Running\nlighthouse-service-*         2/2     Running\nkeptn-mongo-*                1/1     Running\nremediation-service-*        2/2     Running\nconfiguration-service-*      1/1     Running\nsecret-service-*             1/1     Running\nkeptn-nats-cluster-0         2/2     Running\napi-service-*                2/2     Running\nmongodb-datastore-*          2/2     Running\nshipyard-controller-*        2/2     Running\nstatistics-service-*         2/2     Running\n</code></pre> <p>Continue: <pre><code>helm install -n keptn job-executor-service https://github.com/keptn-contrib/job-executor-service/releases/download/0.1.6/job-executor-service-0.1.6.tgz --wait\n</code></pre></p> <p>This will add one extra pod:</p> <pre><code>NAME                      READY   STATUS\njob-executor-service-*    2/2     Running\n</code></pre> <p>Expose Keptn: <pre><code>curl -SL https://raw.githubusercontent.com/keptn/examples/master/quickstart/expose-keptn.sh | bash\n</code></pre></p> <p>Temporary Note: If running on localhost, that script will work perfectly. If running on a cloud provider with Load Balancers available, the script above will point to <code>http://127.0.0.1</code> which obviously isn't correct. To fix, run the following to expose the bridge (UI) on a loadBalancer. We're working on a fixed script and I will remove this step once merged.</p> <pre><code>helm upgrade keptn https://github.com/keptn/keptn/releases/download/0.12.0/keptn-0.12.0.tgz -n keptn --set=control-plane.apiGatewayNginx.type=LoadBalancer --wait\nexport KEPTN_ENDPOINT=$(kubectl get services -n keptn api-gateway-nginx -o=jsonpath='{.status.loadBalancer.ingress[0].ip}')\necho \"Keptn Available at: http://$KEPTN_ENDPOINT\"\nkeptn auth --endpoint=$KEPTN_ENDPOINT\n</code></pre>"},{"location":"keptn-hello-world/#configure-keptn","title":"Configure Keptn","text":"<pre><code>wget https://gist.githubusercontent.com/agardnerIT/8046b8a81bab90a37aef83219a8e8078/raw/341b6d3c8b8dfab30742320402706e903e5bb4ab/shipyard.yaml\nkeptn create project hello-world --shipyard=shipyard.yaml --git-user=$GIT_USER --git-remote-url=$GIT_REPO --git-token=$GIT_TOKEN\nkeptn create service demo --project=hello-world\nwget https://gist.githubusercontent.com/agardnerIT/1d4eaa1425832ee9a9036de92a20b3b7/raw/c0caddfcc3025fb16b55b21ea683ed7f1be328fe/jobconfig.yaml\nkeptn add-resource --project=hello-world --service=demo --stage=dev --resource=jobconfig.yaml --resourceUri=job/config.yaml\n</code></pre>"},{"location":"keptn-hello-world/#trigger-keptn","title":"Trigger Keptn","text":"<p>Trigger Keptn by sending a cloudevent to the API using the <code>keptn send event</code> command. A precrafted cloudevent is available for you:</p> <pre><code>wget https://gist.githubusercontent.com/agardnerIT/005fc85fa86072d723a551a5708db21d/raw/d9efa71969657f7508403f82d0d214f878c4c9ca/hello.triggered.event.json\nkeptn send event -f hello.triggered.event.json\n</code></pre> <p>Go to the Keptn bridge, into the sequence view of the <code>hello-world</code> project and you should see:</p> <p></p>"},{"location":"keptn-hello-world/#what-happened","title":"What Happened?","text":"<p>Keptn core components were installed along with the <code>job-executor-service</code> microservice. The <code>keptn</code> CLI was also downloaded. The Keptn bridge was exposed and, to make this demo easier, all authentication was removed.</p> <p>A Keptn project was created called <code>hello-world</code> and a shipyard file was provided. A shipyard file is the \"blueprint\" of a Keptn project.  Inspect this file. Notice it has one stage (<code>dev</code>), one sequence (<code>hello</code>) and this sequence has one task associated to it (<code>hello-world</code>). A Keptn service was created called <code>demo</code>.</p> <p>The job executor service comes preconfigured to listen for all Keptn events.</p> <p>The <code>job/config.yaml</code> file instructs the job-executor-service microservice to run the <code>alpine</code> docker image and run <code>echo \"hello world\"</code> when it \"hears\" the <code>sh.keptn.event.hello-world.triggered</code> event.</p> <p>The <code>keptn send event</code> is a wrapper around the Keptn API endpoint. The cloudevent asks Keptn to trigger the <code>hello</code> sequence in the <code>dev</code> stage of the <code>hello-world</code> project for the <code>demo</code> service.</p>"},{"location":"keptn-hello-world/#wait-i-triggered-hello-not-hello-world","title":"Wait! I triggered <code>hello</code>, not <code>hello-world</code>?","text":"<p>Correct: You triggered the <code>sequence</code> and Keptn handles the <code>task</code> execution for you.</p> <p>Humans (or other tooling) trigger Keptn sequences. Tooling responds to Keptn tasks.</p> <p>Due to the shipyard file, Keptn knows that the first task in your sequence is called <code>hello-world</code> so:</p> <ol> <li>Keptn crafts a cloud event for you and distributes it to whatever service is listening for the <code>sh.keptn.event.hello-world.triggered</code> event</li> <li>In our case, the job executor service is configured to respond. The JES looks up the config from the yaml file and runs the container</li> <li>The job executor service sends a pair of events back to Keptn (a <code>.started</code> and corresponding <code>.finished</code> event)</li> <li>Keptn receives a matching pair of <code>.started</code> and <code>.finished</code> events from JES so knows the task is done</li> <li>Keptn now finishes the sequence</li> </ol>"},{"location":"keptn-hello-world/#whats-the-big-deal","title":"What's the big deal?","text":"<p>Keptn handles the orchestration and timing, you bring the tools you already use.</p> <p>You've successfully split the process from the tooling. You've hidden the complexities of tooling API interaction. There are thousands of tool integrations already written for Keptn or write your own. Keptn comes out-of-the-box with microservices dedicated to self-healing and code quality gate workflows.</p> <p>With Keptn it is incredibly easy to build complex task sequences whilst no longer worrying about the tooling that will implement those tasks.</p> <ul> <li>Try adding further tasks to your sequence</li> <li>Try sending a webhook to a third party tool instead of using the job executor service</li> <li>Build a full end-to-end delivery pipeline or any other sequence of tasks you can dream up</li> </ul>"},{"location":"keptn-hello-world/#next-steps","title":"Next Steps","text":"<ol> <li>Continue to explore Keptn on the public demo system</li> <li>Use Keptn for multi-stage delivery or self-healing with Prometheus</li> <li>Use Keptn to orchestrate Argo Rollouts</li> <li>Use Keptn for Resilience evaluation with LitmusChaos</li> <li>Explore the tools and integrations that Keptn currently supports</li> </ol> <p>Got questions? Need help? Join the Keptn community on Slack</p>"},{"location":"keptn-install/","title":"Install Keptn with Helm","text":"<p>Install Keptn with Helm in under 2 minutes.</p>"},{"location":"keptn-jes-quality-gates/","title":"Keptn: Running Infracost with the Job Executor Service","text":"<p>This video shows how to run Infracost with Keptn and the Job Executor Service.</p> <p>It also demonstrates how to push metrics to an observability backend (Prometheus is used in this video). Finally a Keptn quality gate is executed which checks the output of Infracost.</p> <p>Now your FinOps data can be included in your pipeline go / no go decisions alongside SRE metrics like response time, availability and throughput.</p>"},{"location":"keptn-pipelines/","title":"We've Been Building Pipelines Wrong. Here's How to Fix Them!","text":"<p>We've been building code pipelines wrong for years. Here's how to fix them...</p>"},{"location":"keptn-pipelines/#quick-note","title":"Quick Note","text":"<p>When reading this post, take the terminology as generically as possible. Treat the technologies / products mentioned as exemplars only.</p> <p>For example, \"Artifact\" in this post refers to any deployable unit. Read that as a container image, a zip file or a single JSON file if you wish.</p>"},{"location":"keptn-pipelines/#the-problem-with-with","title":"The Problem with \"with\"","text":"<p>If someone asked you to describe your pipeline, I bet you would want to describe something like this:</p> <p></p> <p>However, I bet what you'd actually describe is something like this:</p> <p></p> <p>As mentioned previously, the tools above are only examples, replace with the tools you use.</p> <p>As soon as you mention the tooling you use to achieve those steps, you have already introduced several problems:</p> <ol> <li>Your pipeline logic is now tightly coupled to the tooling that delivers that logic.</li> <li>Maintenance is difficult. You need to understand (and be able to code) all of those third party tool APIs. In the above example, someone in your organisation needs an in-depth knowledge of DockerHub integration strategies, JMeter APIs, whatever integration options Tool X provides and of course, Helm.</li> <li>Upgrades: Want to upgrade Tool X to the latest version? Has the API contract changed on their end? Will upgrading this tool break your pipeline?</li> <li>Copy / pasted code (and bugs): Most likely, once you've got one pipeline built, you're going to be copy and pasting that same integration code into other services, apps and teams pipelines. Every copy and paste introduces redundant code and repeated bugs across multiple teams and departments.</li> <li>Switching tools: Want to replace JMeter with NeoLoad or Gatling? Fine - first go away and understand how NeoLoad's APIs work then come back and break the pipeline while you change the tool. Then hope it works!</li> </ol>"},{"location":"keptn-pipelines/#consequences","title":"Consequences","text":"<p>The consequences of the above are stark:</p> <ul> <li>There is no separation of concern between what the pipeline does (logic) and how it implements it (tooling).</li> <li>Pipelines are copy and pasted, bugs and vulnerabilities flowing alongside.</li> <li>Pipelines are fragile: once built, everyone is very reluctant to touch them again in case everything breaks.</li> <li>They become outdated but see point above, everyone is too scared to touch them.</li> <li>DevOps engineers spend more time reacting to \"Help! the pipeline is broken\" than enhancing it.</li> </ul>"},{"location":"keptn-pipelines/#the-solution","title":"The Solution?","text":"<p>How about decoupling the pipeline logic (the what) from the tooling (the how). We could do this in the following way:</p> <ol> <li>Your pipeline uses an event based workflow which says \"do the retrieval workflow\"</li> <li>This event is sent to some co-ordination component</li> <li>The component distributes the event to all listening</li> <li>Someone listening says \"I have the capability to respond to that event. I'll do it\"</li> <li>That individual (or service) responds with: I'm starting. Then, I'm finished and here's your status.</li> <li>The middleware coordinates the status and and extra information back to the pipeline via an event.</li> </ol> <p></p>"},{"location":"keptn-pipelines/#additional-benefits","title":"Additional Benefits","text":"<p>Take another common usecase of a pipeline. At some point you'll want to send an alert outbound to a third party tool to inform someone of a status. It might look like this:</p> <p></p> <p>Again, you will have built specific integrations into that step to tightly couple your email servers and settings with the pipeline.</p> <p>What happens if you want to replace email with a JIRA ticket or a Slack message or a problem notification into a monitoring tool? That's right - rip and replace.</p> <p>What happens if you want to send alerts to two places? Two integrations, two sets of APIs with which to integrate. Two sets of potential bugs. All code managed and updated by you.</p> <p></p>"},{"location":"keptn-pipelines/#a-better-way","title":"A Better Way","text":"<p>How about replacing this logic with a \"send alert\" sequence of tasks? Your pipeline sends that event and whichever tools are configured to listen for that event get to work.</p> <p></p> <p>No longer want email alerts? Fine. Just uninstall the email service (or tell it to stop listening for \"send alert\" events) from the coordination component and your emails stop.</p> <p></p> <p>Zero changes to your pipeline and most likely, someone else has already written the \"slack service\" for you - so zero code upkeep or API knowledge required!</p>"},{"location":"keptn-pipelines/#concerns-about-this-new-flow","title":"Concerns About This New Flow","text":"<p>At this point you probably think two things:</p> <ol> <li>\"This sounds like an event bus. I'll use that.\" - Bear with me, I'll explain why this \"coordination component\" is more than an event bus later in the article.</li> <li>\"What's the catch?\"</li> </ol> <p>My concerns here would be:</p> <ol> <li>I hope the event descriptions, payloads and schemas are not proprietary. I hope that using this workflow doesn't lock me into a particular vendor or methodology.</li> <li>Ideally I hope that there is an open source version of this \"co-ordination component\" available otherwise I'm again locked into a particular vendor.</li> </ol> <p>Both of these concerns are valid, but both can be answered in the negative:</p> <ol> <li>The events that your pipeline send are HTTP based requests in the open source CloudEvents format. No vendor lock-in and tool interoperability guaranteed.</li> <li>The \"co-ordination component\" is also a CNCF Sandbox project called Keptn, so no vendor (or lock-in) there either.</li> </ol>"},{"location":"keptn-pipelines/#one-more-usecase-evaluating-an-slo","title":"One More Usecase: Evaluating an SLO","text":"<p>Imagine the pipeline wishes to evaluate an SLO. There is actually a \"sub\" workflow involved here:</p> <ol> <li>Define the SLIs</li> <li>Define a source (or sources) for the SLI data</li> <li>Retrieve the metrics from the above data source(s)</li> <li>Judge the retrieved SLIs against the SLO</li> <li>Output a result: <code>pass</code>, <code>warning</code> or <code>fail</code></li> </ol> <p>It would be really nice if our tool could encapsulate and understand that sub-sequence so our pipeline only has to ask for an SLO evaluation and the tooling does the rest automatically.</p> <p>Good news, it does!</p>"},{"location":"keptn-pipelines/#just-an-event-bus","title":"Just an Event Bus?","text":"<p>See above - Keptn is not just an event bus. It is much more and exactly what depends on the usecase that you bring to Keptn or your role within an organisation.</p> <p>This post is focused on your first steps of migrating away from an existing pipeline, piece by piece, to Keptn. However you could use Keptn to define your complete end-to-end delivery pipeline.</p> <p>Here is an example of a working Keptn pipeline definition. It has 3 stages: <code>dev</code>, <code>hardening</code> and <code>production</code>.</p> <p>The <code>dev</code> stage contains two tasks: <code>deployment</code> and <code>release</code> (remember that the tooling which actually does these tasks is defined elsewhere). <code>hardening</code> is only triggered when a the <code>delivery finished in dev</code> message is broadcast. Similar is true for production stage but it listens for the <code>delivery sequence in hardening is finished</code> message.</p> <pre><code>apiVersion: spec.keptn.sh/0.2.0\nkind: \"Shipyard\"\nmetadata:\n  name: \"shipyard-example\"\nspec:\n  stages:\n    - name: \"dev\"\n      sequences:\n      - name: \"delivery\"\n        tasks: \n        - name: \"deployment\"\n        - name: \"release\"\n\n    - name: \"hardening\"\n      sequences:\n      - name: \"delivery\"\n        triggeredOn:\n          - event: \"dev.delivery.finished\"\n        tasks: \n        - name: \"deployment\"\n        - name: \"release\"\n\n    - name: \"production\"\n      sequences:\n      - name: \"delivery\"\n        triggeredOn:\n          - event: \"hardening.delivery.finished\"\n        tasks: \n        - name: \"deployment\"\n        - name: \"release\"\n</code></pre>"},{"location":"keptn-pipelines/#in-summary","title":"In Summary","text":"<p>If you're struggling under the weight of multiple complex, duplicated or legacy pipelines. Give Keptn a look.</p> <p>It can do a lot more and is basically a reference SRE template for continuous delivery and automated rollbacks - but more on that in future posts.</p> <p>If you want to know more about Keptn, reach out to me on LinkedIn and we can chat.</p>"},{"location":"keptn-project-service/","title":"Create a Keptn Project and Service","text":"<p>Create a Keptn project and service.</p>"},{"location":"keptn-service-level-providers/","title":"Keptn: Service Level Providers, SLIs and SLOs","text":"<p>What is an SLI Provider in Keptn? How do I define Service Level Indicators (SLIs) and Service Level Objectives (SLOs) in Keptn? How do I build a Keptn quality gate that pushes metrics to Prometheus (or other metric backends)?</p>"},{"location":"keptn-task-lifecycle/","title":"Lifecycle of a Keptn Task","text":"<p>This video explains the lifecycle of a Keptn task.</p>"},{"location":"keptn-trigger-sequence/","title":"Trigger a Keptn Sequence","text":"<p>Trigger a Keptn sequence</p>"},{"location":"keptn-unleash-feature-flags/","title":"Keptn + Unleash = Automated Feature Flag Self Healing for Non Kubernetes Environments","text":"<p>Keptn is an event-based control plane for continuous delivery and automated operations. Unleash is a feature toggling framework. It sounds like these two would play well together, even in non cloud-native environments...</p>"},{"location":"keptn-unleash-feature-flags/#overview","title":"Overview","text":"<p>Self healing software is a cornerstone in software automation maturity. Tooling comes second to the ability and willingness to change the way you create software.</p> <p>Keptn already has an excellent tutorial on running Keptn &amp; Unleash in Kubernetes or OpenShift based environments. The focus of this post is how we can leverage these components in a non-Kubernetes environment.</p> <p>This post will create an application that is coded to include feature flag capability. While disabled, the application will deliver traffic in the normal operating mode ie. serving the content from the application itself. When the feature flag is enabled, the application will instead serve traffic from a static resource (a file hosted on a CDN).</p> <p>The feature flag engine that makes this possible will be Unleash.</p> <p>Keptn will be the orchestration layer responsible for reacting to \"problem events\" and self healing the application by enabling the feature flag.</p> <p>This scenario is used in real-world scenarios when websites wish to include a safety switch that they can toggle in case of emergencies (traffic overload, DDoS attack etc.)</p> <p>Rounding out the system will be a monitoring provider. The monitoring providers job is to inform Keptn whenever there is a problem with the system (in our demo, an increased server-side error rate).</p> <p>Keptn can use metrics from any third party tool, the two most common being Prometheus or Dynatrace. For this demo, we will use Dynatrace.</p>"},{"location":"keptn-unleash-feature-flags/#fast-developer-feedback","title":"Fast Developer Feedback","text":"<p>Not only is it important to have self healing software, but it's imperative that the developers get instant feedback on their code. For this reason, Keptn automatically pushes a stream of comments on to the Dynatrace problem ticket whenever a remediation step is attempted / performed. In this way, the developers can understand exactly what led up to the issue &amp; how Keptn resolved the issue. </p>"},{"location":"keptn-unleash-feature-flags/#architecture-sequence-diagram","title":"Architecture &amp; Sequence Diagram","text":"<p>Here is the system architecture and sequence of events:</p> <ol> <li>Application is monitored by Dynatrace.</li> <li>We will manufacture an increase in error rate to simulate a production issue.</li> <li>Dynatrace identifies the issue and sends a notification to Keptn.</li> <li>Keptn triggers the remediation workflow and informs Unleash to toggle the feature flag to \"on\". Traffic is now served from the CDN rather than the app.</li> <li>Keptn notifies the monitoring solution (Dynatrace) of the remediation attempt.</li> </ol> <p></p>"},{"location":"keptn-unleash-feature-flags/#prerequisites","title":"Prerequisites","text":"<ul> <li>A Dynatrace tenant (free trial available here)</li> <li>A full Keptn installation</li> <li>Dynatrace monitoring and the Dynatrace SLI Provider installed and configured on the Keptn machine.</li> <li>An Ubuntu VM used to host your website and the Unleash Feature Flag service.</li> </ul> <p>Any linux based VM will work, but you will have to modify the instructions to suit your distro.</p>"},{"location":"keptn-unleash-feature-flags/#networking-prerequisites","title":"Networking Prerequisites","text":"<p>For this demo: - The <code>keptn</code> VM will need to allow inbound HTTPS traffic from Dynatrace SaaS. - The <code>application</code> VM (running <code>proxy</code>, <code>app</code>, <code>unleash</code> and <code>postgres</code>) will need to allow inbound HTTP traffic from the <code>keptn</code> machine.</p>"},{"location":"keptn-unleash-feature-flags/#containers","title":"Containers?","text":"<p>I have chosen to deploy these workloads as containers purely for ease during the demo setup. There is nothing here that could not be achieved without using containers.</p> <p>In terms of the demo and outputs, you may entirely ignore the fact that the application and Unleash service are running in containers. It makes no difference to the end result.</p>"},{"location":"keptn-unleash-feature-flags/#install-configure-git-and-docker","title":"Install &amp; Configure Git and Docker","text":"<p>On the application VM, run the following:</p> <pre><code>sudo apt update &amp;&amp; sudo apt install git docker.io -y\nsudo usermod -aG docker $USER\n</code></pre> <p>Launch a new terminal window to pick up the new user permissions. Validate it works with <code>docker ps</code></p> <p>You should see:</p> <pre><code>CONTAINER ID    IMAGE    COMMAND    CREATED    STATUS    PORTS    NAMES\n</code></pre>"},{"location":"keptn-unleash-feature-flags/#clone-repo","title":"Clone Repo","text":"<pre><code>git clone https://github.com/agardnerit/unleashtutorial\ncd unleashtutorial &amp;&amp; chmod +x loadGenErrors.sh\n</code></pre>"},{"location":"keptn-unleash-feature-flags/#create-new-docker-network","title":"Create New Docker Network","text":"<p>This allows containers to talk to each other via their container name.</p> <p>On the application VM, run the following:</p> <pre><code>docker network create agardner-net\n</code></pre>"},{"location":"keptn-unleash-feature-flags/#run-a-postgresdb-for-unleash","title":"Run a PostGresDB for Unleash","text":"<p>On the application VM, run the following:</p> <p><pre><code>docker run -d --name postgres --network agardner-net -e POSTGRES_PASSWORD=mysecretpassword -e POSTGRES_DB=unleash postgres\n</code></pre> Database = <code>unleash</code> Username = <code>postgres</code> Password = <code>mysecretpassword</code></p>"},{"location":"keptn-unleash-feature-flags/#build-run-the-unleash-container","title":"Build &amp; Run the Unleash Container","text":"<p>On the application VM, run the following:</p> <pre><code>docker build -t unleash ./unleash &amp;&amp; docker run -d --name unleash --network agardner-net -e DATABASE_URL=postgres://postgres:mysecretpassword@postgres:5432/unleash unleash\n</code></pre>"},{"location":"keptn-unleash-feature-flags/#build-and-run-the-app","title":"Build and Run the App","text":"<p>On the application VM, run the following:</p> <pre><code>docker build -t app . &amp;&amp; docker run -d --name app --network agardner-net app\n</code></pre>"},{"location":"keptn-unleash-feature-flags/#build-and-run-the-nginx-reverse-proxy","title":"Build and Run the NGINX Reverse Proxy","text":"<p>On the application VM, run the following:</p> <pre><code>docker build -t proxy ./proxy &amp;&amp; docker run -d -p 80:80 --name proxy --network agardner-net -e DT_CUSTOM_PROP=\"keptn_project=website keptn_service=front-end keptn_stage=production\" proxy\n</code></pre>"},{"location":"keptn-unleash-feature-flags/#validate-containers","title":"Validate Containers","text":"<p>Running <code>docker ps</code> should show 4x containers: <code>proxy</code>, <code>app</code>, <code>unleash</code> and <code>postgres</code>.</p> <pre><code>CONTAINER ID    IMAGE   ...  PORTS                NAMES\nc1344de4e69c    proxy        0.0.0.0:80-&gt;80/tcp   proxy\n676935d87028    app                               app\nbe6937f7641c    unleash      4242/tcp             unleash\nfee962f54612    postgres     5432/tcp             postgres\n</code></pre>"},{"location":"keptn-unleash-feature-flags/#validate-user-interfaces","title":"Validate User Interfaces","text":"<ul> <li>The Unleash UI should now be available on <code>http://&lt;APP-VM-IP&gt;/unleash</code></li> <li>The app should now be available on <code>http://&lt;APP-VM-IP&gt;</code></li> </ul> <p>Validate that both of these are available by visiting them in a browser.</p> <p>You can login to unleash with any username &amp; password.</p> <p></p> <p></p>"},{"location":"keptn-unleash-feature-flags/#validate-dynatrace-tags","title":"Validate Dynatrace Tags","text":"<p>In your Dynatrace tenant, open the <code>Transactions and Services</code> page, select the <code>Keptn website production</code> management zone to filter your services and navigate to the <code>unleash-demo</code> service.</p> <p>Ensure that your service is tagged with the following:</p> <p><code>keptn_project:website</code>, <code>keptn_service:front-end</code> and <code>keptn_stage:production</code></p> <p></p> <p>These tags are created when you installed the Dynatrace service on Keptn . If you do not see these tags, please STOP and ensure you follow this instructions linked above.</p> <p>This tutorial WILL NOT WORK without these tags.</p>"},{"location":"keptn-unleash-feature-flags/#validate-problem-notification-integration","title":"Validate Problem Notification Integration","text":"<p>Keptn automatically configures the problem notification integration when you onboard the Dynatrace Service.</p> <p>Validate that it's available now. In Dynatrace, go to <code>Settings &gt; Integration &gt; Problem Notifications</code> and you should see an entry for Keptn. If you do not see this problem notification, STOP and ensure you've installed Dynatrace on the keptn box.</p> <p></p> <p>This tutorial WILL NOT WORK without this integration.</p>"},{"location":"keptn-unleash-feature-flags/#configure-problem-sensitivity","title":"Configure Problem Sensitivity","text":"<p>For demo purposes, we will set Dynatrace to be extremely sensitive to failures. Find the <code>unleash-demo:80</code> nginx service, edit the anomaly detection settings and adjust the settings to match these:</p> <p></p>"},{"location":"keptn-unleash-feature-flags/#create-feature-flag","title":"Create Feature Flag","text":"<ul> <li>Go to <code>http://&lt;APP-VM-IP&gt;/unleash</code> and login (use any fake values you like to login)</li> <li>Create a feature flag called <code>EnableStaticContent</code> (case sensitive and must be called this).</li> <li>Set the flag to <code>disabled</code></li> <li>Refresh the application UI and you should still see the standard (blue bar) page.</li> </ul>"},{"location":"keptn-unleash-feature-flags/#manually-test-flag","title":"Manually Test Flag","text":"<p>Prove that the feature flag works:</p> <ul> <li>Go to the app (<code>http://&lt;APP-VM-IP&gt;</code>) and refresh the page. You should still see the blue banner. This page is served from the <code>app</code> container.</li> <li>Enable the feature flag and refresh the app. Notice the green banner, this page is served from GitHub.</li> </ul> <p></p> <p></p> <p>Set the flag back to <code>disabled</code> so that traffic is being served by the app (blue banner).</p> <p></p> <p></p>"},{"location":"keptn-unleash-feature-flags/#clone-repo-to-keptn-machine-create-keptn-project-service","title":"Clone Repo to Keptn Machine, Create Keptn Project &amp; Service","text":"<p>Execute these commands on the Keptn cluster:</p> <pre><code>cd ~\ngit clone http://github.com/agardnerit/unleashtutorial\ncd unleashtutorial\nkeptn create project website --shipyard=shipyard.yaml\nkeptn create service front-end --project=website\nkeptn add-resource --project=website --service=front-end --stage=production --resource=remediations.yaml --resourceUri=remediation.yaml\n</code></pre> <p>The values in the <code>remediations.yaml</code> file tell Keptn how to respond when it sees a failure rate increase problem for this project (<code>website</code>), service (<code>front-end</code>) and stage (<code>production</code>).</p>"},{"location":"keptn-unleash-feature-flags/#create-secret-bounce-remediation-service","title":"Create Secret &amp; Bounce Remediation Service","text":"<p>Note that the <code>username</code> and <code>token</code> can be set to anything.</p> <p>The <code>remediation-service</code> pod must be recreated so that it picks up this new secret.</p> <p>Execute these commands on the Keptn cluster:</p> <pre><code>kubectl create secret -n keptn generic unleash --from-literal=\"UNLEASH_SERVER_URL=http://&lt;APP-VM-IP&gt;/unleash/api\" --from-literal=\"UNLEASH_USER=me\" --from-literal=\"UNLEASH_TOKEN=whatever\"\nkubectl scale deployment -n keptn remediation-service --replicas=0 &amp;&amp; kubectl scale deployment -n keptn remediation-service --replicas=1\n</code></pre>"},{"location":"keptn-unleash-feature-flags/#load-generator","title":"Load Generator","text":"<p>Run the load generator which will create errors. In another tab, keep refreshing the page and in a few minutes (when DT raises a problem) you'll see the website failover to the green static hosted content.</p> <p>Run this on the application VM hosting the website:</p> <pre><code>cd ~/unleashtutorial\n./loadGenErrors.sh\n</code></pre> <ul> <li>You will see <code>HTTP 500</code> errors being shown. The failure rate registered by Dynatrace on the <code>unleash-demo:80</code> service will also increase.</li> </ul> <pre><code>$ ./loadGenErrors.sh \n&lt;!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 3.2 Final//EN\"&gt;\n&lt;title&gt;500 Internal Server Error&lt;/title&gt;\n&lt;h1&gt;Internal Server Error&lt;/h1&gt;\n&lt;p&gt;The server encountered an internal error and was unable to complete your request. Either the server is overloaded or there is an error in the application.&lt;/p&gt;\n-----------------\n</code></pre> <ul> <li>After about 10 minutes, Dynatrace will register a problem and push a notification to Keptn.</li> <li>The Keptn <code>remediation-service</code> will activate and toggle the feature based on the <code>remediations.yaml</code> file.</li> <li>The feature flag will be <code>enabled</code> and the CDN page will be served (from GitHub) (this is the actual page)</li> </ul> <p></p> <p></p>"},{"location":"keptn-unleash-feature-flags/#bonus-synced-jira-tickets","title":"Bonus: Synced JIRA Tickets","text":"<p>I have the JIRA Service and the Dynatrace for JIRA Cloud plugin installed into the Keptn cluster. Combined, these two do the following:</p> <ol> <li>Create a JIRA ticket whenever a problem event occurs. This JIRA ticket contains all the details of the problem.</li> <li>Sync comments bi-directionally between Dynatrace and JIRA.</li> <li>Link directly to the Keptn's bridge and the Dynatrace problem from the ticket.</li> </ol> <p>My developers now have an automatically created record of the problem and all remediation actions, right within their normal JIRA workflow. They can reply to the ticket from within Dynatrace or the ticket itself, everyone has all relevant details instantly, whichever tool they use.</p> <p>Don't use JIRA? Keptn is plug &amp; play so just switch the service out for a different one such as Slack or ServiceNow (or write your own service for any other tool).</p>"},{"location":"keptn-unleash-feature-flags/#conclusion","title":"Conclusion","text":"<p>Although Keptn and Unleash are primarily designed for Kubernetes-based environments, there is no reason they (and the automation concepts described) cannot be used in a non-containerised environment.</p> <p>Willingness to change the way you design and write software. Willingness to empower developers. Ability to quickly get relevant, actionable feedback to developers. Three key ingredients to a successful automation strategy.</p> <p>Tooling helps, but culture is much more important on your software automation journey. The three big ones are:</p> <ol> <li>A willingness to rethink the way you design and write software.</li> <li>A willingness to empower development teams to take responsibility for the code they produce.</li> <li>The ability to get relevant, actionable feedback to developers - quickly.</li> </ol>"},{"location":"keptn-webhook-integrations/","title":"Keptn: Integrate Tools using Webhooks","text":"<p>Any tool that accepts webhooks can be integrated with Keptn. This video explains how.</p>"},{"location":"kubecon-2023-colocated-events/","title":"Kubecon North America 2023 Colocated Events","text":"<p>The co-located events for Kubecon North America 2023 have been announced. Registration and call for proposals is open now.</p> <p>This year, the colocated events are all held on November 6th 2023:</p> <ul> <li>App Developer Con 2023</li> <li>Argo Con</li> <li>Backstage Con</li> <li>DBaaS Dev Day</li> <li>Cilium Con</li> <li>Cloud Native WASM Day</li> <li>DOK Day</li> <li>Envoy Con</li> <li>Istio Day</li> <li>Kubernetes AI and High Performance Computing Day</li> <li>Kubernetes on Edge Day</li> <li>Multi-tenancy Con</li> <li>Observability Day</li> </ul> <p>Furthe information at the CNCF overview page to register and submit your papers.</p>"},{"location":"kubernetes-cluster-mapping/","title":"Generate a realtime map of a Kubernetes Cluster with this tool","text":"<p>Did you know it's possible to generate a real-time map of your Kubernetes cluster using DNS? Here's how...</p> <p>In this video I demonstrate how the Otterize network mapper can be used to generate a map of service to service communication on your Kubernetes cluster.</p> <p>I then take the output and visualise it with a little Python app I wrote. The code for which is available here.</p>"},{"location":"kubernetes-cluster-on-aws/","title":"Kubernetes Cluster on AWS","text":"<p>Create a single node Kubernetes cluster, fully integrated with AWS (automatic Load Balancers for frontend services) for testing purposes\u2026</p>"},{"location":"kubernetes-cluster-on-aws/#prerequisites","title":"Prerequisites","text":"<ul> <li>An AWS account.</li> <li>An SSH Keypair created in AWS and have the PEM file stored locally.</li> <li>Your AWS Access Key &amp; Secret Key</li> </ul> <p>Following this tutorial will incur AWS costs.</p>"},{"location":"kubernetes-cluster-on-aws/#install-kubectl","title":"Install Kubectl","text":"<p><code>kubectl</code> (kubernetes control) is the command line utility you will use to interact with your Kubernetes cluster once it\u2019s up &amp; running.</p> <p>Depending on your operating system, installation will obviously be different but the official instructions here are a great place to start.</p> <p>Before you proceed, ensure you can successfully run the command <code>kubectl version</code> in your command line.</p> <p>Note: You might see this warning. It\u2019s safe to ignore.</p> <p>The connection to the server localhost:8080 was refused - did you specify the right host or port?</p>"},{"location":"kubernetes-cluster-on-aws/#generate-public-key-from-pem","title":"Generate Public Key From PEM","text":"<p>Ensure your pem has <code>0600</code> permissions: <code>chmod 0600 key.pem</code></p> <p>Generate the public key from your private key PEM file:</p> <p>{% raw %} <pre><code>ssh-keygen -y -f key.pem &gt; key.pub\n</code></pre> {% endraw %}</p>"},{"location":"kubernetes-cluster-on-aws/#install-eksctl-utility","title":"Install EKSCTL Utility","text":"<p><code>eksctl</code> (Amazon Elastic Kubernetes Service Control) is a command line utility which makes it easy to interact with the AWS EKS service.</p> <p>Installing <code>eksctl</code> is very easy, depending on your OS:</p>"},{"location":"kubernetes-cluster-on-aws/#standard-linux-install","title":"Standard Linux Install","text":"<p>{% raw %} <pre><code>curl --silent --location \"https://github.com/weaveworks/eksctl/releases/download/latest_release/eksctl_$(uname -s)_amd64.tar.gz\" | tar xz -C /tmp\nsudo mv /tmp/eksctl /usr/local/bin\n</code></pre> {% endraw %}</p>"},{"location":"kubernetes-cluster-on-aws/#macos-install-using-homebrew","title":"MacOS Install Using Homebrew","text":"<p>{% raw %} <pre><code>brew tap weaveworks/tap\nbrew install weaveworks/tap/eksctl\n</code></pre> {% endraw %}</p>"},{"location":"kubernetes-cluster-on-aws/#windows-install-using-chocolatey","title":"Windows Install Using Chocolatey","text":"<p>{% raw %} <pre><code>chocolatey install eksctl\n</code></pre> {% endraw %}</p> <p>Before proceeding, ensure you can successfully run <code>eksctl version</code> in a command line.</p>"},{"location":"kubernetes-cluster-on-aws/#authenticating-eksctl","title":"Authenticating EKSCTL","text":"<p>Since <code>eksctl</code> will be interacting with AWS on our behalf, we need to ensure it has permissions to authenticate.</p> <p>Ensure you have the following folder structure &amp; file created. If you\u2019ve previously used any other tool that interacts with AWS (kops, Terraform etc) then chances are you\u2019ll be good to go &amp; won\u2019t need to recreate anything.</p> <p>Make sure the <code>~/.aws</code> folder exists and create a file called <code>credentials</code> inside. Edit the <code>credentials</code> file to include your AWS Access Key and AWS Secret Key.</p> <p>Your credentials file should look like this:</p> <p>{% raw %} <pre><code>[default]\naws_access_key_id=AKIA***********\naws_secret_access_key=oRsRbKc*********\n</code></pre> {% endraw %}</p>"},{"location":"kubernetes-cluster-on-aws/#cluster-definition-file","title":"Cluster Definition File","text":"<p>Create a cluster definition file. <code>eksctl</code> will use this file to define exactly how we want our cluster to be created. The file can be called anything (as long as it\u2019s saved with the <code>.yaml</code> extension).</p> <p>I\u2019ll call mine <code>clusterDef.yaml</code> and store it in my home directory <code>~/clusterDef.yaml</code></p> <p>Note on YAML: YAML does not work with tabs. Use space characters only.Use YAMLLint to validate your YAML file format.</p> <p>{% raw %} <pre><code>apiVersion: eksctl.io/v1alpha5\nkind: ClusterConfig\n\nmetadata:\n  name: single-node-cluster\n  region: us-east-2\n\nnodeGroups:\n  - name: node-group-1\n    instanceType: t2.medium\n    desiredCapacity: 1\n    ssh:\n      publicKeyPath: /path/to/key.pub\n</code></pre> {% endraw %}</p> <p>Hopefully this should be fairly self explanatory. We create a cluster called <code>single-node-cluster</code> in the <code>us-east-2</code> region which consists of a single node (<code>desiredCapacity: 1</code>) which is a <code>t2.medium</code> sized node. We also copy our public key onto the instance so that we can SSH into the node if needed (although that shouldn\u2019t be required).</p>"},{"location":"kubernetes-cluster-on-aws/#create-cluster","title":"Create Cluster","text":"<p>All that\u2019s left now is to actually create the cluster. <code>eksctl</code> makes this easy:</p> <p>{% raw %} <pre><code>eksctl create cluster -f ~/clusterDef.yaml\n</code></pre> {% endraw %}  {% raw %} <pre><code>$ eksctl create cluster -f /path/to/clusterDef.yaml\n[i]  using region us-east-1\n[i]  setting availability zones to [us-east-1b us-east-1c]\n[i]  subnets for us-east-1b - public:192.168.0.0/19 private:192.168.64.0/19\n[i]  subnets for us-east-1c - public:192.168.32.0/19 private:192.168.96.0/19\n[i]  nodegroup \"node-group-1\" will use \"ami-0f2e8e5663e16b436\" [AmazonLinux2/1.13]\n[i]  using SSH public key \"/path/to/file.pub\" as \"eksctl-single-node-cluster-nodegroup-node-group-1-6f:84:05:83:7e:99:89:67:69:a6:7d:72:d7:0a:a4:c7\" \n[i]  using Kubernetes version 1.13\n[i]  creating EKS cluster \"single-node-cluster\" in \"us-east-1\" region\n[i]  1 nodegroup (node-group-1) was included\n[i]  will create a CloudFormation stack for cluster itself and 1 nodegroup stack(s)\n[i]  if you encounter any issues, check CloudFormation console or try 'eksctl utils describe-stacks --region=us-east-1 --name=single-node-cluster'\n[i]  2 sequential tasks: { create cluster control plane \"single-node-cluster\", create nodegroup \"node-group-1\" }\n[i]  building cluster stack \"eksctl-single-node-cluster-cluster\"\n[i]  deploying stack \"eksctl-single-node-cluster-cluster\"\n[i]  building nodegroup stack \"eksctl-single-node-cluster-nodegroup-node-group-1\"\n[i]  --nodes-min=1 was set automatically for nodegroup node-group-1\n[i]  --nodes-max=1 was set automatically for nodegroup node-group-1\n[i]  deploying stack \"eksctl-single-node-cluster-nodegroup-node-group-1\"\n[\u2714]  all EKS cluster resource for \"single-node-cluster\" had been created\n[\u2714]  saved kubeconfig as \"/.../.kube/config\"\n[i]  adding role \"arn:aws:iam::782430655514:role/eksctl-single-node-cluster-nodegr-NodeInstanceRole-DJGENWOS1P4E\" to auth ConfigMap\n[i]  nodegroup \"node-group-1\" has 0 node(s)\n[i]  waiting for at least 1 node(s) to become ready in \"node-group-1\"\n[i]  nodegroup \"node-group-1\" has 1 node(s)\n[i]  node \"ip-192-168-10-142.ec2.internal\" is ready\n[i]  kubectl command should work with \"/.../.kube/config\", try 'kubectl get nodes'\n[\u2714]  EKS cluster \"single-node-cluster\" in \"us-east-1\" region is ready\n</code></pre> {% endraw %}</p>"},{"location":"kubernetes-cluster-on-aws/#delete-cluster","title":"Delete Cluster","text":"<p>When you\u2019re finished, delete the cluster with this command:</p> <p>{% raw %} <pre><code>eksctl delete cluster -f ~/clusterDef.yaml\n</code></pre> {% endraw %}</p> <p>Deleting the cluster will not delete any resources that weren\u2019t defined in the <code>clusterDef</code> file. In addition, deleting the cluster in this manner is still a bit buggy. Always double check that everything has actually been deleted to ensure you aren\u2019t getting charged for leftover resources.</p> <p>{% raw %} <pre><code>$ eksctl delete cluster -f ~/Documents/k8s/clusterDef.yaml \n[i]  using region us-east-1\n[i]  deleting EKS cluster \"single-node-cluster\"\n[\u2714]  kubeconfig has been updated\n[i]  cleaning up LoadBalancer services\n[i]  2 sequential tasks: { delete nodegroup \"node-group-1\", delete cluster control plane \"single-node-cluster\" [async] }\n[i]  will delete stack \"eksctl-single-node-cluster-nodegroup-node-group-1\"\n[i]  waiting for stack \"eksctl-single-node-cluster-nodegroup-node-group-1\" to get deleted\n[i]  will delete stack \"eksctl-single-node-cluster-cluster\"\n[\u2714]  all cluster resources were deleted\n</code></pre> {% endraw %}</p>"},{"location":"kubernetes-cluster-on-aws/#summary","title":"Summary","text":"<p>This post gives you a quick and easy way to spin up an on-demand Kubernetes cluster in AWS for test / demo purposes.</p>"},{"location":"log-processing-top-10/","title":"Log Processing Top 10 with the OpenTelemetry Collector","text":"<p>In this video I cover the top 10 ways and methods that I find myself using at customers to process their log files using the OpenTelemetry collector.</p>"},{"location":"management-zones-autonomous-monitoring/","title":"Automatic Management Zones - Autonomous Monitoring","text":"<p>Part 6 of my autonomous cloud tutorial series. This post will use the Dynatrace API to automatically create and update management zones.</p> <p>This tutorial series builds from previous tutorials. I recommend you complete parts 1 through 4 first:</p> <ul> <li>Part one (Host Group Naming)</li> <li>Part two (Host Metadata)</li> <li>Part three (Service &amp; Process Group Naming)</li> <li>Part four (Automatic Tag Rules)</li> <li>Part five (Automatic Application Definitions)</li> </ul>"},{"location":"management-zones-autonomous-monitoring/#recap","title":"Recap","text":"<p>If you\u2019ve followed previous tutorials, you now have 2 EC2 instances. Each running an Apache (<code>httpd</code>) webserver. One webserver serves the <code>staging</code> site, the other serves the <code>production</code> site.</p> <p>We\u2019ve associated metadata with the Dynatrace entities such as an <code>Owner</code>, <code>Location</code> and <code>ChargeCode</code>. We\u2019ve also created rules within Dynatrace which transforms this metadata into Key / Value tags. Finally, we\u2019ve defined two web applications (<code>staging</code> &amp; <code>production</code>). This tells Dynatrace how to categorise the traffic coming from both webservers. All <code>staging</code> traffic into one application and all <code>production</code> traffic into the other application.</p>"},{"location":"management-zones-autonomous-monitoring/#management-zones","title":"Management Zones","text":"<p>Management zones are an incredibly useful and flexible concept within Dynatrace. They provide flexibility by allowing you to segment a large environment into any sub divisions, based on tag values.</p> <p>For example, you probably want a management zone per environment (show me only <code>staging</code> vs. <code>production</code>). You may also want to segment by <code>Owner</code> (when Alice logs in, she should only see her environment).</p> <p>You can also segment at different layers of the smartscape model, so a business owner might only wish to see their applications and services for production. Another group of users may only need to see the infrastructure layer metrics of staging.</p> <p>User permissions, alerting notifications and dashboards all respect the boundaries defined by management zones \u2013 so they\u2019re also useful as filters.</p> <p>Create a single \"global\" dashboard and then use the management zone filter to show only what you care about, all in real time. Onboard a new environment? Just create a new management zone and all your old dashboards still work!</p>"},{"location":"management-zones-autonomous-monitoring/#lets-do-it","title":"Let's Do It","text":"<p>Dynatrace offers an API to manage management zones. Therefore, we will hold our management zones as JSON in a new file called <code>managementZones.json</code></p>"},{"location":"management-zones-autonomous-monitoring/#managementzonesjson","title":"managementZones.json","text":"<p>{% raw %} <pre><code>[{\n\"id\": \"1\",\n  \"name\": \"Staging\",\n  \"rules\": [{\n      \"type\": \"SERVICE\",\n      \"enabled\": true,\n      \"propagationTypes\": [\n        \"SERVICE_TO_PROCESS_GROUP_LIKE\",\n        \"SERVICE_TO_HOST_LIKE\"\n      ],\n      \"conditions\": [{\n          \"key\": {\n            \"attribute\": \"HOST_TAGS\"\n          },\n          \"comparisonInfo\": {\n            \"type\": \"TAG\",\n            \"operator\": \"EQUALS\",\n            \"value\": {\n              \"context\": \"CONTEXTLESS\",\n              \"key\": \"Environment\",\n              \"value\": \"staging\"\n            },\n            \"negate\": false\n          }\n        }]\n    },\n    {\n      \"type\": \"WEB_APPLICATION\",\n      \"enabled\": true,\n      \"propagationTypes\": [],\n      \"conditions\": [{\n          \"key\": {\n            \"attribute\": \"WEB_APPLICATION_TAGS\"\n          },\n          \"comparisonInfo\": {\n            \"type\": \"TAG\",\n            \"operator\": \"TAG_KEY_EQUALS\",\n            \"value\": {\n              \"context\": \"CONTEXTLESS\",\n              \"key\": \"mysite-staging\"\n            },\n            \"negate\": false\n          }\n        }]\n    }]\n}, {\n\"id\": \"2\",\n  \"name\": \"Production\",\n  \"rules\": [{\n      \"type\": \"SERVICE\",\n      \"enabled\": true,\n      \"propagationTypes\": [\n        \"SERVICE_TO_PROCESS_GROUP_LIKE\",\n        \"SERVICE_TO_HOST_LIKE\"\n      ],\n      \"conditions\": [{\n          \"key\": {\n            \"attribute\": \"HOST_TAGS\"\n          },\n          \"comparisonInfo\": {\n            \"type\": \"TAG\",\n            \"operator\": \"EQUALS\",\n            \"value\": {\n              \"context\": \"CONTEXTLESS\",\n              \"key\": \"Environment\",\n              \"value\": \"production\"\n            },\n            \"negate\": false\n          }\n        }]\n    },\n    {\n      \"type\": \"WEB_APPLICATION\",\n      \"enabled\": true,\n      \"propagationTypes\": [],\n      \"conditions\": [{\n          \"key\": {\n            \"attribute\": \"WEB_APPLICATION_TAGS\"\n          },\n          \"comparisonInfo\": {\n            \"type\": \"TAG\",\n            \"operator\": \"TAG_KEY_EQUALS\",\n            \"value\": {\n              \"context\": \"CONTEXTLESS\",\n              \"key\": \"mysite-production\"\n            },\n            \"negate\": false\n          }\n        }]\n    }]\n}, {\n  \"id\": \"3\",\n  \"name\": \"Owner: Alice\",\n  \"rules\": [\n    {\n      \"type\": \"SERVICE\",\n      \"enabled\": true,\n      \"propagationTypes\": [\n        \"SERVICE_TO_PROCESS_GROUP_LIKE\",\n        \"SERVICE_TO_HOST_LIKE\"\n      ],\n      \"conditions\": [\n        {\n          \"key\": {\n            \"attribute\": \"SERVICE_TAGS\"\n          },\n          \"comparisonInfo\": {\n            \"type\": \"TAG\",\n            \"operator\": \"EQUALS\",\n            \"value\": {\n              \"context\": \"CONTEXTLESS\",\n              \"key\": \"Owner\",\n              \"value\": \"Alice\"\n            },\n            \"negate\": false\n          }\n        }]\n    }, {\n      \"type\": \"WEB_APPLICATION\",\n      \"enabled\": true,\n      \"propagationTypes\": [],\n      \"conditions\": [\n        {\n          \"key\": {\n            \"attribute\": \"WEB_APPLICATION_TAGS\"\n          },\n          \"comparisonInfo\": {\n            \"type\": \"TAG\",\n            \"operator\": \"TAG_KEY_EQUALS\",\n            \"value\": {\n              \"context\": \"CONTEXTLESS\",\n              \"key\": \"mysite-staging\"\n            },\n            \"negate\": false\n          }\n        }]\n    }]\n}]\n</code></pre> {% endraw %}</p>"},{"location":"management-zones-autonomous-monitoring/#json-explanation","title":"JSON Explanation","text":"<p>The file above contains 3 management zones. First is called Staging. Second is called Production. Third is called Owner: Alice. All 3 are enabled (\"enabled\": true).</p>"},{"location":"management-zones-autonomous-monitoring/#staging-management-zone","title":"Staging Management Zone","text":"<p>This management zone looks at the tags on the service level of the Smartscape topology (<code>\"rules\": [{ \"type\": \"SERVICE\"...</code>). Whatever matches at this level, we\u2019ve also told Dynatrace to include everything below this, at the process group and host level (<code>\"propagationTypes\": [ \"SERVICE_TO_PROCESS_GROUP_LIKE\", \"SERVICE_TO_HOST_LIKE\"]</code>).</p> <p>We\u2019re looking at any entities that have a tag with the Key: Value pair of (<code>Environment: Staging</code>). Notice that this tag is <code>\"contextless\"</code>. If we were matching against tags imported from a third party, this field might read AWS instead.</p> <p>We add a second rule to match against any web applications (<code>\"type\": \"WEB_APPLICATION\"</code>) that are tagged with the <code>mysite-staging</code> tag. Recall that we defined all of these tags in a previous tutorial.</p>"},{"location":"management-zones-autonomous-monitoring/#production-management-zone","title":"Production Management Zone","text":"<p>The production management zone is an almost identical definition to staging, except that the Value of the KV pair is set to <code>production</code> rather than <code>staging</code>. Similarly, the application tag is <code>mysite-production</code> rather than <code>mysite-staging</code>.</p>"},{"location":"management-zones-autonomous-monitoring/#owner-alice-management-zone","title":"Owner: Alice Management Zone","text":"<p>This management zone uses the same structure as the <code>staging</code> and <code>production</code> management zones, but the values are different. This time, the tag Key / Value pair is set to <code>Owner</code> and <code>Alice</code> respectively.</p> <p>In addition, because we know our system, we also know that Alice is responsible for the staging environment, so we include any web applications with the tag <code>mysite-staging</code> into Alice\u2019s view of the world.</p>"},{"location":"management-zones-autonomous-monitoring/#the-playbook","title":"The Playbook","text":"<p>You\u2019ll need to create an API token with <code>write configuration</code> access. Then replace the <code>tenant_url</code> with your own.</p> <p>Save the following playbook (substituting in your values) as <code>createManagementZone.yaml.</code></p> <p>{% raw %} <pre><code>---\n- name: Create Management Zones\n  hosts: 127.0.0.1\n  gather_facts: no\n\n  vars:\n    tenant_url: ***.live.dynatrace.com\n    dynatrace_api_write_token: ***\n    management_zones: \"{{ lookup('file', 'managementZones.json') }}\"\n\n  tasks:\n\n  - name: Create or Update Management Zones\n    uri:\n      method: PUT\n      status_code: 201, 204\n      url: \"https://{{ tenant_url }}/api/config/v1/managementZones/{{ item.id }}\"\n      headers:\n        Authorization: Api-Token {{ dynatrace_api_write_token }}\n      body_format: json\n      body: \"{{ item }}\"\n    loop: \"{{ management_zones }}\"\n</code></pre> {% endraw %}</p>"},{"location":"management-zones-autonomous-monitoring/#playbook-explanation","title":"Playbook Explanation","text":"<p>This playbook is really simple &amp; the logic can easily be ported to any other non-Ansible system.</p> <p>We tell ansible to run the playbook locally (from the control machine) \u2013 because we\u2019re only doing API calls. We define some variables up front and use these variables any time you see the double curly braces (eg. <code>{{ my_variable }}</code>).</p> <p>The playbook consists of a single task which loops through each management zone. item is a special ansible keyword meaning the \"thing\" we\u2019re looping through (ie. each management zone).</p> <p>For each iteration of the loop, we\u2019ll do a <code>PUT</code> operation to Dynatrace and pass it the JSON exactly as it\u2019s defined in <code>managementZones.json</code>. Dynatrace will return either a <code>201</code> or <code>204</code> status code for a successful <code>PUT</code> request, so we tell Ansible that those codes mean a success.</p>"},{"location":"management-zones-autonomous-monitoring/#results","title":"Results","text":"<p>If you\u2019ve followed this blog post correctly, you should have two files in your directory. The JSON file called managementZones.json and the playbook (above) saved as createManagementZone.yaml.</p> <p>Now run the playbook: <code>ansible-playbook createManagementZone.yaml</code></p> <p>Notice that you now have 3 management zones available in your enviroment:</p> <p></p> <p>Also notice that by default, dashboards show all management zones:</p> <p></p> <p>You can filter dashboards (and other views) via the filter icon on the top bar to show only a subset. I can see all management zones because I\u2019m an admin in my environment. Users will only be able to see management zones that they have permission to see:</p> <p></p> <p>In the image above, Alice only sees one host CPU figure, one host status, one application and one service \u2013 because she only has access to the staging environment \u2013 not production.</p> <p>Here\u2019s an example of a permission that only allows <code>Alice's Team</code> read-only access to the <code>Owner: Alice</code> management zone:</p> <p></p>"},{"location":"manual-tracing-opentelemetry-python/","title":"Manual OpenTelemetry Tracing in Python: Beginners Guide","text":"<p>In this post, you will take your first steps to manually instrumenting a Python application using OpenTelemetry.</p> <p>I recently posted a video (see below) and this is the text version for those who prefer to read instead.</p> <p>If you got here and want to watch, just click the image below.</p> <p></p>"},{"location":"manual-tracing-opentelemetry-python/#start-with-the-basics","title":"Start with the Basics","text":"<p>Start by writing a very simple function called <code>add</code> which takes two parameters: <code>first</code> and <code>second</code>. The function just adds these two numbers and returns them to you.</p> <p>Give yourself a way to run it too (with the <code>__name__ == \"__main__\"</code> block).</p> <p>Save this code as <code>app.py</code>.</p> <pre><code>def add(first, second):\n    return first + second\n\nif __name__ == \"__main__\":\n    return_value = add(11, 3)\n    print(f\"The return value is: {return_value}\")\n</code></pre> <p>Run the code with <code>python app.py</code> and you should see:</p> <pre><code>The return value is: 14\n</code></pre>"},{"location":"manual-tracing-opentelemetry-python/#opentelemetry-requirements","title":"OpenTelemetry Requirements","text":"<p>We want to track how long it takes to add those two numbers, so start adding the OpenTelemetry code now.</p> <p>Create a file called <code>requirements.txt</code> and add this content (note: Go to PyPi to get the latest version numbers).</p> <pre><code>opentelemetry-sdk == 1.24.0\nopentelemetry-api == 1.24.0\n</code></pre> <p>Install these dependencies now: <code>pip install -r requirements.txt</code></p>"},{"location":"manual-tracing-opentelemetry-python/#the-opentelemetry-boilerplate","title":"The OpenTelemetry Boilerplate","text":"<p>Copy the boilerplate code from this page and paste it above your existing code.</p> <p>Your <code>app.py</code> should now look like this:</p> <pre><code>from opentelemetry import trace\nfrom opentelemetry.sdk.trace import TracerProvider\nfrom opentelemetry.sdk.trace.export import (\n    BatchSpanProcessor,\n    ConsoleSpanExporter,\n)\n\nprovider = TracerProvider()\nprocessor = BatchSpanProcessor(ConsoleSpanExporter())\nprovider.add_span_processor(processor)\n\n# Sets the global default tracer provider\ntrace.set_tracer_provider(provider)\n\n# Creates a tracer from the global tracer provider\ntracer = trace.get_tracer(\"my.tracer.name\")\n\ndef add(first, second):\n    return first + second\n\nif __name__ == \"__main__\":\n    return_value = add(11, 3)\n    print(f\"The return value is: {return_value}\")\n</code></pre> <p>This code sets up OpenTelemetry, ready to trace and uses the <code>ConsoleSpanExporter</code> to tell OpenTelemetry to output the spans to, unsurprisingly, the console window.</p> <p>If you re-run this code now, nothing new will happen. We need one more step.</p>"},{"location":"manual-tracing-opentelemetry-python/#tell-opentelemetry-to-create-the-span","title":"Tell OpenTelemetry to Create the Span","text":"<p>A span is a single unit of work. For example, a span may track a single function call (as in our example) or you may decided to split a longer function into multiple spans. That's up to you.</p> <p>There are two ways to create a span. Using a <code>with</code> block or using a decorator. We will use a decorator.</p> <p>Above this line: <code>def add(first, second):</code> add the following:</p> <pre><code>@tracer.start_as_current_span(\"add\")\n</code></pre> <p>This tells the OpenTelemetry tracer (defined on line 16) that when the <code>add</code> function is executed, OpenTelemetry should create a span (which starts and ends when the function does) and the span should also be called <code>add</code>.</p> <p>Your code should now look like this:</p> <pre><code>from opentelemetry import trace\nfrom opentelemetry.sdk.trace import TracerProvider\nfrom opentelemetry.sdk.trace.export import (\n    BatchSpanProcessor,\n    ConsoleSpanExporter,\n)\n\nprovider = TracerProvider()\nprocessor = BatchSpanProcessor(ConsoleSpanExporter())\nprovider.add_span_processor(processor)\n\n# Sets the global default tracer provider\ntrace.set_tracer_provider(provider)\n\n# Creates a tracer from the global tracer provider\ntracer = trace.get_tracer(\"my.tracer.name\")\n\n@tracer.start_as_current_span(\"add\")\ndef add(first, second):\n    return first + second\n\nif __name__ == \"__main__\":\n    return_value = add(11, 3)\n    print(f\"The return value is: {return_value}\")\n</code></pre> <p>The output should look similar to this:</p> <pre><code>The return value is: 14\n{\n    \"name\": \"add\",\n    \"context\": {\n        \"trace_id\": \"0xf9030d21e7e46e03dc17605e38976d0d\",\n        \"span_id\": \"0x089a2d951c598f4e\",\n        \"trace_state\": \"[]\"\n    },\n    \"kind\": \"SpanKind.INTERNAL\",\n    \"parent_id\": null,\n    \"start_time\": \"2024-04-13T06:43:07.534014Z\",\n    \"end_time\": \"2024-04-13T06:43:07.534023Z\",\n    \"status\": {\n        \"status_code\": \"UNSET\"\n    },\n    \"attributes\": {},\n    \"events\": [],\n    \"links\": [],\n    \"resource\": {\n        \"attributes\": {\n            \"telemetry.sdk.language\": \"python\",\n            \"telemetry.sdk.name\": \"opentelemetry\",\n            \"telemetry.sdk.version\": \"1.24.0\",\n            \"service.name\": \"unknown_service\"\n        },\n        \"schema_url\": \"\"\n    }\n}\n</code></pre> <p>\ud83c\udf89 Congratulations! \ud83c\udf89 That JSON blob is your OpenTelemetry span.</p> <p>Stay tuned (or subscribe to my YouTube channel) for the next tutorial where I explain each field and enrich the span with events, metadata and a correct status code. See you then!</p>"},{"location":"migrate-digitalocean-wordpress-droplet/","title":"Migrate a DigitalOcean Wordpress Droplet in Under 5 Minutes","text":"<p>This tutorial will guide you through how to backup, rebuild a DigitalOcean droplet, setup SSL encryption and restore a WordPress site in under 5 minutes...</p>"},{"location":"migrate-digitalocean-wordpress-droplet/#prerequisites","title":"Prerequisites","text":"<ul> <li>A DigitalOcean account (obviously) (Run WordPress free for 2 months with this link.</li> </ul>"},{"location":"migrate-digitalocean-wordpress-droplet/#assumptions","title":"Assumptions","text":"<ul> <li>An existing droplet running a OneClick wordpress instance.</li> <li>You\u2019ve used Let\u2019s Encrypt to enable SSL.</li> </ul>"},{"location":"migrate-digitalocean-wordpress-droplet/#lets-get-to-it","title":"Let's Get To It","text":"<ul> <li>Login to your wp-admin and install the \u201cUpdraftPlus \u2013 Backup/Restore\u201d plugin.</li> <li>In admin section, go to Settings &gt; UpdraftPlus Backups</li> <li>Click Backup now then flick to the Existing Backups tab and download the 5 files it provides (Database, Plugins, Themes, Uploads, Others).</li> <li>Spin up a new OneClick droplet.</li> <li>SSH into the new box to activate WordPress.</li> <li>In the DigitalOcean Control Panel, go to Networking and point your Networking to the new droplet.</li> <li>Back in the new droplet SSH session, add a new non-root user with sudo permissions:</li> </ul> <pre><code>adduser USERNAME\n# Enter Password\nusermod -aG sudo USERNAME\nsu - USERNAME\nmkdir ~/.ssh/\nchmod 700 ~/.ssh\nnano ~/.ssh/authorized_keys\n# Paste in your public SSH Key\nchmod 600 ~/.ssh/authorized_keys\nexit\n</code></pre>"},{"location":"migrate-digitalocean-wordpress-droplet/#set-up-lets-encrypt-ssl","title":"Set Up Let's Encrypt SSL","text":"<p>Still in the new droplet SSH session, run the following commands:</p> <pre><code>sudo add-apt-repository ppa:certbot/certbot\nsudo apt-get update\nsudo apt-get install python-certbot-apache\nsudo certbot --apache -d YOURDOMAIN.com -d www.YOURDOMAIN.com\n# Enter an email address\n# First option is \"a\"\n# Secnd option is \"n\"\n# Third option is \"2\"\n# Fourth option is \"2\"\n</code></pre>"},{"location":"migrate-digitalocean-wordpress-droplet/#test-ssl-renewal","title":"Test SSL Renewal","text":"<p><code>sudo certbot renew --dry-run</code></p> <p>Let\u2019s Encrypt certificates run out after 90 days but don\u2019t worry \u2013 certbot takes care of auto-renewal for you. Nothing else to worry about here.</p> <p>You\u2019ve now got valid SSL for free, forever.</p>"},{"location":"migrate-digitalocean-wordpress-droplet/#setup-restore-wordpress","title":"Setup &amp; Restore Wordpress","text":"<ul> <li>Go to your domain and setup wordpress. Notice that you\u2019re automatically using HTTPS.</li> <li>Enter a new temporary admin user / password here. It\u2019ll be overwritten soon.</li> <li>Once setup, login to your wp-admin section (with the admin details you set above).</li> <li>Install the \u201cUpdraftPlus \u2013 Backup/Restore\u201d plugin and activate it.</li> <li>Go to Settings &gt; UpdraftPlus Backups and click Restore.</li> <li>Upload the 5 files you saved to your computer at the beginning of this tutorial.</li> <li>Follow the wizard to restore your site (and users) so now you\u2019re back to your ORIGINAL usernames and passwords. You can discard the temporary admin account details.</li> </ul>"},{"location":"monitor-digitalocean-lamp-stack-dynatrace/","title":"Monitor a DigitalOcean LAMP Stack in Minutes with Dynatrace","text":"<p>A Linux, Apache, MySQL and PHP (LAMP) Stack is possibly one of the most widely used software stacks on the planet. Spin it up with DigitalOcean. Monitor it with automatically with Dynatrace OneAgent...</p>"},{"location":"monitor-digitalocean-lamp-stack-dynatrace/#goal-of-this-tutorial","title":"Goal of This Tutorial","text":"<p>Spin up a fully functional, fully monitored LAMP stack on DigitalOcean using Dynatrace OneAgent to monitor every log file, user, host metric and every transaction.</p> <p>All of these metrics will be automatically correlated and problems will be automatically generated for root causes, not just symptoms.</p>"},{"location":"monitor-digitalocean-lamp-stack-dynatrace/#prerequisites","title":"Prerequisites","text":"<ul> <li>A DigitalOcean account (obviously) (Run LAMP free for 2 months with this link).</li> <li>A Dynatrace account. Sign up for a free 15 day trial here.</li> <li>5 minutes to spare.</li> </ul>"},{"location":"monitor-digitalocean-lamp-stack-dynatrace/#step-1-droplet-creation","title":"Step 1 - Droplet Creation","text":"<ol> <li>Spin up a DigitalOcean LAMP stack using their One-Click installer. The smallest 1GB droplet will suffice for this demo.</li> </ol> <ol> <li> <p>SSH into your droplet, this will activate the installation.</p> </li> <li> <p>Remove the default <code>index.html</code> (<code>sudo rm /var/www/html/index.html</code>) and replace with something better :)</p> </li> </ol> <pre><code>sudo touch /var/www/html/index.html\necho '&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;head&gt;\n&lt;title&gt;DO LAMP Demo by Adam Gardner&lt;/title&gt;\n&lt;/head&gt;\n&lt;body&gt;\n&lt;h1&gt;DigitalOcean LAMP Stack with Dynatrace OneAgent&lt;/h1&gt;\n&lt;p&gt;This is a simple test page&lt;/p&gt;\n&lt;/body&gt;\n&lt;/html&gt;' | sudo tee /var/www/html/index.html\n</code></pre>"},{"location":"monitor-digitalocean-lamp-stack-dynatrace/#step-2-dynatrace-setup","title":"Step 2 - Dynatrace Setup","text":"<ol> <li> <p>Login to your tenant. Go to Settings &gt; Web and mobile monitoring &gt; Application rules. Click Create custom grouping rule.</p> </li> <li> <p>Give your application a name and set the rule to be any URL which contains your server IP.</p> </li> </ol> <p></p> <ol> <li>Go to Start Deploy Dynatrace and navigate to the Linux installer. Run the first and third command (second is optional) on your box. Run the <code>.sh</code> file as <code>root</code> or with <code>sudo</code>.</li> </ol> <p></p> <ol> <li>Restart apache: <code>sudo service apache2 restart</code></li> </ol>"},{"location":"monitor-digitalocean-lamp-stack-dynatrace/#youre-done-a-fully-monitored-lamp-stack","title":"You're Done. A Fully Monitored LAMP Stack","text":"<p>You now have: - A fully monitored LAMP stack. - Automated availability tracking / alerting. - Full visibility of every user action, click and page visit. - Access to all log files. - Code level tracing of Apache and PHP. Including the performance of Apache modules.</p> <p></p> <p></p> <p></p> <p></p> <p></p>"},{"location":"monitor-json-apis-prometheus/","title":"Monitor JSON APIs with Prometheus","text":"<p>This video explains how to gather values (scrape) from a JSON endpoint and get the metrics into Prometheus.</p> <p>If you want to do this securely, I have a second video which walks through the secure setup.</p>"},{"location":"multigitter/","title":"Update all of your repositories in one command","text":"<p>Let's face it, updating multiple repositories is a pain. It slows us down and is really hard work. Well, no more! Multi Gitter is a CLI tool to update as many repositories as you like with a single command.</p>"},{"location":"my-first-helm-plugin/","title":"My First Helm Plugin","text":"<p>I've just finished creating my first Helm plugin and I thought I'd document the process in case it helps others.</p>"},{"location":"my-first-helm-plugin/#preparation","title":"Preparation","text":"<p>This plugin will \"wrap\" a normal helm command, generate an OpenTelemetry trace and use tracepusher to send it to an OpenTelemetry collector.</p> <p>You'll need the following installed to follow along:</p> <ul> <li>helm</li> <li>git</li> <li>A tracepusher binary added to your $PATH</li> </ul>"},{"location":"my-first-helm-plugin/#the-outline","title":"The Outline","text":"<p>Helm commands usually look like this:</p> <pre><code>helm &lt;command&gt;\n</code></pre> <p>For example:</p> <pre><code>helm version\n</code></pre>"},{"location":"my-first-helm-plugin/#whats-in-a-name","title":"What's in a name?","text":"<p>Helm plugins have a name and that name becomes part of the command. In other words, you invoke your plugin during a regular helm command.</p> <p>If your plugin was called <code>foo</code>, you would invoke it like this:</p> <pre><code>helm foo version\n</code></pre>"},{"location":"my-first-helm-plugin/#what-language","title":"What language?","text":"<p>Helm doesn't care what language your plugin is written in. Powershell, shell, Python, Go etc. Helm can run it all.</p> <p>Of course, you need to know that the end user (the person installing the plugin) has that language available on their machine.</p> <p>If you write your plugin in Python and the user doesn't have Python? Your plugin will fail.</p> <p>So what language does every Mac, Linux, Unix have? Plain old bash. That's what we'll write our plugin in today.</p>"},{"location":"my-first-helm-plugin/#what-should-it-do","title":"What should it do?","text":"<p>What do you want the plugin to do?</p> <p>In my case, I want the plugin to:</p> <ol> <li>Execute the helm command that the user intended</li> <li>Time the amount of time the command took</li> <li>Call another process that I know is installed on the user machine</li> </ol> <p>I know that this other process is installed because I've listed it in the prerequisites for the plugin.</p>"},{"location":"my-first-helm-plugin/#the-payload","title":"The Payload","text":"<p>Create a folder called <code>trace</code>. This folder is important. It must match the name of the plugin.</p> <p>Here's the plugin payload. Save it as a <code>.sh</code> file with any name you like. I've called mine <code>script.sh</code></p> <pre><code>#!/bin/bash\n\ncommandToRun=\"helm $@\"\n\n# If collector env var not set\n# default to localhost:4318\nif [ -z $HP_OTELCOL_ENDPOINT ]\nthen\n  HP_OTELCOL_ENDPOINT=http://localhost:4318\nfi\n\n# Run original helm command\n# w/o 'trace' keyword\n$commandToRun\n\nduration=$SECONDS\n\n# Use tracepusher to generate an OpenTelemetry trace\n# and push it to an OTEL collector \ntracepusher -e $HP_OTELCOL_ENDPOINT -sen helm -spn \"$commandToRun\" -dur $duration\n</code></pre>"},{"location":"my-first-helm-plugin/#explain-it","title":"Explain it!","text":"<p>The first line denotes that this is a bash script.</p> <p>Then the <code>commandToRun</code> is built by using <code>$@</code> which, in bash, means \"all arguments\" and prepending <code>helm</code>.</p> <p>So, the input might be <code>helm trace install X Y</code>. In which case, <code>$@</code> would be <code>install X Y</code>. So now re-add the <code>helm</code> command to the beginning. <code>commandToRun</code> is now <code>helm install X Y</code>.</p> <p>Next the script checks whether an environment variable <code>HP_OTELCOL_ENDPOINT</code> is set. If not, it defaults to <code>http://localhost:4318</code>.</p> <p>Run the command original <code>helm</code> command as the user intended (in our example this is <code>helm install X Y</code>).</p> <p>When helm has finished, calculate the amount of seconds this script has been running. The SECONDS variable is a built-in bash variable which provides the number of seconds since the shell was invoked. The script uses this to denote the nubmer of seconds the script has been running.</p> <p>Finally, the tracepusher binary is executed, using the environment variable previously set. Tracepusher will generate an OpenTelemetry span with a service name of <code>helm</code>, a span name identical to the helm command and a duration which matches how long it took to execute the actual helm command.</p>"},{"location":"my-first-helm-plugin/#create-helm-plugin","title":"Create Helm Plugin","text":"<p>The above gets a running shell script. How do we make that a helm plugin?</p> <p>Create a new file in the <code>trace</code> directory alongside <code>script.sh</code> called <code>plugin.yaml</code> (again, name is important):</p> <pre><code>name: \"trace\"\nversion: \"0.1.0\"\nusage: \"helm trace install foo foo/bar\"\ndescription: \"Generate OpenTelemetry traces for helm commands\"\nignoreFlags: false\ncommand: \"$HELM_PLUGIN_DIR/script.sh\"\nplatformCommand:\n  - os: \"linux\"\n    command: \"$HELM_PLUGIN_DIR/script.sh\"\nhooks:\n  install: \"chmod +x $HELM_PLUGIN_DIR/script.sh\"\n</code></pre>"},{"location":"my-first-helm-plugin/#explain-it_1","title":"Explain it!","text":"<p>The <code>name</code> command should be self-explanatory and should match the directory name (and intended plugin name) in this case: <code>trace</code>.</p> <p>The plugin has a <code>version</code>. SemVer must be used.</p> <p>Provide some brief usage instructions in the <code>usage</code> field.</p> <p><code>description</code> is a field for you to give a plugin description.</p> <p><code>ignoreFlags</code> takes <code>true</code> or <code>false</code>. If <code>ignoreFlags</code> is set to <code>true</code>, helm arguments are silently discarded (and thus unavailable to your plugin). For example: <code>helm version --foo</code> would become <code>helm version</code> with the <code>--foo</code> flag being thrown away. Since we want the exact command the user executed, we set this flag to false and thus retain all arguments.</p> <p>The <code>command</code> parameter tells helm what to do when someone runs the plugin. In this case, helm will execute our shell script. <code>$HELM_PLUGIN_DIR</code> is an in-built variable that is set by helm itself and always points to the correct plugin directory. Handy!</p> <p><code>platformCommand</code> allows the plugin author specify different commands based on operating system. Want a shell script for Linux and a Powershell for Windows? That's possible.</p> <p>Finally, due to Linux's permission requirements, we need the shell script to be executable (have the <code>x</code> bit set). So during plugin installation, after <code>script.sh</code> is downloaded, <code>hooks.install</code> is used to run a <code>chmod +x</code> command to ensure the script is executable.</p> <p>Save <code>plugin.yaml</code></p>"},{"location":"my-first-helm-plugin/#install-and-test-locally","title":"Install and test locally","text":"<p>If you are in the <code>trace</code> folder, you should now have two files:</p> <ul> <li><code>script.sh</code></li> <li><code>plugin.yaml</code></li> </ul> <p>Install your plugin by running:</p> <pre><code>helm plugin install .\n</code></pre> <p>If you find a helm plugin online you like, the syntax changes <code>.</code> to the URL:</p> <pre><code>helm plugin install https://github.com/agardnerit/helm-trace\n</code></pre> <p>Before you ask, yes the repo must be structured like that: <code>plugin.yaml</code> at the root.</p> <p>Back to the locally developed plugin...</p> <p>You should see:</p> <pre><code>Installed plugin: trace\n</code></pre>"},{"location":"my-first-helm-plugin/#try-it-out","title":"Try it out","text":"<p>Run a helm command:</p> <pre><code>helm trace version\n</code></pre> <p>If you have an OpenTelemetry collector running on <code>http://localhost:4318</code> you should see:</p> <pre><code>% helm trace version\nversion.BuildInfo{Version:\"v3.11.2\", GitCommit:\"912ebc1cd10d38d340f048efaf0abda047c3468e\", GitTreeState:\"clean\", GoVersion:\"go1.20.2\"}\n&lt;Response [200]&gt;\n</code></pre> <p>If your collector is running elsewhere, set the <code>HP_OTELCOL_ENDPOINT</code> then run the command above:</p> <pre><code>export HP_OTELCOL_ENDPOINT=http://my.collector.somewhere.com:4318\nhelm trace version\n</code></pre>"},{"location":"my-first-helm-plugin/#summary","title":"Summary","text":"<p>So there you have it, your first Helm plugin! There aren't many limitations to what helm and the plugin system can achieve, so go forth and integrate.</p> <p>Also, please make Helm observable with OpenTelemetry by installing helm trace.</p>"},{"location":"my-first-helm-plugin/#additional-reading-and-links","title":"Additional Reading and Links","text":"<ul> <li>The real helm trace plugin (and code)</li> <li>Helm plugin developers guide</li> <li>tracepusher</li> </ul>"},{"location":"nats-summary/","title":"Hands on with NATS in under 4 minutes: Video Summary","text":"<p>This is a text-based version of this video so if you prefer reading to watching, read on!</p> <p>Want to jump straight to the NATS hands-on? </p>"},{"location":"nats-summary/#nats-a-cncf-incubating-project","title":"NATS: A CNCF Incubating Project","text":"<p>NATS is a cloud native Computing Foundation (or CNCF incubating project). An incubating project is the middle of three tiers (sandbox, incubating and graduated).</p> <p>Incubating projects have a an established security protocol, commits or contributions from a variety of people and not just one person, project or company. All of that means that NATS (or incubating projects in general) should have a fairly stable life and should not just dissapear overnight - meaning they should be safe to adopt.</p>"},{"location":"nats-summary/#what-is-nats","title":"What is NATS?","text":"<p>NATS describes itself as message oriented middleware. Two computer systems (A &amp; B) need to send data to one another.</p> <p>Nat sits in the middle and enables that communication it does so based on subjects.</p> <p>A \"publisher\" will publish a message to a particular subject and then one or more systems can listen to that subject and receive all data destined for them.</p>"},{"location":"nats-summary/#nats-hands-on","title":"NATS Hands On","text":"<p>During the NATS hands-on exercise, NATS will be installed on a Kubernetes cluster.</p> <p>You will then publish a message to a <code>test</code> subject and another consumer will listen for messages on that <code>test</code> subject. You will then create two new subjects: <code>names.dog</code> and <code>names.cat</code> and publish a message on <code>names.dog</code> to show that none of the other consumers receive that message there's only one person receiving that message.</p>"},{"location":"nats-summary/#new-project-each-week","title":"New Project Each Week","text":"<p>Each week in 2023 I will pick a new open source project to review. The entire list is here.</p> <p>The series is designed to be interactive - if you have a project you'd like me to review - please do get in contact and suggest a project!</p> <p>I hope you enjoy the NATS project!</p>"},{"location":"one-more-javascript/","title":"Just One More JavaScript...","text":"<p>The latest feature, a new shiny library, it's all just one new JavaScript include away. What could possibly go wrong? For starters, how about having your credit card details stolen?</p>"},{"location":"one-more-javascript/#intro","title":"Intro","text":"<p>I recently listened to an episode of the Darknet Diaries podcast where they detailed a real-world credit card skimming ring. The episode discusses how a compromised third party JavaScript tag led to millions of leaked card numbers. It got me wondering just how realistic, easy or difficult, that would be.</p> <p>I am not a security professional or hacker - ethical or otherwise. My security and pen-testing background is precisely zero. Unfortunately, it turns out that's how much experience you need to make this a reality... Zero... It is that easy.</p> <p>Before you think I'm just being alarmist, check out this post from Hackernoon which describes a far more elaborate setup using npm modules.</p> <p>The message here should be loud and clear. Don't be scared, but do conduct regular reviews of every dependency and every piece of code, whether or not you wrote it. Ask whether you really understand what each third party library is doing, whether you need it and quantify the value that it provides, weighed against the potential risk. Get business stakeholders onboard too as they need to understand and care about this because ultimately, should something go wrong, it won't be a technical problem. It'll be a board-level business problem.</p> <p>Don't believe me? Here's how easy it is.</p>"},{"location":"one-more-javascript/#step-1-create-basic-infrastructure","title":"Step 1: Create Basic Infrastructure","text":"<p>VM one will act as our main website. VM two will act as our third party website (or CDN) from which we'll load the JavaScript file.</p> <p>Install <code>apache</code> on both VMs. On VM one, create an index page which contains a form (imagine this is your payment page).</p> <pre><code>&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;head&gt;&lt;/head&gt;\n&lt;body&gt;\n&lt;h1&gt;Test Page&lt;/h1&gt;\n&lt;p&gt;Just a basic test page&lt;/p&gt;\n&lt;form id=\"myForm\"&gt;\n&lt;input type=\"text\" /&gt;&lt;br /&gt;\n&lt;input type=\"password\" /&gt;&lt;br /&gt;\n&lt;input type=\"submit\"&gt;\n&lt;/form&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre> <p>On VM 2, create a new JavaScript file in the apache root directory. I've called mine <code>test.js</code>. Imagine this JavaScript is your shiny new feature or library. For now, it's perfectly valid and clean. Nothing nefarious going on here. The code simply waits for the page to load, then prints a message to the console.</p> <pre><code>document.addEventListener(\"DOMContentLoaded\", function(){\n  console.log('Hello, World!');\n});\n</code></pre>"},{"location":"one-more-javascript/#step-2-include-javascript-on-page","title":"Step 2: Include JavaScript on page","text":"<p>Include the JavaScript into the HTML page on VM one. To do this, modify the <code>head</code> section of the <code>index.html</code> page on VM one and include the new script tag:</p> <pre><code>&lt;head&gt;\n  &lt;script src=\"http://IP-ADDRESS-OF-VM2/test.js\"&gt;&lt;/script&gt;\n&lt;/head&gt;\n</code></pre> <p>Save the <code>index.html</code> file, open the DevTools console and refresh the page (hint: choose the option to preserve log lines). You should see the <code>Hello, World!</code> message printed in the console.</p>"},{"location":"one-more-javascript/#step-3-the-hack","title":"Step 3: The Hack","text":"<p>Imagine it's some time later and you are running in production quite happily, everything is fine, payments are being taken successfully and securely and everything is right with the world.</p> <p>However, unknown to you, the CDN or website hosting that third party JavaScript code has just been compromised. A bad actor has gained access to their servers and has modified the Javascript code.</p> <p>Your site still runs just fine, it does what you told it to do - include the JavaScript file from the third party website.</p> <p>Unknown to you though, the JavaScript on the third party site (eg. VM2) now looks like this:</p> <pre><code>document.addEventListener(\"DOMContentLoaded\", function(){\n  console.log('Hello, World!');\n\n  document.getElementById(\"myForm\").addEventListener(\"submit\", function(evt){\n    evt.preventDefault();\n  });\n</code></pre> <p>Refresh the webpage on VM1 and try to submit your payment form... That third party code has just broken your payment system but this is all you see from your browser.</p> <p></p> <p>That <code>evt.preventDefault();</code> line is all it took to break your entire payment system.</p>"},{"location":"one-more-javascript/#step-4-the-hack-part-2","title":"Step 4: The Hack Part 2","text":"<p>The hackers grow bolder and decide to actually scrape your username, password, credit card details and any other details on your form. They modify the JS code to read:</p> <pre><code>document.addEventListener(\"DOMContentLoaded\", function(){\n  console.log('Hello, World!');\n\n  document.getElementById(\"myForm\").addEventListener(\"submit\", function(evt){\n    evt.preventDefault();\n    form = document.getElementById(\"myForm\");\n    console.log(form);\n  });\n</code></pre> <p>Again, refresh the page and try to submit your form. What do you see now in the console? Your username and password in plain text for all to see.</p> <p></p> <p>Scary? Indeed! Hackers have potentially exposed payment details on your website, all without touching your actual website!</p>"},{"location":"one-more-javascript/#aside","title":"Aside","text":"<p>Yes, I know that in the format suggested above, there isn't much danger as the hacker would have to be looking over your shoulder to see the console. However, it only takes a few additional lines of code to get those details off the site on to a third party system. Perhaps an additional 3-4 lines of code...</p>"},{"location":"one-more-javascript/#summary","title":"Summary","text":"<p>As I said at the beginning of this post, I am not trying to scare anyone or vilify JavaScript. I am trying to point out that you are ultimately responsible for every line of code that runs on your website - whether you wrote them or not. The third party libraries you use demand the same level of security and scrutiny as the code you write.</p> <p>If in doubt, leave it out. If you don't need that include, don't use it!</p>"},{"location":"openfeature-intro/","title":"OpenFeature: A Primer","text":"<p>What is OpenFeature and why might you need it?</p>"},{"location":"openfeature-intro/#what-is-openfeature","title":"What Is OpenFeature?","text":"<p>OpenFeature is an open, vendor-neutral specification for feature flagging (FF). It provides a standardised abstraction layer between your code and the underlying feature flag vendor.</p>"},{"location":"openfeature-intro/#what-problems-does-openfeature-solve","title":"What Problems Does OpenFeature Solve?","text":""},{"location":"openfeature-intro/#openfeature-benefits-for-feature-flag-end-users","title":"OpenFeature Benefits for Feature Flag End Users","text":"<p>Imagine you wish to integrate with a feature flagging solution from <code>vendorA</code>. Your code needs to communicate with <code>vendorA</code> using their APIs and their specific code. In other words, your code is tightly coupled to that one vendor. If you want to move to a new vendor - it's a lot of work.</p> <p>How about different parts of the organisation using different vendors? Scale the above scenario up and you'll understand the issue.</p> <p>What if you need to use multiple feature flag vendors at the same time? OpenFeature provides \"federated\" options so you can ask for flag <code>foo</code> and the provider that has it, returns it.</p> <p>This is not just an IT benefit either, the ability to use the \"best of breed\" feature flag tool (and be able to quickly switch) offers business agility and can expedite time to market.</p>"},{"location":"openfeature-intro/#openfeature-benefits-for-feature-flag-vendors","title":"OpenFeature Benefits for Feature Flag Vendors","text":"<p>As a feature flag vendor, you want to encourage ways for potential users to easily trial and adopt your solution. Most likely, you also want to \"play nice\" with other vendors - no-one likes vendor lock-in.</p> <p>OpenFeature provides that opportunity so it is in all feature flag vendors interests to widely adopt OpenFeature.</p>"},{"location":"openfeature-intro/#how-does-openfeature-work","title":"How Does OpenFeature Work?","text":"<p>Rather than directly connecting your code to the feature flag vendor, your code instead interacts in a standard way with the OpenFeature API. Vendor <code>providers</code> translate that code into the vendor specific API calls.</p> <p>Switching vendors is a one line code change: <code>set_provider(VendorAProvider())</code> becomes <code>set_provider(VendorXProvider())</code>.</p> <pre><code># Set which provider to use\nopen_feature_api.set_provider(VendorAProvider())\n\n# Get a String flag called `foo`\nopen_feature_client.get_string_details(key=\"foo\", default_value=\"missing_flag\")\n</code></pre> <p> </p>"},{"location":"openfeature-intro/#what-exactly-is-a-provider","title":"What Exactly is a Provider?","text":"<p>A provider is the \"translation\" code between the OpenFeature API (see above) and the vendor specific calls.</p> <p>The provider is responsible for calling the vendor and returning the flag in an OpenFeature compliant way.</p>"},{"location":"openfeature-intro/#who-writes-providers","title":"Who Writes Providers?","text":"<p>Vendors usually write the providers but you can also create your own.</p> <p>Providers can be written in any language. For example, here is the Split.io JavaScript Provider</p>"},{"location":"openfeature-intro/#openfeature-hooks","title":"OpenFeature Hooks","text":"<p>The OpenFeature spec offers the concept of hooks which, as it sounds, is a way to hook into feature flag execution during runtime.</p> <p>Hooks can fire on one or more of these lifecycle stages:</p> <ul> <li>Before</li> <li>After</li> <li>Error</li> <li>Finally</li> </ul> <p>Integrate feature flag information into OpenTelemetry traces? Sure.</p> <p>Need to notify that a flag has been toggled? Use the <code>After</code> hook.</p> <p>Want to potentially prevent a hook unless it has been approved? The <code>Before</code> hook might be what you're looking for.</p> <p>Need to fix something if a FF toggle errors? Trigger the action using the <code>Error</code> hook.</p> <p>Want to do something every time a flag is toggled, regardless of the status? <code>Finally</code> is probably what you are looking for.</p>"},{"location":"openfeature-intro/#what-else-does-openfeature-offer","title":"What Else Does OpenFeature Offer?","text":"<p>Probably lots more than I've covered here. I'm still learning too, but I know conditional and fractional evaluations are possible. For example, you receive <code>valueA</code> if the email ends in <code>@example.com</code> otherwise <code>valueB</code>.</p> <p>I believe rules can be created like <code>getColour</code> where <code>green</code> is returned <code>X%</code> of the time, <code>blue</code> is given <code>Y%</code> of the time and so on.</p> <p>OpenFeature has a concept called <code>evaluation context</code> which means you can pass in dynamic data at runtime that can be used by the Provider to evaluate a flag. Imagine users on a certain IP range (US-based users) receiving one value and European-based users get a different flag value.</p>"},{"location":"openfeature-intro/#who-is-involved-interested-in-openfeature","title":"Who Is Involved / Interested in OpenFeature?","text":"<p>Here is a list of interested parties. If you / your company is interested, you can add your name to the list.</p>"},{"location":"openfeature-intro/#does-vendorx-support-openfeature","title":"Does {VendorX} Support OpenFeature?","text":"<p>The best way to find out is to ask them! Most feature vendors are already aware, involved or have built integrations.</p> <p>If your vendor doesn't yet support OpenFeature yet, it's a great opportunity to start that conversation.</p>"},{"location":"openfeature-intro/#where-can-i-find-out-more","title":"Where Can I Find Out More?","text":"<p>The OpenFeature website, GitHub and #openfeature on CNCF Slack are all good starting points.</p>"},{"location":"opentelemetry-collector-everything/","title":"Everything you need to know (to get started) with the OpenTelemetry Collector","text":"<p>In this video I deep dive into the OpenTelemetry collector components and explain everything you need to know to get started with the OpenTelemetry collector.</p>"},{"location":"opentelemetry-curl/","title":"OpenTelemetry Logs using only cURL?","text":"<p>Another video where I show how to get started ridiculously easy with OpenTelemetry logging, using a tool we ALL have access to: cURL.</p>"},{"location":"opentelemetry-filelog-receiver/","title":"OpenTelemetry filelog receiver: Hands on","text":"<p>In this video I get hands-on with the OpenTelemetry filelog receiver and send the data into OpenObserve.</p>"},{"location":"opentelemetry-filestats-receiver/","title":"Exploring the OpenTelemetry filestats receiver","text":"<p>In this 5 minute video, I explain the OpenTelemetry filestats receiver.</p>"},{"location":"opentelemetry-job-tracing/","title":"Tracing Kubernetes Jobs with OpenTelemetry and Tracepusher","text":"<p>Did you know there's a Kubernetes Operator which will automatically generate OpenTelemetry traces for Kubernetes Jobs and CronJobs?</p> <p>Get hands on with the tracepusher Job operator with my dev.to post.</p>"},{"location":"opentelemetry-semantic-conventions-best-practices/","title":"OpenTelemetry Semantic Conventions Best Practices","text":"<p>In this video, I respond to a Reddit thread where a user was asking for feedback on leveraging the existing OpenTelemetry semantic conventions vs. a \"roll your own\" approach. I present some things you consider before finalising your choice.</p>"},{"location":"opentelemetry-signaltometric/","title":"Convert anything to a metric with the OpenTelemetry Signal to Metric (signaltometric) connector","text":"<p>In this video I show how to convert any input signal type (metrics, logs, distributed traces or profiles) into metrics using the signaltometric connector in the OpenTelemetry collector.</p>"},{"location":"opentelemetry-spans-jaeger/","title":"Sending Python Spans to Jaeger via a Collector","text":"<p>In the third episode of the series, the spans are sent from Python to Jaeger via an OpenTelemetry collector.</p> <p>This is the text version for those who prefer to read instead. If you got here and want to watch, just click the image below.</p>"},{"location":"opentelemetry-spans-jaeger/#building-the-system-a-place-to-send-spans","title":"Building the System: A Place to Send Spans","text":"<p>For this demo the spans will be send to the CNCF project Jaeger, but it could just as easily be any other open source or commercial backend.</p> <p>Use Docker (or Podman) to start the Jaeger allInOne demo image and expose two ports:</p> <ul> <li>18868 (for the user interface)</li> <li>4317 (so the built in Jaeger collector can receive ports via gRPC on the standard gRPC port)</li> </ul> <pre><code>docker run --rm --name jaeger \\\n  -p 16686:16686 \\\n  -p 4317:4317 \\\n  jaegertracing/all-in-one:1.57\n</code></pre> <p>Open a browser and validate that Jaeger is visible on <code>http://localhost:16686</code></p> <p>After a few seconds, refresh the page and you'll see one service listed: <code>jaeger-all-in-one</code>. That is Jaeger tracing itself, so you can ignore that service (but its presence proves everything is working properly).</p>"},{"location":"opentelemetry-spans-jaeger/#building-the-system-add-an-opentelemetry-collector","title":"Building the System: Add an OpenTelemetry Collector","text":"<p>Some backends allow you to send spans directly to them. The Jaeger allInOne image even comes bundled with it's own collector (that's why it's listening on port <code>4317</code>).</p> <p>So you may choose to send spans directly to the backend. I do not recommend you do this. I do recommend you place a collector (or several) in front of your backend.</p> <p>In reality you will have tens, hundreds or thousands of entities producing spans (processes, pods etc.) so placing collectors between those entities and the backend simplifies configuration and provides lots of benefits (I will explain in depth in a future video / post).</p> <p>Save a file called config.yaml with the following content:</p> <pre><code>receivers:\n  otlp:\n    protocols:\n      grpc:\n        endpoint: 0.0.0.0:6004\n      http:\n        endpoint: 0.0.0.0:6006\n\nprocessors:\n\nexporters:\n  logging:\n    verbosity: detailed\n\n  otlp/localcollector:\n    endpoint: http://localhost:4317\n    tls:\n      insecure: true\n\nservice:\n  extensions: []\n  pipelines:\n    traces:\n      receivers: [otlp]\n      processors: []\n      exporters: [otlp/localcollector, logging]\n</code></pre> <p>This configuration takes spans into the collector on either port 6004 (when they're sent via gRPC) or port 6006 (when sent via HTTP), does nothing to the spans (nothing in the <code>processors</code> block) and sends them back out again to the <code>http://localhost:4317</code> endpoint (you will recognise that as the Jaeger gRPC endpoint).</p> <p>I've also configured some verbose (detailed) logging output. If anything goes wrong during development, I need to quickly see what's wrong. You would probably remove this <code>logging</code> for production.</p>"},{"location":"opentelemetry-spans-jaeger/#why-ports-6004-and-6006","title":"Why Ports <code>6004</code> and <code>6006</code>?","text":"<p>I've chosen to use ports <code>6004</code> and <code>6006</code> for two reasons:</p> <ol> <li>Jaeger is already running on port <code>4317</code> so I need to choose a different one (hence I randomly picked <code>6004</code>)</li> <li>The demo app only sends traces via gRPC but to test the configuration (which you'll see next), I also decided to listen on <code>HTTP</code> and just chose <code>6006</code>.</li> </ol>"},{"location":"opentelemetry-spans-jaeger/#download-and-start-collector","title":"Download and Start Collector","text":"<p>Download the collector binary for your platform from the latest releases.</p> <p>Start the collector, passing your config.yaml file as a parameter:</p> <pre><code>./otelcol --config=config.yaml\n</code></pre>"},{"location":"opentelemetry-spans-jaeger/#the-architecture-so-far","title":"The Architecture So Far","text":""},{"location":"opentelemetry-spans-jaeger/#test-it","title":"Test It","text":"<p>It is time to ensure the system works and the \"wiring\" is correct. To do so, we will use a tool called tracepusher. It is a utility which can emit arbitrary spans, it's useful for many \"additional\" usecases (like tracing CICD pipelines or shell scripts) but we will use it just to send a test span to the collector and see it in Jaeger.</p> <p>tracepusher only emits spans via HTTP, hence why we needed the collector to listen on port <code>6006</code>.</p> <p>Download the tracepusher binary and run it:</p> <pre><code>./tracepusher \\\n  --endpoint http://localhost:6006 --insecure true \\\n  --span-name spanOne \\\n  --service-name serviceA \\\n  --duration 2000 \\\n  --duration-type ms\n</code></pre> <p>You should receive: <code>Response &lt;200&gt;</code> from the collector. This proves the span got to the collector.</p>"},{"location":"opentelemetry-spans-jaeger/#check-jaeger","title":"Check Jaeger","text":"<p>Refresh the Jaeger UI and two services are available. The second one matching the <code>--service-name</code> from tracepusher (ie. <code>serviceA</code>).</p> <p>Select that service and click <code>Find Traces</code> and you'll see the single span.</p> <p>The wiring works!</p>"},{"location":"opentelemetry-spans-jaeger/#add-dependencies","title":"Add Dependencies","text":"<p>Some new dependencies are required, so overwrite <code>requirements.txt</code> with this file:</p> <pre><code>opentelemetry-api == 1.24.0\nopentelemetry-sdk == 1.24.0\nopentelemetry-exporter-otlp-proto-grpc == 1.24.0\nopentelemetry-semantic-conventions == 0.45b0\n</code></pre> <p>Use pip to install these new dependencies:</p> <pre><code>pip install -r requirements.txt\n</code></pre>"},{"location":"opentelemetry-spans-jaeger/#the-app-export-spans-to-collector","title":"The App: Export spans to Collector","text":"<p>If you have been following part one and part two of this tutorial, you will already have a file called <code>app.py</code>.</p> <p>Overwrite the contents of that file with this app.py.</p> <p>99% of the content is identical but let's run the app and explore the changes.</p> <pre><code>python app.py\n</code></pre>"},{"location":"opentelemetry-spans-jaeger/#the-architecture-so-far_1","title":"The Architecture So Far","text":"<p>The architecture now looks like this:</p> <p></p>"},{"location":"opentelemetry-spans-jaeger/#differences-in-apppy","title":"Differences in app.py","text":"<p>Line <code>15</code> &amp; <code>16</code> define two new variables: <code>COLLECTOR_ENDPOINT</code> and <code>COLLECTOR_GRPC_PORT</code>.</p> <p>Notice that these both point to the standalone collector.</p> <p>Line <code>19</code> replaces the old <code>ConsoleSpanExporter</code> instead with an <code>OTLPSpanExporter</code> which emits spans to <code>http://&lt;COLLECTOR_ENDPOINT&gt;:&lt;COLLECTOR_GRPC_PORT&gt;</code> ie. <code>http://localhost:6004</code>.</p> <p></p>"},{"location":"opentelemetry-spans-jaeger/#summary","title":"Summary","text":"<p>In this post, and the accompanying video, we have successfully sent OpenTelemetry traces from a Python application, via an OpenTelemetry collector, into a span storage backend (Jaeger).</p> <p>In the next installment of this series, we will re-write the application to emit a trace which contains multiple spans. So, subscribe to my channel on Youtube and I'll see you then!</p>"},{"location":"opentelemetry-spans-vs-traces/","title":"OpenTelemetry Spans vs. Traces","text":"<p>What's the difference between an OpenTelemetry span and an OpenTelemetry trace?</p> <p>In this video I explain exactly what a span is, and what a trace is. You'll understand each and therefore understand what you need, when and why.</p>"},{"location":"opentelemetry-weaver-telemetry-quality/","title":"Judging Telemetry Quality using Weaver","text":"<p>\"Is my telemetry good?\" In this post I'll show you how to answer that question using the Weaver tool from the OpenTelemetry project.</p> <p>Watch</p> <p>If you'd prefer to watch rather than read, I have the following content as a YouTube video:</p> <p></p>"},{"location":"opentelemetry-weaver-telemetry-quality/#overview","title":"Overview","text":"<p>I regularly get asked a variation of: \"is my telemetry any good?\" and \"what can we do to improve our telemetry data to better assist us during incidents?\"</p> <p>Assuming the asked actually has the correct telemetry to assist them, then what these questions usually boil down to, is the fact that their telemetry does not carry the correct metadata.</p> <p>When telemetry signals (metrics, logs, events, traces, profiles etc.) are used during an incident, you need to quickly identify the issue, who is affected, who those entities belong to and thus you can quickly notify and begin the remediation process.</p>"},{"location":"opentelemetry-weaver-telemetry-quality/#so-what-is-the-right-metadata","title":"So What is \"the right metadata\"?","text":"<p>Easy to say, harder to achieve, right? Defining standards across an organisation is hard - doing so across multiple organisations is even harder.</p> <p>Well, a great place to start are semantic conventions. These are agreed-upon standards that define what metadata is included on the signals. Semantic conventions usually use the words <code>MUST</code>, <code>SHOULD</code> and <code>COULD</code> to indicate how important each piece of metadata is and thus how important</p> <p>Let's make this more concrete: The OpenTelemetry semantic conventions defines attributes for all kinds of things - from how to provide a host name (<code>Recommended</code> with <code>host.name</code>) to how to indicate the type of database(eg. <code>redis</code>) (<code>Required</code> with <code>db.system.name</code>).</p> <p>The OpenTelemetry semantic conventions even tell us how to report certain metrics. For example, if you and I both wanted to report CPU usage, we MUST use http.server.request.duration and report a histogram.</p> <p>At first glance this seems like uneccessary control and overhead, but this kind of standardisation pays dividends at scale because the backend system (wherever we send telemetry data) can now understand our signals as like-for-like and this know that it can treat things equivalently.</p> <p>This standardisation is also the key to answering our question: How good is our telemetry?</p>"},{"location":"opentelemetry-weaver-telemetry-quality/#roll-your-own-semantic-convention","title":"Roll Your Own Semantic Convention?","text":"<p>Your first instinct might be to develop your own in-house convention. However I would caution against that, for a few reasons:</p> <ol> <li>It is not usually required - a lot of thought (and expertise from across the industry) has gone into the OpenTelemetry SemConv.</li> <li>Third party suppliers, SaaS vendors you rely upon, and others won't be following your in-house semantic conventions. Meaning you're creating downstrea work to \"map\" one metric to another.</li> <li>Following the standard can be a sales tool. If you supply metrics for another entity to consume, you're creating work for them as they need to map your custom \"stuff\" to their \"stuff\". If I was evaluating vendors and was given the choice between two equal products - one which followed a widely adopted standard and one which \"rolled their own\" - I'd pick the standard-following one every time.</li> </ol> <p>Defining an in-house standard can be appropriate - if you're doing something so incredibly specialised that no other company is doing it. Let's be honest though, you're probably not (and even if you were, you should submit it to the OpenTelemetry spec, make a lot of noise about it and use it as a marketing / PR activity that you're a thought leader).</p> <p>All of that said, the Weaver tool we'll use below can work with your custom telemetry spec, if you still choose to go down this path.</p>"},{"location":"opentelemetry-weaver-telemetry-quality/#semantic-conventions-aid-quality-judgments","title":"Semantic Conventions Aid Quality Judgments","text":"<p>Now that there's a way of defining things against a standard (OpenTelemetry, an in-house one or both) the question can now legitimately be asked: Is my telemetry any good?</p> <p>Tip</p> <p>All you need to do is find out \"how far away\" from fully meeting the specifications you are.</p> <ul> <li>If the telemetry is a 100% match, then yes, your telemetry is perfect</li> <li>If you are only barely matching the agreed-upon rules, your telemetry has room for improvement</li> </ul>"},{"location":"opentelemetry-weaver-telemetry-quality/#weaver","title":"Weaver","text":"<p>As you've probably guessed by now, Weaver, from the OpenTelemetry project, is tool which allows you to evaluate your live realtime telemetry against the semantic conventions you choose.</p> <p>Weaver acts as an endpoint. Telemetry is sent to Weaver and Weaver produces an output report.</p>"},{"location":"opentelemetry-weaver-telemetry-quality/#step-1-start-weaver","title":"Step 1: Start Weaver","text":"<p>Start Weaver, informing it to wait for <code>60</code> seconds without receiving telemetry before shutting down. The command below also sets the output format to <code>JSON</code>, creates a new folder called <code>weaver-output</code> and saves a new file in there called <code>live_check.json</code> when Weaver closes.</p> <ul> <li>Port <code>4317</code> is used to receive telemetry data (on the gRPC port).</li> <li>Port <code>4320</code> is optional and should be included if you wish to stop Weaver via the <code>curl</code> command (see later).</li> </ul> <p>Weaver Binary</p> <p>There is also a standalone Weaver binary available on GitHub</p> <pre><code>docker run --rm \\\n  -p 4317:4317 \\\n  -p 4320:4320 \\\n  -u $(id -u ${USER}):$(id -g ${USER}) \\\n  --env HOME=/tmp/weaver \\\n  otel/weaver:v0.15.2 registry live-check \\\n  --inactivity-timeout=60 \\\n  --output=weaver-output \\\n  --format=json\n</code></pre> <p>or (for standalone binary):</p> <pre><code>./weaver registry live-check \\\n  --inactivity-timeout=60 \\\n  --output=weaver-output \\\n  --format=json\n</code></pre> <p>Info</p> <p>Notice that when Weaver starts, it defaults to reading the OpenTelemetry semantic convention:</p> <pre><code>Weaver Registry Live Check\nResolving registry `https://github.com/open-telemetry/semantic-conventions.git[model]`\n</code></pre> <p>You'll know Weaver is ready when you see:</p> <pre><code>The OTLP receiver will stop after 60 seconds of inactivity.\n</code></pre>"},{"location":"opentelemetry-weaver-telemetry-quality/#step-3-send-a-trace","title":"Step 3: Send a Trace","text":"<p>Use docker / podman and the OpenTelemetry telemetrygen tool to send a single trace to Weaver.</p> <p>The command sends the trace to the special <code>host.docker.internal</code> address (which means <code>localhost</code> on the host machine where docker is running). The <code>--otlp-insecure</code> flag is also set because Weaver is listening insecurely (that's OK because this is test data + we're on localhost).</p> <pre><code>docker run --rm \\\n  ghcr.io/open-telemetry/opentelemetry-collector-contrib/telemetrygen:v0.128.0 \\\n  traces --traces=1 \\\n  --otlp-insecure=true \\\n  --otlp-endpoint=host.docker.internal:4317\n</code></pre> <p>The container will run and exit. If it works, the last message should be:</p> <pre><code>INFO    traces/traces.go:64  stopping the exporter\n</code></pre>"},{"location":"opentelemetry-weaver-telemetry-quality/#step-4-stop-weaver","title":"Step 4: Stop Weaver","text":"<p>Stop Weaver either by pressing <code>Ctrl + C</code> or sending a <code>curl</code> command to the <code>/stop</code> endpoint of Weaver:</p> <pre><code>curl -X POST http://localhost:4320/stop\n</code></pre>"},{"location":"opentelemetry-weaver-telemetry-quality/#step-5-inspect-weaver-output-report","title":"Step 5: Inspect Weaver Output Report","text":"<p>Open <code>weaver-output/live_check.json</code> and inspect the output report.</p> <p>In this report Weaver has checked <code>8</code> entities: <code>1</code> resource, <code>2</code> spans and <code>5</code> attributes. It has found <code>6</code> advisories.</p> <pre><code>...\n\"total_advisories\": 6,\n\"total_entities\": 8,\n\"total_entities_by_type\": {\n  \"attribute\": 5,\n  \"resource\": 1,\n  \"span\": 2\n}\n...\n</code></pre> <p>Scrolling back up, each \"thing\" gets an array of <code>all_advice</code> given for that \"thing\". For example, here's the output for <code>1</code> of the span attributes.</p> <p>Weaver has identified one <code>violation</code> and one <code>improvement</code> and thus, for this span attribute, the <code>highest_advice_level</code> (ie. the worst thing about it) is a <code>voilation</code>.</p> <pre><code>{\n  \"live_check_result\": {\n    \"all_advice\": [\n      {\n        \"advice_level\": \"violation\",\n        \"advice_type\": \"deprecated\",\n        \"message\": \"Replaced by `network.peer.address`.\",\n        \"value\": \"renamed\"\n      },\n      {\n        \"advice_level\": \"improvement\",\n        \"advice_type\": \"stability\",\n        \"message\": \"Is not stable\",\n        \"value\": \"development\"\n      }\n    ],\n  \"highest_advice_level\": \"violation\"\n},\n\"name\": \"net.sock.peer.addr\",\n\"type\": \"string\",\n\"value\": \"1.2.3.4\"\n}\n</code></pre> <p>There are so many more statistics that Weaver produces and this is a gold-mine for things like preventing CICD pipelines from progressing if telemetry is not \"good enough\".</p> <p>Watch this in action</p> <p>If you want to see this in action, check out this YouTube video.</p>"},{"location":"opentelemetry-weaver-telemetry-quality/#summary","title":"Summary","text":"<p>In this post you've discovered how Weaver, a tool from the OpenTelemetry project, can help you to understand the realtime quality of your telemetry data against semantic conventions.</p> <p>Weaver is a new tool and there is a lot more it can do. I'm only just starting to learn it now, so stay tuned and I'll bring you more when I understand more about it. It certainly seems very useful for DevOps / SRE folk!</p>"},{"location":"oras-store-anything-dockerhub/","title":"Using CNCF ORAS to Store Anything on Dockerhub","text":"<p>Did you know that you can store ANYTHING on Dockerhub, not just containers / microservices?</p> <p>In this video, I demonstrate how you can use the CNCF project ORAS to do exactly that...</p>"},{"location":"osquery-opentelemetry/","title":"Bringing osquery together with OpenTelemetry","text":"<p>osquery is a fantastic little tool, typically used in security setups, which turns your endpoints into a queryable SQL database (eg. <code>SELECT * from processes</code>).</p> <p>In this video I show you how to grab that data and send it to your Observability system so that now, both Observability (APM) data and security data can co-exist in a single place.</p>"},{"location":"osticket-automation/","title":"osTicket Automated Install Script","text":"<p>osTicket is one of the leading open source ticketing systems. Here's an easy way to spin up an instance on Ubuntu.</p> <p>Note: This script is purely intended for use in short-lived demo systems. The passwords are obvious (<code>password</code> !?) and there is no hardening applied to any of this system. DO NOT USE THIS SCRIPT IN PRODUCTION. </p>"},{"location":"osticket-automation/#create-shell-script","title":"Create Shell Script","text":"<p>Spin up an Ubuntu instance then create a new shell script and give it executable permissions:</p> <pre><code>touch osTicket.sh\nchmod +x osTicket.sh\n</code></pre>"},{"location":"osticket-automation/#script-content","title":"Script Content","text":"<p>Paste this into the script:</p> <pre><code>#!/usr/bin/env bash\n\nsudo apt update -y\nsudo apt install unzip apache2 php7.2 php7.2-mysql mysql-server -y\nsudo a2enmod rewrite\nsudo rm /var/www/html/index.html\nmkdir osticket &amp;&amp; cd osticket\nwget https://github.com/osTicket/osTicket/releases/download/v1.14.1/osTicket-v1.14.1.zip\nunzip osTicket-v1.14.1.zip\nsudo mv upload/* /var/www/html\nsudo mv /var/www/html/include/ost-sampleconfig.php /var/www/html/include/ost-config.php\nsudo chmod 0666 /var/www/html/include/ost-config.php\nsudo mysql -u root -e \"CREATE DATABASE osticket;\"\nsudo mysql -u root -e \"CREATE USER 'osticket'@'localhost' IDENTIFIED BY 'password';\"\nsudo mysql -u root -e \"GRANT ALL PRIVILEGES ON * . * TO 'osticket'@'localhost';\"\nsudo chown ubuntu:ubuntu /etc/apache2/sites-available/000-default.conf\ncat &lt;&lt;EOF &gt;&gt; /etc/apache2/sites-available/000-default.conf\n&lt;Directory /var/www/&gt;\nAllowOverride All\n&lt;/Directory&gt;\nEOF\nsudo service apache2 restart\n</code></pre>"},{"location":"osticket-automation/#install-via-ui","title":"Install via UI","text":"<p>Open a browser and navigate to <code>http://IPADDRESS/setup</code> and follow the wizard.</p> <p>The script has created a database already (I advise adjusting these details for security):</p> <ul> <li>Database Name: <code>osticket</code></li> <li>Database User: <code>osticket</code></li> <li>Database Password: <code>password</code></li> </ul> <p>Note: You may get an HTTP <code>500</code> error during the install. That's fine. Just refresh the page and it'll work.</p>"},{"location":"osticket-automation/#cleanup-after-install","title":"Cleanup after Install","text":"<p>After the installation, you'll need to delete the <code>setup</code> directory and reset some file permissions. Just run this:</p> <pre><code>sudo rm -rf /var/www/html/setup\nsudo chmod 0644 /var/www/html/include/ost-config.php\n</code></pre>"},{"location":"osticket-automation/#conclusion","title":"Conclusion","text":"<p>At this point, you should have a fully working osTicket installation. Enjoy!</p>"},{"location":"otel-for-everything/","title":"OpenTelemetry for everything","text":"<p>I've wrote a blog over on <code>dev.to</code> introducing tracepusher and OpenTelemetry.</p> <p>Check out OpenTelemetry traces for everything with tracepusher.</p>"},{"location":"pitometer-open-source-autonomous-quality-gates/","title":"Pitometer - Open Source Autonomous Quality Gates","text":"<p>Legacy Service</p> <p>This tutorial refers to the legacy \"Pitometer\" service which has now been superceded by KEPTN QUALITY GATES.</p> <p>I recommend reading this Keptn + Prometheus Tutorial instead.</p> <p>Imagine releasing software with zero upfront meetings. Imagine a release that was tested and promoted (or rejected) through your pipeline automatically. Imagine being able to mix and match the tools that provide those metrics. Imagine a world where you didn\u2019t constantly argue over release acceptance / quality criteria. Imagine an open source tool that did all of that. Meet Pitometer...</p> <p>Pitometer is an open source tool which allows you to specify your definitions of software quality in code, pull metrics from anywhere, then automatically grade your software into pass, warning or fail states.</p> <p>Your pipeline can then leverage that decision to promote or reject the deployment.</p> <p>Pitometer is a module within the keptn project (more on keptn in future blog posts). However Pitometer can also be used as a standalone tool in any environment, not only cloud native environments.</p>"},{"location":"pitometer-open-source-autonomous-quality-gates/#simple-example","title":"Simple Example","text":"<ul> <li>You have an upcoming code deployment.</li> <li>You know the figures you need to hit. The business have signed off on them.</li> <li>You\u2019re using both Prometheus and Dynatrace to monitor various parts of the system.</li> </ul>"},{"location":"pitometer-open-source-autonomous-quality-gates/#criteria-1-throughput","title":"Criteria 1 - Throughput","text":"<ul> <li>Measured by: Prometheus</li> <li>Success Criteria: 100 requests per second.</li> </ul>"},{"location":"pitometer-open-source-autonomous-quality-gates/#criteria-2-response-time","title":"Criteria 2 - Response time","text":"<ul> <li>Measured by: Dynatrace</li> <li>Success Criteria: Average response time less than 3 seconds.</li> </ul>"},{"location":"pitometer-open-source-autonomous-quality-gates/#translation-to-pitometer","title":"Translation To Pitometer","text":"<p>Let\u2019s translate the above into Pitometer code.</p> <p>Crucially, this performance as code specification can be committed to any code repository, just like your source code.</p> <p>{% raw%} <pre><code>{\n  \"indicators\": [\n  {\n    \"id\": \"prometheus_throughput\",\n    \"source\": \"Prometheus\",\n    \"query\": \"rate(http_requests_total)\",\n    \"grading\": {\n      \"type\": \"Threshold\",\n      \"thresholds\": {\n        \"lowerWarning\": 110,\n        \"lowerSevere\": 100\n      },\n      \"metricScore\": 50\n    }\n  },\n  {\n    \"id\": \"dynatrace_response_time\",\n    \"source\": \"Dynatrace\",\n    \"query\": {\n      \"timeseriesId\": \"com.dynatrace.builtin:service.responsetime\",\n      \"aggregation\": \"avg\"\n    },\n    \"grading\": {\n        \"type\": \"Threshold\",\n        \"thresholds\": {\n          \"upperSevere\": 3000000,\n          \"upperWarning\": 2500000\n        },\n        \"metricScore\": 50\n      }\n  }],\n  \"objectives\": {\n    \"pass\": 90,\n    \"warning\": 75\n  }\n}\n</code></pre> {% endraw %}</p>"},{"location":"pitometer-open-source-autonomous-quality-gates/#explanation","title":"Explanation","text":"<p>We have specified two indicators. The first sources its data from Prometheus by running the <code>rate(http_requests_total)</code> query. Pitometer will then grade the results based on the Threshold grader.</p> <p>Since we\u2019re using lower thresholds, the results will be evaluated as \"are they below the threshold?\".</p> <p>For example, a throughput value of 112 would be a <code>pass</code>. A throughput value of 109 would be in a <code>warning</code> state. A throughput of 100 or less would mean this indicator is in a <code>fail</code> state.</p> <p>For thresholds, an indicator that passes gets 100% of the <code>metricScore</code>.</p> <ul> <li>An indicator in a <code>warning state</code> = 50% of metricScore.</li> <li>An indicator in a <code>fail state</code> = metricScore value of zero.</li> </ul> <p>The second indicator pulls the average response time data from Dynatrace then evaluates against its thresholds.</p> <ul> <li>A response time above 2.5 seconds is in <code>warning state</code>.</li> <li>A response time above 3 seconds is in a <code>severe state</code>.</li> </ul> <p>Both indicators are weighted equally (due to each having a <code>metricScore</code> of 50).</p> <p>You can adjust the importance of each indicator by assigning different total weights to each metric score.</p>"},{"location":"pitometer-open-source-autonomous-quality-gates/#outcomes-objectives","title":"Outcomes (Objectives)","text":"<p>The final block of code objectives evaluates both indicator results against a threshold.</p> <ul> <li>If the total metric score is above 90, the deployment is in a pass state.</li> <li>Between 75 and 90 and the deployment is a warning state.</li> <li>Below 75 and the deployment is in a fail state.</li> </ul>"},{"location":"pitometer-open-source-autonomous-quality-gates/#final-thoughts","title":"Final Thoughts","text":"<ul> <li>If you\u2019re using Kubernetes, I highly recommend you use keptn, rather than a standalone implementation of Pitometer.</li> <li>You get all the benefits of Pitometer plus the awesome power of keptn.</li> <li>Pitometer follows the \u201ceverything as code\u201d philosophy.</li> <li>No more meetings or arguments about deployment health!</li> <li>Pitometer can assess metrics from any source that is accessible via an API.</li> <li>If you can get the metrics out, Pitometer can evaluate them.</li> </ul>"},{"location":"pitometer-open-source-autonomous-quality-gates/#stay-tuned","title":"Stay Tuned","text":"<p>If you\u2019d like to know more about keptn or Pitometer, contact me or follow me on LinkedIn.</p> <p>I\u2019ll be releasing a full working demonstration system of Pitometer soon.</p> <p>Stay tuned...</p>"},{"location":"prometheus-metrics-python-flask-api/","title":"Prometheus Metrics & Python Flask API","text":"<p>In this video I will show you how to instrument a Python FastAPI to emit or send Prometheus Metrics. I also show those same metrics being captured in Prometheus.</p>"},{"location":"prometheus-service-discovery-otel-target-allocator/","title":"Prometheus Service Discovery and OpenTelemetry Target Allocator","text":"<p>In this post &amp; video I'll explain how to get metrics from a Kubernetes cluster the easy way. Starting with service discovery and <code>http_sd_config</code> then using the OpenTelemetry Target Allocator.</p> <p>Here's how to get Prometheus metrics from your Kubernetes applications easily. First I show the \"basic\" way for a single cluster using the service discovery mechanisms in the OpenTelemetry collector http_sd_config.</p> <p>Then I extend the usecase to cover a more realistic multi-cluster, production ready setup using the OpenTelemetry Operator and Target Allocator.</p> <p>GitHub repo: https://github.com/agardnerIT/k8s-target-allocator</p> <p> CrewAI Observability</p>"},{"location":"prometheus-service-discovery/","title":"Prometheus Service Discovery","text":"<p>Any realistic usage of Prometheus will have tens, if not hundreds, of scraped endpoints. It becomes impossible to know, track and manage each of these manually.</p> <p>Thankfully, Prometheus offers a feature called Service Discovery which allows you to delegate the maintenance and discovery of endpoints to a secondary service. In other words, you tell Prometheus to \"go over here and get the endpoints from this thing\" and it will. No more manual effort involved from you.</p> <p>This video explains how it all works, with a live demo - of course.</p>"},{"location":"pyrsia-summary/","title":"Hands on with Pyrsia in under 5 minutes: Video Summary","text":"<p>This is a text-based version of this video so if you prefer reading to watching, read on!</p> <p>Want to jump straight to the Pyrsia hands-on? </p>"},{"location":"pyrsia-summary/#pyrsia-decentralized-package-network","title":"Pyrsia: Decentralized Package Network","text":"<p>Pyrsia is the newest CDFoundation project.</p>"},{"location":"pyrsia-summary/#what-is-pyrsia","title":"What is Pyrsia?","text":"<p>Pyrsia is a decentralized package network. Independant build nodes individually build open source software package (at the time of writing, Docker images and Maven packages).</p> <p>Each build node in the network commits the build results to a blockchain and only when consensus is reached, will the artifact be available on the network.</p> <p>This design ensures that images are what they promise to be and no one actor can compromise the network - because all build nodes need to agree. If one build node was to be compromised and commit a different set of metadata to the blockchain, the network would reject the build.</p> <p>This blockchain (immutable ledger) and consensus approach promises a step up in security for those relying on open source software and hopefully will minimise software supply chain risks.</p> <p>All actions are committed to the Pyrsia transparency log - which any Pyrsia user can inspect at any time.</p>"},{"location":"pyrsia-summary/#pyrsia-hands-on","title":"Pyrsia Hands On","text":"<p>During the Pyrsia hands-on exercise, you will install Pyrsia, connect to the network and pull an image from the network.</p> <p>You will then retrieve the transparency log for that image to see how the network came to a consensus about that image.</p>"},{"location":"pyrsia-summary/#new-project-each-week","title":"New Project Each Week","text":"<p>Each week in 2023 I will pick a new open source project to review. The entire list is here.</p> <p>The series is designed to be interactive - if you have a project you'd like me to review - please do get in contact and suggest a project!</p> <p>I hope you enjoy the Pyrsia project!</p>"},{"location":"python-fastapi/","title":"My First API with Python FastAPI","text":"<p>Last week I wrote my first API in Python and I thought I'd share my learnings. FastAPI is a brilliantly easy-to-use framework. Here's how to create an API in under 6 minutes.</p>"},{"location":"realtime-tracking-prometheus/","title":"Realtime Visual Tracking of People plus Prometheus Stats","text":"<p>In this post I describe a realtime human tracking application, written in Python which also makes statistics available via Prometheus metrics.</p> <p> </p> <pre><code># HELP number_of_people_detected Current count of people detected\n# TYPE number_of_people_detected gauge\nnumber_of_people_detected 1.0\n</code></pre> <p>Next week I'll be attending the AWS Summit in Sydney. The weather was also atrocious this weekend, which gave me time to pose myself a hypothetical question:</p> <p>The Question</p> <p>How could we leverage AI &amp; open source standards to track demand at the Dynatrace booth?</p> <p>For this exercise I envisioned a camera on the booth facing downwards and tracking the current number of individuals at the Dynatrace booth. The task would be to track the number of people in frame - not individual people. The output of this would be somehow fed into Dynatrace and therefore we could track, in realtime, the demand at the booth throughout the day.</p> <p>I am happy to say: It works! and here's a link to the source code: Realtime video recognition in Python.</p> <p>Theory Only</p> <p>This is a theoretical exercise only. We will not be doing this at AWS Summit in Sydney.</p>"},{"location":"realtime-tracking-prometheus/#privacy-concerns","title":"Privacy Concerns","text":"<p>Before I dig into the technical, a quick note about the privacy concerns that could be presented with a solution like this:</p> <ol> <li>Individual tracking of users</li> <li>Transmission and storage of the footage</li> <li>Tracking of other items of interest in the footage</li> </ol> <p>As written, the code:</p> <ol> <li>Does not track individual users. It tracks the number of people currently detected in the frame. That's all.</li> <li>Any other items detected in the footage (ie. anything the model detects from the object classes are simply ignored). We don't care if a fire hydrant, elephant or book is detected in the frame - the code ignores it.</li> <li>The footage is never streamed nor stored. It is analysed in realtime and discarded. For such a system to work, there is simply no need to record or archive the footage in any way.</li> </ol> <p>What results is a completely anonymous \"person count\" currently detected in frame - and that's all. No personally identifiable information (PII), no saved data. Nothing.</p>"},{"location":"realtime-tracking-prometheus/#hardware-required","title":"Hardware Required","text":"<p>I'm happy to say there is no special hardware required. I'm running this on my laptop and leveraging the built-in camera. I've also verified it working with a USB connected camera - so your hardware should be fine.</p>"},{"location":"realtime-tracking-prometheus/#software-explained","title":"Software Explained","text":"<p>I'll be honest, I'm an AI newbie, especially with computer vision &amp; recognition but this post and this post helped tremendously - along with the great documentation from Ultralytics. Most of my code is not really mine - it's code from the above blogs. Credit where it's due!</p> <p>Start the App</p> <p>You may need to experiment with the camera-index value until you get the correct camera. Start at <code>0</code> and work up until you hit the correct one.</p> <pre><code>python webcamdetect.py --camera-index=0\n</code></pre> <p>Here's how it works:</p> <ol> <li>The program starts</li> <li>The program downloads the correct model (<code>yolov8n</code> by default, but this is configurable)</li> <li>The program gets a handle to your webcam stream</li> <li>The YOLO model is used to detect humans in each frame (there are parameters to tweak the model + certainty required to detect a human)</li> <li>The total number of humans currently detected are written to a Prometheus Gauge metric</li> <li>A prometheus server is started on <code>http://localhost:8000</code> (there's another parameter to alter the port)</li> </ol> <p>The end result is a live feed which may look smooth or somewhat jittery depending on the value you set with the <code>--sleep-time-between-updates</code> flag.</p> <p>Any humans that enter the frame will have a box placed around them - as long as the model is certain enough that they really are a human (you can tweak this confidence interval using the <code>--minimum-confidence-required</code> flag).</p> <p>Point a browser at <code>http://localhost:8000</code> and wonderful, scrapable Prometheus metrics shall appear.</p>"},{"location":"realtime-tracking-prometheus/#sending-data-to-dynatrace","title":"Sending Data to Dynatrace","text":"<p>To get the data from the Prometheus endpoint into Dynatrace, use the OpenTelemetry Collector.</p> <p>Start by saving a YAML file called <code>config.yaml</code></p> <pre><code>receivers:\n    prometheus:\n      config:\n        scrape_configs:\n          - job_name: 'detections'\n            scrape_interval: 5s\n            static_configs:\n              - targets: ['127.0.0.1:8000']\n\nprocessors:\n    resourcedetection:\n      detectors: [\"system\"]\n      system:\n        hostname_sources: [\"os\"]\n\nexporters:\n  otlphttp/dynatrace:\n    endpoint: https://${env:DT_ENVIRONMENT}.live.dynatrace.com/api/v2/otlp\n    headers:\n      Authorization: \"Api-Token ${env:DT_API_TOKEN}\"\n\nservice:\n  pipelines:\n    metrics:\n      receivers: [prometheus]\n      processors: [resourcedetection]\n      exporters: [otlphttp/dynatrace]\n</code></pre> <p>scrape interval</p> <p>Set the <code>scrape_interval</code> to be longer than the value you use for <code>--sleep-time-between-updates</code>.</p> <p>After all, it makes no sense to pull metrics from Prometheus more often than they're actually updated, right?</p> <p>Config YAML Explained</p> <p>This configuration YAML tells the OTEL collector to retrieve metrics from <code>http://127.0.0.1:8000</code> every 5 seconds, enrich the metrics it finds with information pulled from the operating system (so Dynatrace users know where the metrics originated) and then to send the metrics to Dynatrace.</p>"},{"location":"realtime-tracking-prometheus/#start-collector","title":"Start Collector","text":"<p>First, go to Dynatrace, press <code>ctrl + k</code>. Search for <code>API tokens</code> and create a token with <code>metrics.ingest</code> permissions.</p> <p>Also make a note of your environment ID. It is the first portion of the URL. eg. <code>abc12345</code>.</p> <p>Customise the following snippet then run to start the collector and push metrics into Dynatrace.</p> <pre><code>// Customise these two values\nexport DT_ENVIRONMENT=abc12345\nexport DT_API_TOKEN=dt0c01.sample.secret\n/path/to/collector/otelcol-contrib --config=config.yaml\n</code></pre> <p>If all goes well, you should be able to chart the <code>number_of_people_detected</code> metric.</p> <p></p> <p>Get the Code</p> <p>Reminder: All code for this is open source here: Realtime Video Recognition with Prometheus Statistics</p>"},{"location":"redis-in-python/","title":"Using Redis with Python for Beginners","text":"<p>Here's how to use Python in Redis in under 4 minutes. This video will get you started with the very basics of working with Redis in Python. You'll also see cache busting techniques (aka automated removal of values from Redis after a set amount of time).</p> <p>This video is meant for beginners to get you up and running as quickly as possible.</p>"},{"location":"saltstack-101-setup-configuration/","title":"Saltstack 101 - Setup and Configuration Guide","text":"<p>Salt / SaltStack is an open-source piece of software for remote administration, configuration automation and event-driven orchestration. This tutorial will show you how to get it up and running...</p>"},{"location":"saltstack-101-setup-configuration/#prerequisites","title":"Prerequisites","text":"<ul> <li>A DigitalOcean account (free trial using this link).</li> </ul>"},{"location":"saltstack-101-setup-configuration/#saltstack-architecture","title":"Saltstack Architecture","text":"<p>SaltStack has a very simple architecture. One or more master nodes issue commands to one or more minion nodes. Each minion has an id (a name) and the minion is configured to point to its master. By default, the id is the hostname of the machine, but this can be overridden.</p> <p>Note: Multiple masters and masterless configurations are supported, but this guide will not discuss these further.</p> <p></p>"},{"location":"saltstack-101-setup-configuration/#communication-paths","title":"Communication Paths","text":"<p>Minions are configured to know about their master. Minions then reach back to their master. An administrator chooses to accept or deny that minion.</p> <p>Once \u201caccepted\u201d, commands are issued from the master out to the minion(s).</p>"},{"location":"saltstack-101-setup-configuration/#setup","title":"Setup","text":"<p>General Setup</p> <p>Note: There is a bootstrap option available here but in the interests of transparency, I\u2019ll be installing it the \"full\" way.</p> <ul> <li>Create two Ubuntu 16.04 DigitalOcean droplets (other cloud hosts or VMs will also work). The smallest $5 droplet is sufficient for this tutorial.</li> <li>Give each machine a different hostname. I\u2019ll call mine <code>saltstack-master</code> and <code>saltstack-minion</code></li> </ul> <p></p> <p>Master Setup</p> <p>SSH into <code>saltstack-master</code> and run the following commands. The first three lines just get things setup and add the saltstack repository to your system. The fourth line updates your packages and pulls down the latest package list. The fifth line does the magic, installing the <code>salt-master</code> with the <code>-y</code> parameter to skip all manual inputs and assume yes to all questions.</p> <pre><code>wget -O - https://repo.saltstack.com/apt/ubuntu/16.04/amd64/latest/SALTSTACK-GPG-KEY.pub | apt-key add -\ntouch /etc/apt/sources.list.d/saltstack.list\necho 'deb http://repo.saltstack.com/apt/ubuntu/16.04/amd64/latest xenial main'| tee -a /etc/apt/sources.list.d/saltstack.list\napt-get update\napt-get install salt-master -y    \n</code></pre> <p>Minion Setup</p> <p>SSH into <code>saltstack-minion</code> and run the following commands. Notice they\u2019re very similar to the master. The only difference is that we install <code>salt-minion</code> and not <code>salt-master</code>.</p> <pre><code>wget -O - https://repo.saltstack.com/apt/ubuntu/16.04/amd64/latest/SALTSTACK-GPG-KEY.pub | apt-key add -\ntouch /etc/apt/sources.list.d/saltstack.list\necho 'deb http://repo.saltstack.com/apt/ubuntu/16.04/amd64/latest xenial main'| tee -a /etc/apt/sources.list.d/saltstack.list\napt-get update\napt-get install salt-minion -y\n</code></pre>"},{"location":"saltstack-101-setup-configuration/#point-minion-at-master","title":"Point Minion at Master","text":"<p>Remember at the beginning of the article I said that the <code>minion</code> knows about the <code>master</code>? How? We need to specify the master\u2019s IP address on the minion.</p> <p>On the minion:</p> <pre><code>nano /etc/salt/minion\n</code></pre> <p>Look for the line which reads <code>#master:salt</code>. Below this line, add a new line. Replace <code>10.0.0.0</code> with the IP address of your master. Save and exit the file.</p> <p>Now run your salt minion simply by typing <code>salt-minion</code> on the minion.</p>"},{"location":"saltstack-101-setup-configuration/#accept-minion-on-master","title":"Accept Minion on Master","text":"<p>Flip back to the master node and run <code>salt-key -L</code>. This will list the status of all keys on the master. You should see something like this:</p> <pre><code>root@saltstack-master:~# salt-key -L\nAccepted Keys:\nDenied Keys:\nUnaccepted Keys:\n  saltstack-minion\nRejected Keys:\n</code></pre> <p>This proves the minion has been able to attempt a connection to the master. Tell the master to accept connections from the minion:</p> <pre><code>salt-key -a saltstack-minion -y\n</code></pre> <p>Note: You can also accept all minions with <code>salt-key -A -y</code></p>"},{"location":"saltstack-101-setup-configuration/#youre-installed-test-it-out","title":"You're Installed. Test It Out.","text":"<p>Now let\u2019s verify that the master can find the minion. We\u2019ll use a basic ping from the <code>test</code> module. From master: <code>salt 'saltstack-minion' test.ping</code> and you\u2019ll see:</p> <pre><code>root@saltstack-master:~# salt 'saltstack-minion' test.ping\nsaltstack-minion:\n  True\n</code></pre>"},{"location":"saltstack-101-setup-configuration/#bonus-create-write-a-file","title":"Bonus: Create &amp; Write a File","text":"<p>If you\u2019ve got this far, then you have a working SaltStack and the world is your proverbial Oyster. One more demo while you\u2019re here. Let\u2019s create and write a line of text from the master to the minion:</p> <pre><code>salt 'saltstack-minion' file.touch \"/tmp/myText.txt\"\nsalt 'saltstack-minion' file.append /tmp/myText.txt \"This is my content...\"\n</code></pre>"},{"location":"saltstack-101-webhooks/","title":"Saltstack 101 - Webhooks","text":"<p>Control Saltstack remotely via HTTP(S) webhooks and automate your IT infrastructure...</p>"},{"location":"saltstack-101-webhooks/#prerequisites","title":"Prerequisites","text":"<p>If you\u2019ve followed the previous tutorial you will have a working salt master and minion machine which can successfully talk to each other. I highly recommend you review the previous tutorial before continuing.</p> <p>While following this tutorial you have two choices:</p> <ul> <li>Use a domain name with a proper (not self signed) SSL certificate (recommended approach).</li> <li>Setup the system in unsecured HTTP mode (easier, but obviously less secure).</li> </ul> <p>We will be using LetsEncrypt to issue a free SSL certificate and since domain names are cheap, I recommend you get one too.</p> <p>There are a number of sites offering free domain names available. I\u2019ve never used any of these services so I\u2019m not willing to vouch for them.</p>"},{"location":"saltstack-101-webhooks/#scenario","title":"Scenario","text":"<p>In this tutorial, we will:</p> <ul> <li>Install an Apache server on the <code>minion</code>. This will mimic the customer\u2019s webserver.</li> <li>Create an HTTP endpoint on the salt <code>master</code>. This will act as a webhook to receive POST values.</li> <li>Control the <code>minion</code> remotely via webhooks.</li> <li>Demonstrate the webhook functionality by remotely starting and stopping the apache server.</li> </ul>"},{"location":"saltstack-101-webhooks/#point-domain-to-master","title":"Point Domain to Master","text":"<p>If using your own domain, make sure it\u2019s pointing at your master now.</p>"},{"location":"saltstack-101-webhooks/#install-customer-web-server","title":"Install Customer Web Server","text":"<p>INSTALL CUSTOMER WEB SERVER</p> <p>Install apache2 on the minion.</p> <p>Remember this apache2 will not be served your domain, but will be accessible via the minion\u2019s IP address.</p> <p>From the master, run: <code>salt 'saltstack-minion' pkg.install apache2</code></p> <p>Check that it\u2019s up and running. You should see the default Apache page.</p>"},{"location":"saltstack-101-webhooks/#install-salt-api-on-master","title":"Install Salt API on Master","text":"<p>The <code>salt-api</code> package allows remote connections to the salt master.</p> <p>On the master run: <code>sudo apt-get install salt-api -y</code></p> <p>Ensure outputs of both <code>salt-api --version</code> and <code>salt --version</code> match (on the master):</p> <pre><code>root@saltstack-master:~# salt-api --version\n  salt-api 2018.3.0 (Oxygen)\nroot@saltstack-master:~# salt --version\n  salt 2018.3.0 (Oxygen)\n</code></pre>"},{"location":"saltstack-101-webhooks/#create-rest-user","title":"Create REST User","text":"<p>This user will be used to authenticate while using the REST interface. On the master:</p> <pre><code>sudo useradd demouser\nsudo passwd demouser\nGRANT REST USER PERMISSIONS\n</code></pre> <p>Now give your <code>demouser</code> permissions to access all saltstack functions.</p> <p>On the master: <code>nano /etc/salt/master</code></p> <p>Search for <code>external_auth</code> and uncomment the template to give your demouser permissions.</p> <p>Your code should look like this:</p> <pre><code>external_auth:\n  pam:\n    demouser:\n      - .*\n</code></pre>"},{"location":"saltstack-101-webhooks/#install-cherrypy","title":"Install CherryPy","text":"<p>This is a python webserver which will give us our webhook endpoints First, we need the <code>python3-setuptools</code> package as it\u2019s used by cherrypy.</p> <pre><code># Install pip and the setuptools package as it's used by cherrypy\nsudo apt-get install python-pip python3-setuptools -y\n# Now install cherrypy\nsudo pip install cherrypy\n# Make sure cherrypy is up to date\nsudo pip install --upgrade cherrypy\n</code></pre>"},{"location":"saltstack-101-webhooks/#create-ssl-certificate","title":"Create SSL Certificate","text":"<p>If you\u2019re using HTTP, skip this step. Install the LetsEncrypt certbot and generate your SSL certificates. Replace <code>example.com</code> and <code>www.example.com</code> with your domain names.</p> <p>Run the following on the master:</p> <pre><code>sudo add-apt-repository ppa:certbot/certbot -y\nsudo apt-get update\nsudo apt-get install certbot -y\nsudo certbot certonly --standalone --agree-tos --email me@example.com -n -d example.com -d www.example.com\n</code></pre> <p>Certs will be stored in:</p> <pre><code>/etc/letsencrypt/live/example.com/fullchain.pem\n/etc/letsencrypt/live/example.com/privkey.pem\n</code></pre>"},{"location":"saltstack-101-webhooks/#add-cherrypy-configuration-to-master","title":"Add CherryPy Configuration to Master","text":"<p>Add the following configuration (altering as appropriate) to the end of your <code>/etc/salt/master</code> config file.</p> <pre><code>nano /etc/salt/master\n</code></pre> <p>Then add this:</p> <pre><code>rest_cherrypy:\n  port: 8000\n  # Uncomment following line only if you're running without a domain name and in HTTP mode.\n  #disable_ssl: True\n  # Uncomment following lines if using custom domain with LetsEncrypt\n  #ssl_crt: /etc/letsencrypt/live/example.com/fullchain.pem\n  #ssl_key: /etc/letsencrypt/live/example.com/privkey.pem\n</code></pre>"},{"location":"saltstack-101-webhooks/#restart-salt-master-and-salt-api","title":"Restart Salt Master and Salt API","text":"<pre><code>sudo service salt-master stop &amp;&amp; sudo service salt-api stop\nsudo service salt-master start &amp;&amp; sudo service salt-api start\n</code></pre>"},{"location":"saltstack-101-webhooks/#youve-come-a-long-way-baby","title":"You've Come A Long Way Baby...","text":"<p>If you\u2019re here you should be able to access the master on port <code>8000</code> (either via HTTPS with your domain or HTTP with an IP address). Either way, you should have no SSL certificate warnings.</p> <p>You\u2019ll get a <code>200 OK</code> response and this text:</p> <pre><code>{\"clients\": [\"local\", \"local_async\", \"local_batch\", \"local_subset\", \"runner\", \"runner_async\", \"ssh\", \"wheel\", \"wheel_async\"], \"return\": \"Welcome\"}\n</code></pre>"},{"location":"saltstack-101-webhooks/#retrieve-your-auth-token","title":"Retrieve Your Auth Token","text":"<p>In order to use the REST endpoints, we need to authenticate. The portable authentication module (<code>pam</code>) makes this easy.</p> <p>Login once with your username and password and you\u2019ll receive a token</p> <pre><code>POST https://DOMAIN-OR-MASTER-IP:8000/login\nHeaders:\n  Content-Type: application/json\n\nBody:\n{\n  \"username\": \"demouser\",\n  \"password\": \"password\",\n  \"eauth\": \"pam\"\n}\n</code></pre> <p>Copy the token value from the returned content and include it in the <code>X-Auth-Token</code> header.</p>"},{"location":"saltstack-101-webhooks/#ping-minions-via-rest","title":"Ping Minions Via REST","text":"<pre><code>POST https://DOMAIN-OR-MASTER-IP:8000\nHeaders:\n  X-Auth-Token: token value from above...\n\nRequest Body:\n{\n  \"client\": \"local\",\n  \"tgt\": \"*\",\n  \"fun\": \"test.ping\"\n}\n</code></pre> <p>fun meaning function, not fun and frivolity :)</p> <p>If successful, you should see this content returned:</p> <pre><code>{\n\"return\": [\n    {\n        \"saltstack-minion\": true\n    }\n  ]\n}\n</code></pre> <p>Congratulations. Your ping test was successful and you can now successfully control your infrastructure via webhooks and REST calls. How very DevOps of you!</p>"},{"location":"saltstack-dynatrace-integration/","title":"Saltstack 101 - Dynatrace Integration","text":"<p>Auto-heal your infrastructure when problems occur with Dynatrace &amp; SaltStack...</p> <ul> <li>Part 1 of this 3 part series showed how to setup and configure SaltStack</li> <li>Part 2 of this series showed how to use setup HTTPS endpoints with CherryPy in SaltStack</li> </ul> <p>This, the final part of the series will show you how to effortlessly monitor and create an auto-healing service with a combination of Dynatrace and SaltStack.</p>"},{"location":"saltstack-dynatrace-integration/#prerequisites","title":"Prerequisites","text":"<ul> <li>A Dynatrace account. Sign up for a free 15 day trial here.</li> </ul> <p>All of the following prerequisites will be met if you\u2019ve followed parts 1 and 2 of this series.</p> <ul> <li>Running master and minion SaltStack instances.</li> <li>CherryPy running and configured on the master</li> <li>An apache2 server running on the minion.</li> </ul>"},{"location":"saltstack-dynatrace-integration/#scenario","title":"Scenario","text":"<p>Imagine that the <code>apache2</code> process running on the <code>minion</code> is your critical infrastructure. Not only do you need to monitor this, but you want to ensure it auto-heals in case of any issues.</p>"},{"location":"saltstack-dynatrace-integration/#installing-dynatrace-oneagent","title":"Installing Dynatrace OneAgent","text":"<p>Install your Dynatrace OneAgent by grabbing the install commands from your tenant: <code>https://YOUR-TENANT-ID.live.dynatrace.com/#install/agentlinux</code></p> <p></p> <p>Execute these commands on your minion\u2026</p> <p>For example:</p> <pre><code>wget -O Dynatrace-OneAgent-Linux-1.141.124.sh \"https://YOUR-TENANT-ID.live.dynatrace.com/api/v1/deployment/installer/agent/unix/default/latest?Api-Token=***&amp;arch=x86&amp;flavor=default\"\nsudo /bin/sh Dynatrace-OneAgent-Linux-1.141.124.sh APP_LOG_CONTENT_ACCESS=1\n</code></pre> <p>Navigate to the hosts screen in Dynatrace and after a few moments you will see your <code>saltstack-minion</code>. Drill into it and notice that it has already automatically discovered the <code>apache2</code> process.</p> <p>In order to monitor this fully, we need to restart <code>apache2</code>. Either execute <code>service apache2 restart</code>, or if you want to do it via REST call to SaltStack:</p> <pre><code>POST https://YOUR-DOMAIN-OR-SALT-MASTER-IP:8000\nHeaders\nX-Auth-Token: YOUR-AUTH-TOKEN\nRequest Body\n{\n  \"client\": \"local\",\n  \"tgt\": \"saltstack-minion\",\n  \"fun\": \"service.restart\",\n  \"arg\": \"apache2\"\n}\n</code></pre> <p>Notice how Dynatrace has automatically detected all process stats and the process restart...</p> <p></p>"},{"location":"saltstack-dynatrace-integration/#integrate-saltstack-via-webhook","title":"Integrate SaltStack Via Webhook","text":"<p>Create a new webhook integration from within your tenant: <code>https://YOUR-TENANT-ID.live.dynatrace.com/#settings/integration/notification/integrationwebhook</code></p>"},{"location":"saltstack-dynatrace-integration/#process-death-alerts-vs-user-impact-alerts","title":"Process Death Alerts vs. User Impact Alerts","text":"<p>This is an optional step, but it is worth pointing out.</p> <p>In order to prevent \u201calert storms\u201d and noise, the default behaviour of Dynatrace is to only create problems when they are genuine and impacting real users. In other words, if requests to the service are impacted (ie. if web requests to <code>apache2</code> fail). Dynatrace will raise such problems after a few moments.</p> <p>For this reason, if we kill <code>apache2</code> now, Dynatrace will raise a problem with two events. The first will be that service requests are impacted and the second, the root cause is that the <code>apache2</code> process is unavailable.</p> <p>This behaviour is configurable. You can choose to receive an alert when the process dies, should you wish.</p> <p>To configure this, go to the Technologies link &gt; Click Apache &gt; Click Process Group Details &gt; Edit &gt; Availability Monitoring.</p> <p>Now select if any process becomes unavailable from the dropdown box.</p> <p></p>"},{"location":"saltstack-dynatrace-integration/#create-chaos","title":"Create Chaos","text":"<p>All that remains is to kill <code>apache2</code>, sit back and watch everything else auto-heal thanks to Dynatrace and SaltStack.</p> <pre><code>POST https://YOUR-DOMAIN-OR-SALT-MASTER-IP:8000\nHeaders\nX-Auth-Token: YOUR-AUTH-TOKEN\nRequest Body\n{\n  \"client\": \"local\",\n  \"tgt\": \"saltstack-minion\",\n  \"fun\": \"service.stop\",\n  \"arg\": \"apache2\"\n}\n</code></pre>"},{"location":"saltstack-dynatrace-integration/#what-really-happens","title":"What Really Happens","text":"<ul> <li>You kill the <code>apache2</code> process</li> <li>Dynatrace notices and raises a problem. As soon as this problem is raised, Dynatrace triggers the call to the SaltStack webhook.</li> <li>SaltStack takes remediation action. In this case, restarting <code>apache2</code></li> </ul> <p>Notice that you can also see the impact while the <code>apache2</code> process was offline. Dynatrace gives you the rate of TCP error connections too so you can see how many users were impacted during the downtime.</p> <p></p> <p></p>"},{"location":"secure-prometheus-json/","title":"Securely Transform JSON into Prometheus","text":"<p>In this video I will show you how to instrument a Python FastAPI to emit or send Prometheus Metrics. I also show those same metrics being captured in Prometheus.</p> <p>(click the graphic to watch on YouTube )</p>"},{"location":"serverless-implications/","title":"Implications of Going Serverless","text":"<p>As I have previously mentioned, I recently moved this site to GitHub Pages. This post will investigate the performance impacts of that decision.</p>"},{"location":"serverless-implications/#reliability","title":"Reliability","text":"<p>Reliability is defined as the \"probability that a system will produce correct outputs\" (source Wikipedia).</p> <p>Think about the old site vs. the new site, the old site consisted of a webserver (Apache), PHP engine and a MySQL database. The new site consists of a markdown file (and potentially a few images), committed to a Git repository.</p> <p>There are far fewer failure points (inside my control) in the new setup than in the old. In the old system, I had to ensure I kept each of the following components up to date:</p> <ul> <li>Apache webserver.</li> <li>PHP version.</li> <li>MySQL DB.</li> <li>The droplet (virtual machine).</li> <li>Wordpress installation.</li> </ul> <p>All of these were potential failure points &amp; additional overhead. There was always the chance that any of the connections between these components failed and affected the reliability of the site.</p> <p>For example, the DB could be hacked. The webserver would still be running and available but the data would not be retrieved - therefore affecting the reliability.</p> <p>In the new system, the parts inside my control are: - My GitHub account. - The markdown pages and images.</p> <p>I've committed the markdown content to GitHub, which is of course, a version control system - so incremental backups are taken care of.</p> <p>I've also saved a copy of the files to my cloud storage: My redundant backup should GitHub decide to go offline.</p> <p>GitHub Pages powers help.github.com among many other websites. If GitHub Pages were to have a problem, you can bet that GitHub would know about it and care enough to fix it.</p> <p>Unfortunately Pages does not publish availability figures, although I would be willing to bet that they match or exceed anything I could achieve on my own.</p>"},{"location":"serverless-implications/#availability","title":"Availability","text":"<p>Commit the markdown code to the <code>master</code> branch &amp; let GitHub handle the rest. Sounds promising! It also sounds like whatever magic occurs in the background, it had better be reliable.</p> <p>This post from 2015 suggests that GitHub Pages powered about 700,000 sites. I am going to suppose that number is significantly higher today.</p> <p>If GitHub are providing a service to over 1 million sites, it is probably fair to assume they'll take their availability seriously.</p> <p>As previously mentioned, the fewer components to go wrong, all else being equal, the more reliable and available the site / service will be. It does not get much simpler than a markdown file.</p>"},{"location":"serverless-implications/#maintenance","title":"Maintenance","text":"<p>In the old world, I had to periodically update each of the components. The webserver software, database, the Wordpress installation and the VM itself.</p> <p>What do I need to maintain in the new world? Nothing. Well, I need to ensure my GitHub account doesn't get hacked. Then again, there's nothing to hack - everything is publicly available.</p> <p>Don't tell me you're relying on security through obscurity...</p>"},{"location":"serverless-implications/#response-time","title":"Response Time","text":"<p>Response time is a function of various factors but in simple terms:</p> <ul> <li>Page weight (the number of objects it takes to load a page &amp; their size).</li> <li>Geolocation relative to the end user.</li> <li>Quality of the infrastructure serving the content.</li> </ul>"},{"location":"serverless-implications/#before","title":"Before","text":"<p>Look at the \"before\" picture. The old site loaded 25 resources. The page size was 0.5MB. The old page took over 2 seconds to load:</p> <p></p>"},{"location":"serverless-implications/#after","title":"After","text":"<p>Now the \"after picture\". The new site loads 12 resources. The page size is almost half the previous weight at 280kb. This means that the page loads in just 0.5 seconds. That's a 75% percent improvement in response time:</p> <p></p>"},{"location":"serverless-implications/#conclusion","title":"Conclusion","text":"<p>Moving to a \"serverless\" model simply means delegating responsibility of infrastructure to someone else. Whether or not that fits your operating model, usecases and level of trust depends entirely on you.</p> <p>The negative impacts of going serverless stem from the above - if something goes wrong with their infrastructure, you're affected but cannot necessarily do anything to rectify the situation. That is why great quality monitoring is essential in a serverless world.</p> <p>That said, if you're running a small personal site or blog, I see few reasons not to move to a serverless model.</p> <p>As always, if you have any questions or comments, feel free to contact me. </p>"},{"location":"serverless-voting/","title":"Serverless, Zero Database Voting System","text":"<p>I needed a voting system for this website which was compatible with serverless pages. I also wanted it to be zero-login which ruled out using a third-party plugin. The result was a serverless, zero database &amp; zero login voting system using AWS. Here is how...</p>"},{"location":"serverless-voting/#visualising-results","title":"Visualising Results","text":"<p>An essential piece of this system was the ability to quickly visualise the results.</p> <p>CloudWatch makes this easy. Votes are counted from the log files and displayed on a CloudWatch dashboard:</p> <p></p>"},{"location":"serverless-voting/#overview-architecture","title":"Overview / Architecture","text":"<p>The system consists of 4 parts:</p> <ol> <li>An HTML form</li> <li>AWS API Gateway</li> <li>AWS Lambda Code</li> <li>AWS CloudWatch (with dashboard)</li> </ol> <p></p>"},{"location":"serverless-voting/#build-html-form","title":"Build HTML Form","text":"<p>First, you'll need an HTML form to register the vote. You can see a running example at the bottom of this article, but basically any form will do.</p> <p>You will <code>POST</code> this form to the AWS Gateway. The Gateway acts as the \"front door\" for the Lambda serverless function.</p>"},{"location":"serverless-voting/#build-lambda-function","title":"Build Lambda Function","text":"<p>The backend of our voting system looks like this:</p> <p></p> <p>The Lambda function will process the incoming data and log it to CloudWatch. In this architecture, Lambda is our backend processing. Crucially though, Lambda functions only exist while they're executing.</p> <p>Log in to AWS and go to Lambda. Click \"Create Function\" and \"Author from Scratch\".</p> <p>Give your function a sensible name and set the Runtime to be \"Python 3.7\".</p> <p>Leave everything else as-is and click \"Create Function\".</p> <p>If everything works you should see the default Lambda code:</p> <pre><code>import json\n\ndef lambda_handler(event, context):\n    # TODO implement\n    return {\n        'statusCode': 200,\n        'body': json.dumps('Hello from Lambda!')\n    }\n</code></pre> <p>Remove all that code and replace with something like this:</p> <pre><code>import json\nimport boto3\n\ndef lambda_handler(event, context):\n\n    # \"event\" has all our info. Get the HTTP method.\n    httpMethod = event[\"httpMethod\"]\n\n    # If it is a POST, get the eventBody JSON.\n    if httpMethod == \"POST\":\n\n      eventBody = json.loads(event[\"body\"])\n\n      pageURL = \"\"\n\n      if (\"pageURL\" in eventBody):\n          pageURL = eventBody[\"pageURL\"];\n          # Log this vote into CloudWatch\n          print(\"VOTE {}\".format(pageURL))\n\n      return {\n          'statusCode': 200,\n          'headers': {\n              \"Access-Control-Allow-Origin\": \"https://YOUR-DOMAIN-NAME-HERE\",\n              \"Access-Control-Allow-Headers\": \"Content-Type\"\n            },\n          'body': json.dumps('Vote recorded.')\n      }\n    # Else HTTP Method is not a post, return the default.\n    else:\n      return {\n        'statusCode': 200,\n        'headers': {\n              \"Access-Control-Allow-Origin\": \"https://YOUR-DOMAIN-NAME-HERE\",\n              \"Access-Control-Allow-Headers\": \"Content-Type\"\n            },\n        'body': 'OK. Non POST used. Try POST instead.'\n      }\n</code></pre> <p>The integration is not yet complete, but ultimately, the above code will execute every time someone clicks the \"thumbs up\" icon on the webpage.</p> <p>First, the <code>event</code> is interrogated to retrieve the HTTP method. If it is a <code>POST</code> then we execute some code, <code>else</code> we return an HTTP <code>200 OK</code> to the user with a message: <code>OK. Non POST used. Try POST instead.</code> We have thus defined that our endpoint should only process <code>POST</code> requests.</p> <p>Your HTML form should <code>POST</code> data to the AWS Gateway endpoint.</p>"},{"location":"serverless-voting/#access-control-headers","title":"Access Control Headers","text":"<p>Two return headers are required by AWS and they add an extra layer of security: <code>Access-Control-Allow-Origin</code> and <code>Access-Control-Allow-Headers</code>.</p> <p><code>Access-Control-Allow-Origin</code> specifies that we will only allow traffic from the specified domain. This helps to ensure that someone else can't just copy and paste your HTML form and register fake votes from their domain.</p> <p>The <code>Access-Control-Allow-Headers</code> header specifies that the only header we allow is the <code>Content-Type</code> header. The HTML form sets a <code>Content-Type</code> header of <code>application/json</code>.</p>"},{"location":"serverless-voting/#post-request","title":"POST Request","text":"<p>If the request is a <code>POST</code> then the code loads the <code>eventBody</code> as a JSON formatted object. Then we look inside the <code>eventBody</code> for a variable called <code>pageURL</code>. This maps directly to the JSON we're passing from the form: <code>{ \"pageURL\": \"/somePage\" }</code>.</p> <p>If a <code>pageURL</code> is found, we log that into cloudwatch using the <code>print</code> statement. The format of this log entry will be <code>VOTE /myPage</code>.</p> <p>For example, if the form JSON read: <code>{ \"pageURL\": \"/pageOne\" }</code> then the log line would be:</p> <pre><code>VOTE /pageOne\n</code></pre>"},{"location":"serverless-voting/#test-lambda-function","title":"Test Lambda Function","text":"<p>Test your function with this data:</p> <pre><code>{\n  \"httpMethod\": \"POST\",\n  \"body\": \"{ \\\"pageURL\\\": \\\"/testPage\\\"}\"\n}\n</code></pre> <p>Click \"Test\". When the function succeeds, view the CloudWatch log and you should see:</p> <pre><code>START RequestId: ***\nVOTE /testPage\nEND RequestId: ***\nREPORT RequestId: ***\n</code></pre> <p></p>"},{"location":"serverless-voting/#create-entry-point-with-api-gateway","title":"Create Entry Point with API Gateway","text":"<p>The Lambda function has no way of being triggered yet. Fix that now by creating an API Gateway. The API Gateway will provide a URL and will be the access point to your Lambda code. The gateway is where the HTML form will <code>POST</code> data.</p> <p>Edit your Lambda function and click \"Add Trigger\" button. Select \"API Gateway\". Choose to create a new API (HTTP API). Click \"Add\".</p> <p>Back on the Lambda main page, scroll down and you should see a URL. Make a note of this.</p> <p>Your HTML form will <code>POST</code> data to the AWS Gateway URL.</p> <p></p>"},{"location":"serverless-voting/#recap","title":"Recap","text":"<p>If you've followed this far, you have:</p> <ol> <li>An HTML form configured to <code>POST</code> events to the API Gateway URL.</li> <li>Requests to the gateway cause Lambda to execute.</li> <li>Lambda then prints a log line into Cloudwatch.</li> </ol> <p>The only bit we're missing is a dashboard to view the votes.</p>"},{"location":"serverless-voting/#build-cloudwatch-dashboard","title":"Build CloudWatch Dashboard","text":"<p>Within CloudWatch, go to Dashboards &gt; Create.</p> <p>When prompted to add a widget, just click Cancel.</p> <p>You should now see a blank dashboard.</p> <p>Click Actions &gt; View/Edit source.</p> <p>Remove the current content and replace with the following. Change the <code>***</code> value to match your Lambda function name. Adjust the values to match the AWS region in which you're executing.</p> <p>For example, if your Lambda function was called <code>blogWriteup</code> then it would be: <code>SOURCE '/aws/lambda/blogWriteup'</code>.</p> <pre><code>{\n    \"widgets\": [\n        {\n            \"type\": \"log\",\n            \"x\": 0,\n            \"y\": 0,\n            \"width\": 24,\n            \"height\": 6,\n            \"properties\": {\n                \"query\": \"SOURCE '/aws/lambda/***' | fields @message\\n| filter @message like /(VOTE)/\\n| parse 'VOTE *' as page\\n| stats count(page) as `Vote Count` by page | sort `Vote Count` desc\",\n                \"region\": \"ap-southeast-2\",\n                \"title\": \"Votes Per Page\",\n                \"view\": \"bar\"\n            }\n        }\n    ]\n}\n</code></pre> <p></p> <p>Save your dashboard.</p> <p></p>"},{"location":"serverless-voting/#summary","title":"Summary","text":"<p>Congratulations. You now have a serverless, zero database, zero infrastructure, zero login and zero cost voting system.</p> <p>If you'd like to test this, click the thumbs-up button below. Also click the thumbs-up button if you'd like a Youtube walkthrough of this post.</p>"},{"location":"service-process-group-naming-autonomous-monitoring/","title":"Service & Process Group Naming - Autonomous Monitoring","text":"<p>Part three of my Dynatrace autonomous cloud management (ACM) series will focus on automatic service and process group naming.</p> <p>This tutorial series builds from one stage to the next. If you haven\u2019t read part one (host group naming) and part two (automated metadata)), I strongly suggest you start there.</p>"},{"location":"service-process-group-naming-autonomous-monitoring/#aim-of-tutorial","title":"Aim of Tutorial","text":"<p>We\u2019re aiming for a situation whereby the process groups (PGs) &amp; services are prepended with the host group tag. That is, the <code>staging</code> processes are split (and visually shown) as separate from the <code>production</code> PGs and services. This makes it easy to see which service belongs to which environment.</p> <p>Note: The naming rules described in this article don\u2019t actually split the groups. The splitting is due to the different host groups. The naming rule is just a visual aid \u2013 otherwise you\u2019d have two Apache Web Server <code>httpd</code> process groups &amp; services.</p> <p></p> <p></p>"},{"location":"service-process-group-naming-autonomous-monitoring/#how","title":"How?","text":"<p>First, we need to install and start Apache on both instances. Add the following tasks at the end of your playbook:</p> <p>{% raw%} <pre><code>- name: Install HTTPD (Apache)\n  become: yes\n  package:\n    name: httpd\n    state: present\n\n- name: Start HTTPD (Apache)\n  become: yes\n  service:\n    name: httpd\n    state: started\n</code></pre> {% endraw %}</p> <p>Execute your playbook then check in Dynatrace to see the <code>httpd</code> processes.</p> <p>Open port 80 inbound to the instances and visit the public IP address for each instance in a browser. Hit refresh a few times per server to generate some load. This ensures the services show in the Dynatrace UI. </p> <p>Services without traffic are hidden in the Dynatrace UI.</p> <p>As mentioned above, the Apache process groups and services are already split because the hosts belong to different host groups. We simply need to prepend the host group name to both the process group and service.</p>"},{"location":"service-process-group-naming-autonomous-monitoring/#process-group-naming","title":"Process Group Naming","text":"<p>Go to: Settings &gt; Processes and Containers &gt; Process group naming Create a new rule which has the following name format:</p> <p>{% raw %} <pre><code>{HostGroup:Name} - {ProcessGroup:DetectedName}\n</code></pre> {% endraw %}</p> <p>Set the Condition to be where the <code>Host group name exists</code>.</p>"},{"location":"service-process-group-naming-autonomous-monitoring/#service-naming","title":"Service Naming","text":"<p>Go to Settings &gt; Server-side service monitoring &gt; Service naming rules</p> <p>Create a new rule which has the following name format:</p> <p>{% raw %} <pre><code>{HostGroup:Name} - {Service:DetectedName}\n</code></pre> {% endraw %}</p> <p>Set the Condition to be where the <code>Host group name exists</code>.</p> <p></p> <p></p>"},{"location":"service-process-group-naming-autonomous-monitoring/#validation","title":"Validation","text":"<p>Navigate to the Technologies menu (under \u2018Monitor\u2019) and click the Apache tile. You should see two Apache process groups:</p> <p></p> <p>Navigate to the Transactions and Services screen (under \u2018Monitor\u2019). You should see two Apache services:</p> <p></p> <p>This technique is entirely future proof. Whenever a new environment is onboarded (aka a new host group), Dynatrace will automatically split and rename your process groups and services accordingly.</p>"},{"location":"sql-injection-security-basics/","title":"SQL Injection: Security Basics","text":"<p>What is a SQL Injection? How is one performed? How can you mitigate against these attacks?</p>"},{"location":"sql-injection-security-basics/#introduction","title":"Introduction","text":"<p>Despite being 48 years old, SQL is still going strong. Also still going strong is the SQL injection (OWASP has this at third spot on their vulnerabilities list for 2021). 48 years and we're still suffering from this!</p>"},{"location":"sql-injection-security-basics/#live-demo","title":"Live Demo","text":"<p>Here is a live demo tutorial hosted on Killercoda which walks you through this post with a hands-on demo.</p>"},{"location":"sql-injection-security-basics/#what-is-a-sql-injection","title":"What is a SQL Injection?","text":"<p>A SQL injection (or really any kind of Injection attack) occurs primarily because the developer has trusted the user input and passed it straight to the backend - in this case, the SQL database.</p> <p>When a user is allowed to pass input directly into a backend, inevitably \"bad\" users will attempt to abuse this trust and make the program (in this case the database) run something it \"shouldn't\".</p> <p>It's important at this point to note that the SQL being passed is technically \"valid\". The SQL engine sees that it is a valid command and tries to run it.</p> <p>That said, what a language parser / program considers \"technically valid\" is not at all what a human (i.e. the developer) may call \"valid\".</p> <p>For example, imagine a program asks you to enter the last name (surname) of a user to retrieve their details.</p> <p>You enter <code>Bloggs</code> and the input is taken and used to formulate this SQL statement:</p> <pre><code>SELECT Address FROM Accounts WHERE LastName='Bloggs'\n</code></pre> <p>Nothing wrong with that and nothing nefarious either.</p> <p>Now instead of <code>Bloggs</code>, enter <code>fake' OR true;--</code></p> <p>Suddenly the application returns all user details, not just <code>Bloggs</code>.</p>"},{"location":"sql-injection-security-basics/#why-did-this-happen","title":"Why Did This Happen?","text":"<p>Look again at the first SQL statement. Your input is passed into the <code>WHERE</code> clause.</p> <p>So the second (dangerous) SQL statement is:</p> <pre><code>SELECT Address FROM Accounts WHERE LastName='fake' OR true;--'\n</code></pre> <p>If LastName is <code>fake</code> then why does it even work? Surely the DB doesn't contain anyone with the surname <code>fake</code>? No, but let's break that SQL statement down...</p> <pre><code>SELECT Address FROM Accounts WHERE LastName='fake'\n</code></pre> <p>That part is valid SQL and you're correct that no-one has the surname <code>fake</code>. So far, so good.</p> <p><code>OR</code> is a valid SQL keyword and <code>true</code>. Well, <code>true</code> is always going to be <code>true</code> right?</p> <p>So in effect you're saying: \"Give me all addresses where the LastName is fake or yes\". Since SQL interprets the \"or yes\" to always true then in effect, every user in the database will match that last part and will be included for you.</p> <p>The final parts of the statement: <code>;--</code> are some SQL magic. The <code>;</code> means \"end the line here\", but remember we have that pesky extra <code>'</code> to deal with (the counts of which always need to match). Lucky for us, <code>--</code> in SQL means \"ignore everything afterwards\".</p> <p>So in the end, we end up with an effective SQL statement of:</p> <pre><code>SELECT Address FROM Accounts WHERE LastName='fake' OR true;\n</code></pre> <p>We now know that, due to the <code>OR true</code>, this will always match and thus you get every Address in the database.</p>"},{"location":"sql-injection-security-basics/#contrived-example","title":"Contrived Example?","text":"<p>At this point, you might think this is a simplified and contrived example only dreamt up in order to create a blog post.</p> <p>You might reasonably point out that many frameworks attempt to protect against SQL injection attacks automatically. You may also say (correctly) that all input should be validated.</p> <p>I agree with you, everything above is true.</p> <p>But if this is so easy to avoid, why is it still the 3rd most popular vulnerability in 2021 according to OWASP?</p> <p>Precisely because this is so simple means it is pervasive.</p>"},{"location":"sql-injection-security-basics/#prevention","title":"Prevention","text":"<p>How can you prevent SQL injections?</p> <ol> <li>Never trust user input and do not attempt to \"sanitise\" input. Always positively validate input. If it looks \"fishy\", throw it away. Do not try to \"make sense\" the input.</li> <li>Use available tools like frameworks that attempt to check for SQL injections.</li> <li>Use controls like <code>LIMIT</code> to slow down or prevent mass data breaches if you are victim to an attack.</li> <li>Some commercial and open source tools offer on-the-fly runtime SQL injection blocking (like Dynatrace)</li> </ol> <p>More details available via OWASP.</p> <p>Title image credit: Photo modify from original by Diana Polekhina on Unsplash</p>"},{"location":"sunglasses-software-performance/","title":"What Do Sunglasses Have To Do With Software Performance?","text":"<p>So, what do sunglasses have to do with software performance?</p> <p>For the moment, leave aside the marketing aspect. The how you found out that these sunglasses were good quality. For the purpose of this post, I include price in the marketing aspect as pricing and discounts really are a function of marketing. When a consumer purchases on price alone, they're purchasing on marketing alone, not solely quality. See loss leaders as one example strategy for this.</p> <p>We'll focus instead on two areas:</p> <ol> <li>The customer (or prospective buyer) perspective of why they believe the sunglasses are good quality</li> <li>The business perspective of how they drove that perception of quality</li> </ol> <p>This really isn't rocket science but it amazes me how many companies still (in 2021) neglect these (to me) basic guiding principles.</p>"},{"location":"sunglasses-software-performance/#two-opportunities","title":"Two Opportunities","text":"<p>If my company is in the business of manufacturing sunglasses, it has two opportunities to improve the quality of the glasses and the perception of my company:</p> <ol> <li>During design &amp; manufacture cycle</li> <li>During post sales cycle</li> </ol>"},{"location":"sunglasses-software-performance/#quality-during-design-and-manufacture","title":"Quality During Design and Manufacture","text":"<p>How does my company differentiate our sunglasses from every other market player? Perhaps we invest in better R&amp;D so our shapes are better, different or more suited to our customer profile than our competitors. I invest in style.</p> <p>Perhaps we invest in better materials? Clearer lenses, scratch resistant coatings, UV protections, polarisation technology and so forth. Maybe the materials from which make the frames are lighterweight or more durable so the glasses don't break as easily as the competitors glasses.</p> <p>In short, we improve the quality of the product that goes out the door.</p>"},{"location":"sunglasses-software-performance/#post-sales-quality","title":"Post Sales Quality","text":"<p>So a customer has purchased and comes to you with an issue. Their glasses are defective in some way. The quicker you fix (or remediate) their issue, the better they will think of your product and company.</p> <p>Do you offer free exchanges (over and above consumer protection laws) for faults? If so, how effortless, quick and cheap is the process for me to get the glasses back to you?</p> <p>Do you take that one step further and offer exchanges even if they aren't faulty? Do you offer 24/7 support? Perhaps you offer a money back guarantee or free exchanges if they don't suit within a certain time period? What are you</p> <p>In short, how quickly and accurately can you fix my issue and make me happy again?</p>"},{"location":"sunglasses-software-performance/#software-sunglasses","title":"Software = Sunglasses","text":"<p>The software your company produces really is no different from my sunglasses. You too have two opportunities to improve quality of your software:</p> <ol> <li>Better software into production in the first place</li> <li>If and when things go wrong (and they will) how fast you remediate those issues and return customers to their happy state</li> </ol>"},{"location":"sunglasses-software-performance/#final-thoughts","title":"Final Thoughts","text":"<p>As I said at the beginning, this isn't rocket science but companies I deal with still regularly either struggle to achieve this or (worse) seem to ignore these basic principles.</p> <p>To me, the worse the two above things are, the more money you have to spend on marketing to \"paper over the cracks\" so to me, it just makes sense! Then again, I'm not in marketing so maybe I'm missing something!?</p> <p>I'm very keen to know your experiences:</p> <ul> <li>What have you / your company tried or implemented in this area?</li> <li>What worked, what didn't? What have you learned, unlearned (old habits die hard)</li> <li>Where did you succeeded or fail?</li> <li>Where are you stuck for next steps? What blockers are you facing?</li> </ul> <p>Please do connect with me on LinkedIn and we'll continue the conversation there either publicly or via private mesage if you prefer.</p>"},{"location":"tomcat-9-on-aws-centos/","title":"Tomcat 9 on AWS CentOS","text":"<p>The Amazon Linux AMI 2018.03.0 image comes with Tomcat 7 and 8. Copy and paste this code into your SSH terminal to install Java 8 and Tomcat 9.</p> <pre><code># Install Java 8\nsudo yum install java-1.8.0-openjdk.x86_64 -y\n# Set Java_HOME and JRE_HOME to java8\necho JAVA_HOME=\"/usr/lib/jvm/jre-1.8.0-openjdk.x86_64\" | sudo tee -a /etc/profile\necho JRE_HOME=\"/usr/lib/jvm/jre-1.8.0-openjdk.x86_64\" | sudo tee -a /etc/profile\n# Refresh JAVA_HOME and JRE_HOME\nsource /etc/profile\nwget https://www-eu.apache.org/dist/tomcat/tomcat-9/v9.0.19/bin/apache-tomcat-9.0.19.tar.gz\ntar xvf apache-tomcat-9.0.19.tar.gz\n~/apache-tomcat-9.0.19/bin/startup.sh\n# Server running at http://SERVERIP:8080\n# If not, check ~/apache-tomcat-9.0.12/logs/catalina.out\n</code></pre>"},{"location":"tracing-gitlab-pipelines-opentelemetry/","title":"OpenTelemetry Tracing Gitlab Pipelines","text":"<p>In this video, I explain how to automatically generate OpenTelemetry traces for all your Gitlab pipelines.</p> <p>Links:</p> <ul> <li>Gitlab sample .gitlab-ci.yml</li> <li>Tracepusher</li> <li>OpenTelemetry</li> </ul>"},{"location":"trivy-keptn-quality-gates/","title":"Security Quality Gates with Aquasec Trivy and Keptn","text":"<p>Building an automated security quality gate with Aquasec Trivy and Keptn...</p>"},{"location":"trivy-keptn-quality-gates/#prerequisites","title":"Prerequisites","text":"<p>To follow along you'll need:</p> <ul> <li>Docker (optional)</li> <li>A Dynatrace account (15 day free trial here)</li> <li>A Kubernetes cluster</li> <li>Helm</li> </ul>"},{"location":"trivy-keptn-quality-gates/#outcomes","title":"Outcomes","text":"<p>Problem Statement: - I want to build security into my release pipeline - I want to do so in a flexible way that affords me the ability to switch out my tooling later - I want the flexibility to use any monitoring backend of my choosing</p>"},{"location":"trivy-keptn-quality-gates/#evaluating-a-container-image","title":"Evaluating a Container Image","text":"<p>I will use Aquasec Trivy to scan a container image for vulnerabilities.</p> <p>This container image will also (optionally) post metrics (vulnerability counts) to a backend. In this case, Dynatrace, whenever the <code>METRICS_ENDPOINT</code> and <code>METRICS_API_TOKEN</code> are set. </p> <p>Try it now in standalone mode (without pushing metrics to a backend):</p> <pre><code>$ docker run --rm -e TRIVY_SECURITY=CRITICAL -e IMAGE=nginx -e TAG=1.21.1 gardnera/trivy:v1\n</code></pre> <p>Congratulations, Trivy has identified 8 <code>CRITICAL</code> vulnerabilities in <code>nginx:1.21.1</code>. <pre><code>METRICS_ENDPOINT and/or METRICS_API_TOKEN environment variables not set. Tool will still work but metrics will not be pushed to a backend.\nChecking 1 security levels: ['CRITICAL']\nFinished. Total Vulnerability Count: 8\n</code></pre></p>"},{"location":"trivy-keptn-quality-gates/#emit-metrics-to-a-backend","title":"Emit Metrics to a Backend","text":"<p>The container contains logic to be compatible with Dynatrace as a backend. Just add two environment variables and metrics will be pushed to Dynatrace.</p> <p>Dynatrace stores metrics using a unique metric key and the container writes metrics with the following key format: <code>trivy.vulnerabilities.SEVERITY</code> split by <code>image</code> and <code>tag</code> values.</p> <p>For example, you'll receive a metric called <code>trivy.vulnerabilities.CRITICAL</code> which you can then (optionally) split by <code>image</code> and / or <code>tag</code> values.</p> <p>Generate a new Dynatrace API token (Settings &gt; Integration &gt; Dynatrace API) with <code>ingest metrics</code> and <code>read metrics</code> permissions.</p> <p></p> <p>Execute the image again, this time also specifying your backend details:</p> <p><pre><code>$ docker run --rm \\\n-e TRIVY_SECURITY=CRITICAL \\\n-e IMAGE=nginx \\\n-e TAG=1.21.1 \\\n-e METRICS_ENDPOINT=abc123.live.dynatrace.com \\\n-e METRICS_API_TOKEN=dtc01.*** \\\ngardnera/trivy:v1\n</code></pre> Go to <code>Explore data</code> and chart the <code>trivy.vulnerabilities.CRITICAL</code> metric.</p> <p></p>"},{"location":"trivy-keptn-quality-gates/#create-github-repo","title":"Create GitHub Repo","text":"<ol> <li>Create an uninitialised Git repo. Do not even add a readme file.</li> <li>Create a Personal access token with <code>repo</code> scope</li> </ol>"},{"location":"trivy-keptn-quality-gates/#install-keptn","title":"Install Keptn","text":"<p>Execute <code>kubectl get namespaces</code> and you should see the default namespaces.</p> <p>Download the keptn CLI and install Keptn to your cluster, exposing via a LoadBalancer:</p> <pre><code>$ curl -sL https://get.keptn.sh | bash\n$ keptn install --endpoint-service-type=LoadBalancer\n</code></pre> <p>Set some environment variables to store the Keptn API URL and API token:</p> <pre><code>$ KEPTN_API_URL=http://$(kubectl get service/api-gateway-nginx -n keptn -o=jsonpath='{.status.loadBalancer.ingress[0].ip}')/api\n$ KEPTN_API_TOKEN=$(kubectl get secret keptn-api-token -n keptn -ojsonpath='{.data.keptn-api-token}' | base64 --decode)\n</code></pre> <p>Use these details to authenticate the keptn CLI and check the status of keptn to get your bridge URL:</p> <pre><code>$ keptn auth --endpoint=$KEPTN_API_URL --api-token=$(kubectl get secret keptn-api-token -n keptn -ojsonpath={.data.keptn-api-token} | base64 --decode)\n$ keptn status\n</code></pre> <p>Store the Keptn's bridge username and password in environment variables and log into to the bridge via your browser:</p> <pre><code>$ KEPTN_BRIDGE_USERNAME=$(kubectl get secret -n keptn bridge-credentials -o jsonpath=\"{.data.BASIC_AUTH_USERNAME}\" | base64 --decode)\n$ KEPTN_BRIDGE_PASSWORD=$(kubectl get secret -n keptn bridge-credentials -o jsonpath=\"{.data.BASIC_AUTH_PASSWORD}\" | base64 --decode)\n</code></pre> <p>Set two final environment variables to hold your Dynatrace tenant details:</p> <pre><code>$ DT_TENANT=abc123.live.dynatrace.com\n$ DT_API_TOKEN=dtc01.***\n</code></pre>"},{"location":"trivy-keptn-quality-gates/#create-dynatrace-secret-in-keptn","title":"Create Dynatrace Secret in Keptn","text":"<p>Create a secret using hte <code>keptn create secret</code> command. Thsi will be used later to interact and authenticate with Dynatrace during hte quality gate execution:</p> <pre><code>$ keptn create secret dynatrace-api-token \\\n--from-literal=\"DT_TENANT=$DT_TENANT\" \\\n--from-literal=\"DT_API_TOKEN=$DT_API_TOKEN\" \\\n--from-literal=\"KEPTN_API_URL=$KEPTN_API_URL\" \\\n--from-literal=\"KEPTN_API_TOKEN=$KEPTN_API_TOKEN\"\n</code></pre>"},{"location":"trivy-keptn-quality-gates/#recap-so-far","title":"Recap So Far","text":"<p>So far, you have: 1. Executed a docker container which uses Aquasecurity Trivy to identify vulnerabilities in a container image. 1. Executed the same container providing a metrics backend which pushed data into Dynatrace 1. Charted that metric in Dynatrace 1. Installed Keptn onto your cluster</p>"},{"location":"trivy-keptn-quality-gates/#keptn-dynatrace","title":"Keptn &amp; Dynatrace","text":"<p>So far, Keptn is installed with the core services. Keptn interacts with other tools and services via additinoal microservices.</p> <p>We want to give Keptn the ability to interact with Dynatrace, so let's install the Dynatrace service:</p> <pre><code>$ DT_SERVICE_VERSION=0.16.0\n$ helm upgrade --install dynatrace-service -n keptn https://github.com/keptn-contrib/dynatrace-service/releases/download/$DT_SERVICE_VERSION/dynatrace-service-$DT_SERVICE_VERSION.tgz\n</code></pre>"},{"location":"trivy-keptn-quality-gates/#keptn-executes-arbitrary-containers-jobs","title":"Keptn Executes Arbitrary Containers / Jobs","text":"<p>We also need to give Keptn a way to execute our <code>gardnera/trivy:v1</code> container in response to a cloudevent. For that, we'll install the Job Executor Service:</p> <pre><code>$ git clone https://github.com/keptn-sandbox/job-executor-service\n</code></pre> <p>There is a typo in the service.yaml file so fix that quickly and install the service: <pre><code>$ sed -i 's/value: false/value: \"false\"/g' ~/job-executor-service/deploy/service.yaml\n$ kubectl apply -f ~/job-executor-service/deploy/service.yaml\n</code></pre></p>"},{"location":"trivy-keptn-quality-gates/#verify-installation","title":"Verify Installation","text":"<p>Execute <code>kubectl get pods -n keptn</code> and you should see:</p> <pre><code>NAME                      READY   STATUS\napi-gateway-nginx         1/1     Running\napi-service               2/2     Running\napproval-service          2/2     Running\nbridge                    1/1     Running\nconfiguration-service     1/1     Running\ndynatrace-service         2/2     Running\njob-executor-service      2/2     Running\nkeptn-nats-cluster-0      3/3     Running\nlighthouse-service        2/2     Running\nmongodb                   1/1     Running\nmongodb-datastore         2/2     Running\nremediation-servic        2/2     Running\nsecret-service            1/1     Running\nshipyard-controller       2/2     Running\nstatistics-service        2/2     Running\n</code></pre>"},{"location":"trivy-keptn-quality-gates/#create-keptn-project","title":"Create Keptn Project","text":"<p>Create a new file called <code>shipyard.yaml</code>.</p> <p>Everything Keptn does is backed by a cloudevent and Keptn will generate and distribute cloudevents on our behalf.</p> <p>For example, we will send an <code>sh.keptn.event.main.demosequence.triggered</code> event and keptn will automatically trigger the tasks in that workflow. The tooling is then responsible for listening for those task events and performing an action.</p> <p>In our case, we'll tell the trivy container to listen for <code>sh.keptn.event.securityscan.triggered</code> and when it hears that event, the trivy container will run.</p> <pre><code>---\napiVersion: spec.keptn.sh/0.2.0\nkind: Shipyard\nmetadata:\n  name: myshipyard\nspec:\n  stages:\n    - name: main\n      sequences:\n        - name: demosequence\n          tasks:\n          - name: securityscan\n          - name: evaluation\n</code></pre>"},{"location":"trivy-keptn-quality-gates/#create-keptn-project_1","title":"Create Keptn Project","text":"<p>Set your Git details as environment variables and create the Keptn project:</p> <pre><code>$ GIT_USER=YOURUSERNAME\n$ GIT_REPO=https://github.com/YOU/YOUR-REPO\n$ GIT_TOKEN=***\n\n$ keptn create project trivyintegration \\\n--shipyard=shipyard.yaml \\\n--git-user=$GIT_USER \\\n--git-remote-url=$GIT_REPO \\\n--git-token=$GIT_TOKEN\n</code></pre> <p>Next, create a new keptn service inside the <code>trivyintegration</code> project to hold our evaluations: <pre><code>$ keptn create service trivyservice --project=trivyintegration\n</code></pre></p>"},{"location":"trivy-keptn-quality-gates/#use-dynatrace-to-monitor-keptn-project","title":"Use Dynatrace to Monitor Keptn Project","text":"<p>Tell Keptn that you wish to use Dynatrace with this keptn project:</p> <pre><code>$ keptn configure monitoring dynatrace --project=trivyintegration\n</code></pre> <p>You will receive the following warning. We have created and will use a secret called <code>dynatrace-api-token</code> (see <code>keptn create secret</code> command above) so you can safely ignore this warning:</p> <pre><code>failed to load Dynatrace credentials: key DT_TENANT was not found in secret \"dynatrace\"\n</code></pre>"},{"location":"trivy-keptn-quality-gates/#configure-job-executor","title":"Configure Job Executor","text":"<p>It is time to tell the job executor service:</p> <ol> <li>Which Keptn cloudevent it should listen for</li> <li>Which image to execute when that event is heard</li> </ol> <p>This is done via a file created in the repo. Navigate to the <code>main</code> branch of your repo (this branch reflects the stage name in your shipyard file). Create a new folder called <code>job</code> inside the <code>trivyservice</code> folder.</p> <p>Inside this folder, create a file called <code>config.yaml</code>:</p> <pre><code>---\napiVersion: v2\nactions:\n  - name: \"Run Aquasec Trivy\"\n    events:\n      - name: \"sh.keptn.event.securityscan.triggered\"\n    tasks:\n      - name: \"Run Trivy Evaluation\"\n        image: \"gardnera/trivy:v1\"\n        env:\n          - name: TRIVY_SECURITY\n            valueFrom: event\n            value: \"$.data.scan.level\"\n          - name: IMAGE\n            valueFrom: event\n            value: \"$.data.scan.image\"\n          - name: TAG\n            valueFrom: event\n            value: \"$.data.scan.tag\"\n          - name: METRICS_ENDPOINT\n            valueFrom: event\n            value: \"$.data.scan.metrics_endpoint\"\n          - name: METRICS_API_TOKEN\n            valueFrom: event\n            value: \"$.data.scan.metrics_token\"\n</code></pre> <p>This file stipulates that the job executor service will listen for <code>sh.keptn.event.securityscan.triggered</code> events and run the <code>gardnera/trivy:v1</code> image which setting some environment variables that will be pulled from the incoming cloudevent (we'll set these later).</p> <p></p>"},{"location":"trivy-keptn-quality-gates/#use-dynatrace-api-token-secret","title":"Use dynatrace-api-token Secret","text":"<p>Tell Keptn to use a secret called <code>dynatrace-api-token</code> when interacting with Dynatrace. Recall that we created a token near the beginning of this article.</p> <p>On the <code>main</code> branch, navigate back up to the <code>trivyservice</code> directory and create a new folder called <code>dynatrace</code>. Inside this folder, create a file called <code>dynatrace.conf.yaml</code>:</p> <pre><code>---\ndtCreds: dynatrace-api-token\n</code></pre>"},{"location":"trivy-keptn-quality-gates/#provide-service-level-indicators-and-objectives","title":"Provide Service Level Indicators and Objectives","text":"<p>Quality gates are an out-of-the-box feature of Keptn (if you're interested, they're implemented by the <code>lighthouse-service</code>). However, we still need to tell Keptn what metrics we care about (aka what our SLIs are).</p> <p>Alongside the <code>dynatrace.conf.yaml</code> file, create a new file called <code>sli.yaml</code>:</p> <pre><code>---\nindicators:\n  trivy_vulns: \"metricSelector=trivy.vulnerabilities.CRITICAL\"\n</code></pre> <p></p> <p>Now tell Keptn what our Service Level Objectives should be. This is where we add a threshold to our metric.</p> <p>Navigate back to the <code>trivyservice</code> directory and create a file called <code>slo.yaml</code>:</p> <pre><code>---\nspec_version: \"0.1.0\"\nfilter: {}\ncomparison:\n  compare_with: single_result\n  include_result_with_score: pass\n  number_of_comparison_results: 1\n  aggregate_function: avg\nobjectives:\n- sli: trivy_vulns\n  pass:\n    - criteria:\n      - \"&lt;10\"\ntotal_score:\n  pass: 90%\n  warning: 75%\n</code></pre> <p>This file tells Keptn to use the <code>trivy_vulns</code> SLI we defined in <code>sli.yaml</code> and that if the number of vulnerabilities is less than 10, the quality gate is a <code>pass</code>. If the critical vulnerability count is 10 or higher, the gate will <code>fail</code>.</p> <p></p>"},{"location":"trivy-keptn-quality-gates/#ask-keptn-to-execute-sequence","title":"Ask Keptn to Execute Sequence","text":"<p>It is time to ask Keptn to run your task sequence. Do this by <code>POST</code>ing a payload to Keptn.</p> <p>This payload contains some important information, such as:</p> <ol> <li>The image to test (<code>mysql</code>)</li> <li>The image tag to test (<code>8.0.26</code>)</li> <li>The vulnerability level(s) that we care about (<code>CRITICAL</code>)</li> <li>The metrics backend URL (your Dynatrace tenant)</li> <li>The metrics backend API key (you generated this in Dynatrace)</li> <li>The timeframe of the quality gate evaluation (look <code>5 minutes</code> back into the past and grab data)</li> <li>The cloudevent type (the sequence we wnat to run and in which Keptn stage: <code>sh.keptn.event.main.demosequence.triggered</code>)</li> </ol> <pre><code>curl -X POST $KEPTN_API_URL/v1/event \\\n-H \"x-token: $KEPTN_API_TOKEN\" \\\n-H \"Content-Type: application/json\" \\\n-d '{\n  \"data\": {\n    \"project\": \"trivyintegration\",\n    \"service\": \"trivyservice\",\n    \"stage\": \"main\",\n    \"labels\": {\n        \"runBy\": \"agardner.net\"\n    },\n    \"scan\": {\n      \"level\": \"CRITICAL\",\n      \"image\": \"mysql\",\n      \"tag\": \"8.0.26\",\n      \"metrics_endpoint\": \"https://$DT_TENANT\",\n      \"metrics_token\": \"$DT_API_TOKEN\"\n    },\n    \"evaluation\": {\n        \"timeframe\": \"5m\"\n    }\n  },\n  \"source\": \"POSTMAN\",\n  \"specversion\": \"1.0\",\n  \"shkeptnspecversion\": \"0.2.3\",\n  \"contenttype\": \"application/json\",\n  \"type\": \"sh.keptn.event.main.demosequence.triggered\"\n}'\n</code></pre> <p></p>"},{"location":"trivy-keptn-quality-gates/#summary","title":"Summary","text":"<p>This post has shown how easy it is to have Keptn execute arbitrary sequences of tasks using any tooling. I hope this has given you some inspiration and food for thought.</p> <p>Connect with me on LinkedIn and let me know how you use this.</p>"},{"location":"vc-automated-synthetic-scripts/","title":"Version Controlled and Automated Synthetic Scripts","text":"<p>If only there was a way to version control and automate the creation and deployment of Dynatrace Synthetic scripts. Well, I\u2019ve got good news for you...</p> <p>I\u2019ve recently been working with a large DevOps customer who, like many, want everything-as-code so that it can be audited, automated and version controlled. They love the Dynatrace Synthetic robot and it\u2019s capabilities but they needed a way to specify these scripts as code rather than relying on the Dynatrace GUI to record and edit these scripts.</p> <p></p> <p>Luckily for them, the clever minds in the Dynatrace development team have already thought of this and so we can create, store and edit synthetic scripts as JSON files.</p>"},{"location":"vc-automated-synthetic-scripts/#whats-a-synthetic-script-anyway","title":"What's a Synthetic Script, Anyway?","text":"<p>Before we get into how to do this, it\u2019s worth recapping exactly what a synthetic script is and how it\u2019s formed.</p> <ul> <li>A synthetic script is a user journey. It has multiple steps.</li> <li>Each step can have one (or more) actions (wait for time / wait for text / click / form field input etc.)</li> </ul> <p>An example might be a journey in which does the following: - Navigate to the homepage. - Click on Sign In. Enter their username and password. Click Submit. - Once logged in, click another link. - Click logout and wait for the logout message to be shown.</p>"},{"location":"vc-automated-synthetic-scripts/#sample-json-journey","title":"Sample JSON Journey","text":"<p>Here\u2019s a sample user journey defined as a JSON file. The structure is fairly straightforward in that there is an array of steps and within each step, there is an array of actions.</p> <p>This script: 1. Navigates to my homepage, waits for the page to load then waits until the text Posted is found. 2. Clicks the about me link, waits for the page to load then waits until the text About Me is found. 3. Clicks the contact me link, waits for the page to load then completes and submits the contact form. 4. After submission, the robot waits until the text Thank you for your message is displayed.</p>"},{"location":"vc-automated-synthetic-scripts/#store-in-vcs","title":"Store in VCS","text":"<p>Go ahead, store that in your favourite version control system. Then when you\u2019re ready, just upload to the synthetic portal as you would with any other Dynatrace Synthetic script. Easy!</p>"},{"location":"vc-automated-synthetic-scripts/#go-forth-and-monitor-synthetically","title":"Go Forth and Monitor Synthetically","text":"<p>I hope this post has demonstrated how easy it is to integrate Dynatrace Synthetic monitoring into your automated testing pipelines so now there really is no excuse for \u201cwell it worked in Dev\u201d, is there!?</p>"},{"location":"web-performance-101-caching-dynamic-content/","title":"Web Performance 101 - Caching Dynamic Content","text":"<p>What is caching, how does it relate to (and differ from) static content and how can you use it to increase the performance of your websites?</p> <p>As you\u2019d expect, most web performance techniques are complementary to one another. Functionality and terms tend to be blended / used interchangeably a lot of the time. As one example, you can cache static content on a CDN edge server (aka edge node). This will require different technologies, technical know-how and skillsets \u2013 but they all lean towards the same goal; delivering content back to the user as quickly as possible.</p> <p>Most web performance techniques and technologies are complementary\u2026 All aim to deliver content back to the user as quickly as possible.</p> <p>Caching takes many forms (caching to disk, browser caching, in memory caching, GPU caching etc.) This article will focus on a specific usecase: caching dynamic content for performance.</p>"},{"location":"web-performance-101-caching-dynamic-content/#the-problem-with-dynamic-content","title":"The \"Problem\" With Dynamic Content","text":"<p>Why are we looking to cache dynamic content at all? As above, it\u2019s all about the performance your end user experiences.</p> <p>Imagine the user requests some content. Instinctively, which of these do you think will perform better:</p> <p></p> <p>If the request has to be passed (and processed) all the way through your infrastructure to the database and back again \u2013 it\u2019s likely to be much slower than simply serving a copy of the data from the first \u201chop\u201d in the chain.</p>"},{"location":"web-performance-101-caching-dynamic-content/#caching-dynamic-content","title":"Caching Dynamic Content","text":"<p>Imagine you have a service which is dynamic in nature. That is, you\u2019re pulling data from a source which is likely to change regularly for most users. Some examples:</p> <ul> <li>A list of users</li> <li>A product list</li> <li>A \"top 10\" list</li> <li>A list of news articles</li> </ul> <p>Notice I\u2019ve emphasised regularly and most users. This is deliberate and it all depends on your definition of those terms.</p> <p>If your product list changes once a week, you could consider your product list as static content thereby serving the same static list for a week, changing it once per week. This would reduce the load on your database / backend systems and thus ultimately, save money.</p> <p>Your product list changes once per week. Cache it, serve it as static content and save load (and money) on infrastructure costs.</p>"},{"location":"web-performance-101-caching-dynamic-content/#search-results-truly-dynamic","title":"Search Results - Truly Dynamic?","text":"<p>Imagine you are responsible for a search engine. A company internal engine or a worldwide engine such as Google. It doesn\u2019t matter.</p> <p>Is Google truly dynamic? Would you want it to be?</p> <ul> <li>How dynamic do you want your search engine to be?</li> <li>Imagine a user repeatedly searches for \"hammer\". Would they really want a different top result every time or would they want some consistency between searches?</li> <li>How \"fresh\" should the search results be?</li> <li>Do you cache results per user or across the userbase?</li> <li>As soon as one user searches for \"hammer\", does this user see the same top results repeatedly or do all users see this set of results?</li> </ul>"},{"location":"web-performance-101-caching-dynamic-content/#caching-third-party-calls","title":"Caching Third Party Calls","text":"<p>Caching third party calls can save money, improve performance and reliability.</p> <p>I recently wrote some software that took input from a user which included a UK postcode. The software performed a geo-lookup of the <code>latitude</code> and <code>longitude</code> (using a third party service) then plotted the location on map.</p> <p>I therefore had two choices when designing this application:</p> <ol> <li>Store the postcode in the database. Perform the geo-lookup for all postcodes every time the user viewed the map.</li> <li>Do the geo-lookup once and store the latitude and longitude in the database. Thus to render the \"map view\", the software doesn\u2019t need the third party again.</li> </ol> <p>I decided on option 2 for three reasons:</p> <ul> <li>Geographic locations of postcodes are unlikely to change often. To all intents and purposes, they\u2019re static.</li> <li>Each geo-lookup costs money. Fewer calls to the 3rd Party = Less money spent.</li> <li>Each geo-lookup makes us reliant on a third party. Fewer calls = Less reliance on sites we don\u2019t control.</li> </ul> <p>Eagle eyed viewers will recognise that we could take this a step further: Since the <code>lat / long</code> data is static, we could simply store this as a static file and thus avoid the database call.</p>"},{"location":"web-performance-101-caching-dynamic-content/#the-downside-of-caching","title":"The Downside of Caching","text":"<p>As we\u2019ve seen, caching is basically saving a static copy of some data and returning that copy to the user, rather than the \"real\" (underlying) data.</p> <p>One big issue with caching is when the cached copy becomes \"out of date\".</p>"},{"location":"web-performance-101-caching-dynamic-content/#scenario-ecommerce-prices","title":"Scenario - Ecommerce Prices","text":"<p>A company releases new products and prices once per week. They\u2019ve decided to cache their product pages each week. This is working perfectly... Until a staff member realises they\u2019ve misplaced a decimal point so their product is $10.00 and not the $100.00 it should cost.</p> <p>Does the company simply wait a week and live with the lost revenue?</p>"},{"location":"web-performance-101-caching-dynamic-content/#scenarios-misworded-blog","title":"Scenarios - Misworded Blog","text":"<p>A blog has decided to cache their posts. Unfortunately, an author has misspelled a word and thus introduced a swear word into a blog post. The blog post cache updates once a month. You can see the potential for embarrassment here...</p>"},{"location":"web-performance-101-caching-dynamic-content/#conclusion","title":"Conclusion","text":"<p>As with most things in life, there is no singular answer on how / when or why to use caching. It all depends on your application, use-case and (in a lot of cases) organisational policies.</p> <p>Notice that I also haven\u2019t discussed the technical mechanisms (or how we cache the data) at all in this post. Again, it depends on your specific use-case.</p> <p>There\u2019s always a different way to do things. Is it worth it? Will the benefits to my users outweigh the cost and effort of making the change?</p> <p>As with all web performance topics, there\u2019s almost always a different way to do things. The question is: Is it worth it? Will it really bring benefits to my users and do those benefits outweigh the effort of making the change in the first place.</p>"},{"location":"web-performance-101-image-loading-sync-vs-async-vs-lazy-loading/","title":"Web Performance 101 - Image Loading (Sync vs. Async vs. Lazy Loading)","text":"<p>Let\u2019s examine the difference to your users (and your server bill) between various image loading strategies. We\u2019ll compare \u201cstandard\u201d synchronous loading vs. asynchronous vs. lazy loading with the new <code>IntersectionObserver</code> API.</p> <p>The modern web is heavily dependent on images. I don\u2019t think that\u2019s news to anyone. As such it\u2019s worth considering your strategy when loading heavy content such as images or video.</p> <p>Sending content that is too heavy (in terms of MB) is bad for many reasons, here are 4:</p> <ul> <li>Slow down your website performance.</li> <li>Annoy your users \u2013 your site is slow and it\u2019s eating all their bandwidth.</li> <li>Waste your money \u2013 you\u2019re spending money on unnecessary infrastructure / third parties just to deliver and support your massive files.</li> <li>Destroy your search engine rankings.</li> </ul> <p>As with most things, there\u2019s a balance to be struck. Usually this is a tripartite discussion between marketing (who want the highest possible content quality), web developers (who want the lowest allowable quality to improve page load speed) and business (who want to make money &amp; don\u2019t necessarily care how that\u2019s done).</p> <p>How do we strike an acceptable compromise between these three competing interests and ensure that you\u2019re getting the best balance? Here are a few steps which I\u2019d attack in this order:</p>"},{"location":"web-performance-101-image-loading-sync-vs-async-vs-lazy-loading/#resize-your-images","title":"Resize Your Images","text":"<p>Modern digital cameras will produce images thousands of pixels square. 99% of the time these are inappropriate for directly uploading to a website. Your screen is a thousand pixels square at most. If you\u2019re know the physical pixel size of an image (say 250x250px for an icon) \u2013 why bother loading a 2500x2500px image?</p> <p>You\u2019re sending (and paying for) 10x the amount of data to be sent to the user. The user\u2019s browser then has to spend time resizing it down to 250\u00d7250 anyway. A total waste of time.</p> <p>Reduce the physical size of your images wherever appropriate. Especially for mobile-focused websites.</p>"},{"location":"web-performance-101-image-loading-sync-vs-async-vs-lazy-loading/#optimise-your-images","title":"Optimise Your Images","text":"<p>There are various online websites offering image optimisation these days. One I particularly like is TinyPNG.</p> <p>Using this image from Adrien Ledoux, TinyPNG shrunk the image from <code>1.2MB</code> to <code>367KB</code> (a 70% reduction) and I can\u2019t see any difference in quality.</p> <p>Sites like TinyPNG can shrink images by as much as 90% with no discernible loss of quality.</p>"},{"location":"web-performance-101-image-loading-sync-vs-async-vs-lazy-loading/#load-your-images-appropriately","title":"Load Your Images Appropriately","text":"<p>So, by now your images are an appropriate physical (pixel) size and you\u2019ve optimised them to get the best balance of quality and size.</p> <p>How you deliver images to your end users is critical. Let\u2019s investigate 3 possibilities...</p> <p>How you deliver images to your end users is a critical factor. It\u2019ll save you money, improve your search engine rankings and make for happier customers.</p>"},{"location":"web-performance-101-image-loading-sync-vs-async-vs-lazy-loading/#synchronous-loading","title":"Synchronous Loading","text":"<p>This is the standard, default solution. Just include an <code>&lt;img&gt;</code> tag on your page. The page will load top-down, in order, which means large images will block the rest of the content (text) until they\u2019re finished.</p> <p>I created a large (13.6MB) dummy page, loaded it &amp; recorded the timings from Chrome DevTools:</p> Connections Size Finish DOMContentLoaded Load 7 13.6MB 4.18s 356ms 4.01s <p>We used almost 14MB of data to load this page and it took over 4s.</p>"},{"location":"web-performance-101-image-loading-sync-vs-async-vs-lazy-loading/#asynchronous-loading","title":"Asynchronous Loading","text":"<p>We load a default \"loading icon\" for each image. Then, in separate threads (without blocking the loading of the page text), we load the images.</p> <p>This improves loading time of the content (arguably the most important part) and delays your \"lower value\" assets (images).</p> <p>However, you still load all of the images, whether or not the user actually ever sees them.</p> Connections Size Finish DOMContentLoaded Load 8 13.6MB 4.31s 267ms 443ms <p>We\u2019ve improved Load time by 89% but still loaded 14MB of data.</p>"},{"location":"web-performance-101-image-loading-sync-vs-async-vs-lazy-loading/#lazy-loading","title":"Lazy Loading","text":"<p>This solution uses the new <code>IntersectionObserver</code> API which intelligently \"knows\" what the user can see (ie. what content is currently within the visible browser window).</p> <p>Only when the images are actually visible are they loaded. This saves a huge amount of bandwidth for you and your end users.</p> Connections Size Finish DOMContentLoaded Load 5 3.9MB 1.52s 229ms 401ms <p>Huge improvement here for page size. We\u2019ve saved almost 10MB of data.</p>"},{"location":"web-performance-101-image-loading-sync-vs-async-vs-lazy-loading/#conclusion","title":"Conclusion","text":"<p>There is no single \"best practice\" - each website is unique and will most likely demand a mixture of these techniques.</p> <p>The key here is that there are a range of techniques to choose from and you owe it to yourself (and your users) to investigate each one and use the most appropriate for you.</p>"},{"location":"web-performance-101-location/","title":"Web Performance 101 - Location","text":"<p>How much does server location influence the performance and response time of your website?</p> <p>A lot...</p> <p>Performance professionals have a habit of diving in at the deep-end of a problem. Immediately talking about JVM heap sizes and concurrent thread counts. This is often perfectly valid. However, it really does pay not to forget the basics.</p>"},{"location":"web-performance-101-location/#location-location-location","title":"Location, Location, Location","text":"<p>As they say, location is everything. In the physical world and the digital. It\u2019s logical if you think about it.</p> <p>If I asked you to go walk to the shop and back, it\u2019d take you longer if the shop was far away vs. if it was just around the corner. That\u2019s logical since we can only move at a certain speed.</p> <p>Since everything in the universe (as far as we know) abides by the laws of physics, websites (more accurately the bits and bytes that make them) are similarly constrained.</p> <p>It\u2019ll take longer to serve your site from somewhere geographically remote to your users than somewhere geographically near.</p> <p>It all comes down to where your users are located in relation to your server. The closer these two things are, the faster the bits and bytes of your site will get to them.</p>"},{"location":"web-performance-101-location/#prove-it","title":"Prove It","text":"<p>I\u2019ve created 3 indentical WordPress droplets using DigitalOcean.</p> <ul> <li>The user (client) is in London, UK.</li> <li>Droplet #1 is hosted London, UK.</li> <li>Droplet #2 is hosted in New York, USA.</li> <li>Droplet #3 is hosted in San Francisco, USA.</li> <li>All 3 are hosting the stock WordPress theme image with nothing added on a $5 droplet.</li> <li>I\u2019ll be measuring the performance with Chrome DevTools, ensuring I\u2019ve got Disable cache clicked.</li> <li>Each page load is executed 10 times per location.</li> </ul>"},{"location":"web-performance-101-location/#raw-london-values","title":"Raw London Values","text":"<p>All values in milliseconds.</p> Finish DOMContentLoaded Load 564 537 644 540 528 603 578 549 686 609 591 690 631 615 706 575 567 644 585 570 662 584 557 662 641 594 704 573 548 653"},{"location":"web-performance-101-location/#london-summary","title":"London Summary","text":"Finish DOMContentLoaded Load Average 588ms 565.6ms 665.4ms Standard Deviation 30.73 27.50 31.89"},{"location":"web-performance-101-location/#raw-new-york-values","title":"Raw New York Values","text":"<p>All values in milliseconds. </p> Finish DOMContentLoaded Load 756 700 785 1008 1000 1200 927 796 929 1008 951 1008 952 822 957 952 846 954 981 852 983 1100 1009 1160 947 860 949 997 799 999"},{"location":"web-performance-101-location/#new-york-summary","title":"New York Summary","text":"Finish DOMContentLoaded Load Average 962.8ms 863.5ms 992.4ms Standard Deviation 87.52 97.23 117"},{"location":"web-performance-101-location/#raw-san-francisco-values","title":"Raw San Francisco Values","text":"<p>All values in milliseconds.</p> Finish DOMContentLoaded Load 1570 1140 1570 1520 1100 1520 1510 1007 1510 1500 1005 1500 1580 1130 1580 1470 1130 1480 1510 1140 1510 1530 1120 1530 1540 1160 1540 1480 1100 1480"},{"location":"web-performance-101-location/#san-francisco-summary","title":"San Francisco Summary","text":"<p>| | Finish | DOMContentLoaded | Load | | Average | 1521ms | 1103ms | 1522ms | Standard Deviation | 35.42 | 54.34 | 33.93 |</p> <p>For a London-based user, serving exactly the same site from San Francisco takes nearly 159% longer than London.</p> <p>The consistency of response time drops due to the unreliability of the internet \u2013 you\u2019re more likely to get a bad response time, more often.</p>"},{"location":"web-performance-101-static-content/","title":"Web Performance 101 - Static Content","text":"<p>You may have heard of static content, but what difference does it really make to web performance? Let\u2019s find out...</p>"},{"location":"web-performance-101-static-content/#what-is-static-content","title":"What Is Static Content?","text":"<p>Simply put, it is content (words, text or more accurately HTML content) that doesn\u2019t change.</p> <p>In reality, this means that all users (or most users) will receive that same content each time they load the page. Think about blog posts, news articles or tutorials \u2013 things that will rarely change once they\u2019ve been published. It doesn\u2019t mean it never changes \u2013 you can always edit the static pages and upload them again.</p> <p>The opposite of static content is dynamic content. That is, content that changes each time the page is reloaded. Think about search result pages or online catalogues. There is another mechanism which can handle dynamic pages \u2013 caching\u2026 More about this in a future post\u2026</p> <p>So, what difference will it make to serve exactly the same page via a static file vs. the standard dynamic method?</p>"},{"location":"web-performance-101-static-content/#test-it","title":"Test It","text":"<p>This site is hosted on WordPress with an Apache webserver and a MySQL database. Every time a user requests a page, the Apache server needs to pull the data from the database, via a layer of PHP. That\u2019s not to mention the imagery and any third party scripts. Each layer adds time to serve this request.</p> <p>To test dynamic vs. static content, we\u2019ll:</p> <ul> <li>Use an identical webpage</li> <li>Hosted on an identical server</li> <li>Served via the same domain name</li> <li>We will not alter the images, where or how they\u2019re served.</li> <li>We will not alter / remove any third party tags or how they\u2019re served.</li> </ul>"},{"location":"web-performance-101-static-content/#dynamic-content","title":"Dynamic Content","text":"<p>I\u2019ll use this post which is a fairly long, image heavy post.</p>"},{"location":"web-performance-101-static-content/#static-content","title":"Static Content","text":"<p>I\u2019ll save the source code of the same page as a static <code>.html</code> page.</p>"},{"location":"web-performance-101-static-content/#raw-values-for-dynamic-page","title":"Raw Values for Dynamic Page","text":"<p>All values in milliseconds.</p> Finish DOMContentLoaded Load 1230 1003 1230 1270 1003 1280 1330 1006 1330 1190 984 1190 1350 1120 1350 1240 992 1240 1190 912 1200 1170 991 1180 1270 1001 1270 1280 1006 1280"},{"location":"web-performance-101-static-content/#raw-values-for-static-page","title":"Raw Values for Static Page","text":"<p>All values in milliseconds.</p> Finish DOMContentLoaded Load 915 721 916 936 730 937 888 694 889 933 724 934 892 681 894 874 686 875 950 759 952 866 668 870 936 697 942 899 697 900"},{"location":"web-performance-101-static-content/#dynamic-summary","title":"Dynamic Summary","text":"Finish DOMContentLoaded Load Average 1252ms 1002ms 1255ms Standard Deviation 59.78 50.11 57.59"},{"location":"web-performance-101-static-content/#static-summary","title":"Static Summary","text":"Finish DOMContentLoaded Load Average 909ms 706ms 911ms Standard Deviation 29.12 27.30 29.33 <p>By every metric, the static page is significantly faster. Static page response time is 27% faster than the dynamic page.Static page = much lower standard deviation. It\u2019s faster, more often and more reliably.</p> <p>As an added bonus, going \"static\" can open you up to possibility of serving sites via serverless infrastructure such as AWS S3.</p> <p>Not only is it better for the environment (you\u2019re using less electricity and resources), its better for your productivity (#NoOps anyone) but it\u2019s better for your bottom-line too! You only pay for what you use.</p>"},{"location":"web-scraper-jq-pup/","title":"Build a Web Scraper with jq and pup","text":"<p>Recently I wanted to build a rudimentary webpage scraper which would run from the command line.</p> <p>As I progressed I noticed myself having to extensively <code>grep</code>, <code>cut</code> and stitch bits and pieces of logic together.</p> <p>One of my rules is that if the activity you're attempting seems fairly standard, someone has probably already done it or at least made utilities to make the activity easier. They've probably put more time and effort into it than you have &amp; thus it's better than your effort would be. Don't reinvent the wheel.</p> <p>This led me to two linux libraries: <code>pup</code> and <code>jq</code>.</p> <p><code>pup</code> is a handy HTML parser for the command line and <code>jq</code> is the equivalent JSON parser.</p> <p>By combining these two utilities, it was very easy to achieve my goal without lines and lines of convoluted code.</p>"},{"location":"web-scraper-jq-pup/#scenario","title":"Scenario","text":"<p>Say I want to extract some information from this page: <code>https://agardner.net/serverless-voting/</code>. Take a look at the source code of that page and you'll see standard HTML text like this:</p> <pre><code>&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;head&gt;\n    &lt;!-- Begin Jekyll SEO tag v2.7.1 --&gt;\n    &lt;title&gt;Serverless, Zero Database Voting System | Adam Gardner&lt;/title&gt;\n    &lt;meta name=\"generator\" content=\"Jekyll v3.9.0\"/&gt;\n    &lt;meta property=\"og:title\" content=\"Serverless, Zero Database Voting System\"/&gt;\n    &lt;meta name=\"author\" content=\"Adam Gardner\"/&gt;\n</code></pre> <p>You'll also see <code>&lt;script&gt;</code> tags, one of which looks like this: <pre><code>&lt;script type=\"application/ld+json\"&gt;\n    {\n      \"@type\":\"BlogPosting\",\n      \"@context\":\"https://schema.org\"\n      \"headline\":\"Serverless, Zero Database Voting System\",\n      \"dateModified\":\"2020-06-14T00:00:00+00:00\",\n      \"datePublished\":\"2020-06-14T00:00:00+00:00\",\n      \"mainEntityOfPage\":{\n        \"@type\":\"WebPage\",\n        \"@id\":\"https://agardner.net/serverless-voting/\"\n      },\n      \"author\":{\n        \"@type\":\"Person\",\n        \"name\":\"Adam Gardner\"\n      },\n      \"url\":\"https://agardner.net/serverless-voting/\",\n      \"description\":\"I needed a voting system for this website which was compatible with serverless pages. I also wanted it to be zero-login which ruled out using a third-party plugin. The result was a serverless, zero database &amp;amp; zero login voting system using AWS. Here is how\u2026\"\n    }\n&lt;/script&gt;\n</code></pre></p> <p>In this scenario, I want to extract the content of the <code>&lt;title&gt;</code> tag and then extract the content of the <code>datePublished</code> field from this JSON snippet.</p>"},{"location":"web-scraper-jq-pup/#install-both-libraries","title":"Install both libraries","text":"<pre><code>pip install jq\n</code></pre> <p>Depending on your platform, <code>pup</code> can be installed in different ways. Easiest is to use <code>go get</code> or <code>brew</code>.</p> <pre><code>go get github.com/ericchiang/pup\nOR\nbrew install pup\n</code></pre> <p>Alternatively, see instructions on the releases page.</p> <p>If all is successful, these two commands should provide output:</p> <pre><code>jq --version\npup --version\n</code></pre>"},{"location":"web-scraper-jq-pup/#extracting-title","title":"Extracting Title","text":"<p>Remember that I want to do this via a command line script, so create a new file in <code>/tmp</code> called <code>scraper.sh</code>. </p> <p>Make it executable: <pre><code>chmod +x /tmp/scraper.sh\n</code></pre></p> <p>Paste the following content:</p> <pre><code>#!/bin/bash\ncurl $URL\n</code></pre> <p>Now set the URL value and call the scraper.sh:</p> <pre><code>URL=https://agardner.net/serverless-voting/ /tmp/scraper.sh\n</code></pre> <p>Notice that it prints the entire HTML content and we only want the <code>&lt;title&gt;</code> tag so modify <code>scraper.sh</code> to look like this:</p> <pre><code>#!/bin/bash\ntitle=$(curl $URL | pup 'title')\necho $title\n</code></pre> <p>Here we're piping the output of curl to the <code>pup</code> command and asking <code>pup</code> to print only the <code>&lt;title&gt;</code> tag.</p> <p>The output should look like this:</p> <pre><code>% URL=https://agardner.net/serverless-voting/ /tmp/scraper.sh\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100 24491  100 24491    0     0   254k      0 --:--:-- --:--:-- --:--:--  254k\n&lt;title&gt; Serverless, Zero Database Voting System | Adam Gardner &lt;/title&gt;\n</code></pre> <p>Let's clean that up a bit. Modify the <code>/tmp/scraper.sh</code> file as such that we add the silent (<code>-s</code>) flag to the curl command. Adding this flag means we don't see the download stats bar.</p> <p>Then add the <code>text{}</code> modifier to the <code>pup</code> command. This tells <code>pup</code> that we only want to see the text of the tag, not the actual start and end tags themselves.</p> <p>Your script should now look like this:</p> <pre><code>#!/bin/bash\ntitle=$(curl -s $URL | pup 'title' text{})\necho $title\n</code></pre> <p>Re-run and you should see this output:</p> <pre><code>% URL=https://agardner.net/serverless-voting/ /tmp/scraper.sh\nServerless, Zero Database Voting System | Adam Gardner\n</code></pre>"},{"location":"web-scraper-jq-pup/#extract-date-published","title":"Extract Date Published","text":"<p>Recall that there's a block of JSON in the source code which contains the information we need:</p> <pre><code>&lt;script type=\"application/ld+json\"&gt;\n    {\n      \"@type\":\"BlogPosting\",\n      \"@context\":\"https://schema.org\"\n      \"headline\":\"Serverless, Zero Database Voting System\",\n      \"dateModified\":\"2020-06-14T00:00:00+00:00\",\n      \"datePublished\":\"2020-06-14T00:00:00+00:00\",\n      \"mainEntityOfPage\":{\n        \"@type\":\"WebPage\",\n        \"@id\":\"https://agardner.net/serverless-voting/\"\n      },\n      \"author\":{\n        \"@type\":\"Person\",\n        \"name\":\"Adam Gardner\"\n      },\n      \"url\":\"https://agardner.net/serverless-voting/\",\n      \"description\":\"I needed a voting system for this website which was compatible with serverless pages. I also wanted it to be zero-login which ruled out using a third-party plugin. The result was a serverless, zero database &amp;amp; zero login voting system using AWS. Here is how\u2026\"\n    }\n&lt;/script&gt;\n</code></pre> <p>From this JavaScript snippet we need to extract the <code>datePublished</code> field:</p> <pre><code>\"2020-06-14T00:00:00+00:00\"\n</code></pre> <p>Adjust your <code>scraper.sh</code> file to look like this:</p> <pre><code>#!/bin/bash\npage_html=$(curl -s $URL)\ntitle=$(echo $page_html | pup 'title' text{})\necho $title\n</code></pre> <p>All we've done here is store the output of the <code>curl</code> command into a variable called <code>page_html</code> so we can manipulate and query the HTML without repeated <code>curl</code> calls to the website.</p> <p>Use <code>pup</code> to extract the <code>&lt;script type=\"application/ld+json\"&gt;</code> tag. Modify your <code>scraper.sh</code> again:</p> <pre><code>#!/bin/bash\npage_html=$(curl -s $URL)\ntitle=$(echo $page_html | pup 'title' text{})\ndate_published=$(echo $page_html | pup 'script[type=\"application/ld+json\"] text{}')\necho $title\necho $date_published\n</code></pre> <p>This sort of works but it outputs the title then entire JSON object which isn't quite what we want:</p> <pre><code>% URL=https://agardner.net/serverless-voting/ /tmp/scraper.sh\nServerless, Zero Database Voting System | Adam Gardner\n{\"@type\":\"BlogPosting\",\"headline\":\"Serverless, Zero Database Voting System\",\"dateModified\":\"2020-06-14T00:00:00+00:00\",\"datePublished\":\"2020-06-14T00:00:00+00:00\",\"mainEntityOfPage\":{\"@type\":\"WebPage\",\"@id\":\"https://agardner.net/serverless-voting/\"},\"author\":{\"@type\":\"Person\",\"name\":\"Adam Gardner\"},\"url\":\"https://agardner.net/serverless-voting/\",\"description\":\"I needed a voting system for this website which was compatible with serverless pages. I also wanted it to be zero-login which ruled out using a third-party plugin. The result was a serverless, zero database &amp;amp; zero login voting system using AWS. Here is how\u2026\",\"@context\":\"https://schema.org\"}\n</code></pre> <p>We need to take this JSON output and pass it to <code>jq</code>. JQ is a JSON parser. It works like so:</p> <pre><code>some input text | jq 'some_desired_output'\n</code></pre> <p>The most basic would be to just ask for the entire input document back as output. The <code>.</code> character is the shorthand for this. Run the following in a terminal window</p> <pre><code>echo '{\"foo\": \"bar\"}' | jq '.'\n</code></pre> <p>You should see a pretty printed JSON object as output:</p> <pre><code>%  echo '{\"foo\": \"bar\"}' | jq '.'\n{\n  \"foo\": \"bar\"\n}\n</code></pre> <p>After the <code>.</code> you can request any JSON field. So to get <code>\"bar\"</code> as the output, do this:</p> <pre><code>echo '{\"foo\": \"bar\"}' | jq '.foo'\n</code></pre> <p>You'll see:</p> <pre><code>% echo '{\"foo\": \"bar\"}' | jq '.foo'\n\"bar\"\n</code></pre> <p>We can use this concept in our script so after we've used <code>pup</code> to retrieve the <code>&lt;script&gt;</code> tag, we will use <code>jq</code> to retrieve only the <code>datePublished</code> JSON field.</p> <p>Modify <code>scraper.sh</code> as follows:</p> <pre><code>#!/bin/bash\npage_html=$(curl -s $URL)\ntitle=$(echo $page_html | pup 'title' text{})\ndate_published=$(echo $page_html | pup 'script[type=\"application/ld+json\"] text{}' | jq '.datePublished')\necho $title\necho $date_published\n</code></pre> <p>So we're echoing the <code>page_html</code> content then using <code>pup</code> to extract just the <code>&lt;script type=\"application/ld+json\"&gt;</code> block. Then we're passing that extracted value to <code>jq</code> and asking for the <code>datePublished</code> value. Which gives us:</p> <pre><code>RL=https://agardner.net/serverless-voting/ /tmp/scraper.sh\nServerless, Zero Database Voting System | Adam Gardner\n\"2020-06-14T00:00:00+00:00\"\n</code></pre> <p>One final (optional) cleanup step would be to remove the quotation marks. That's as easy as adding the <code>-r</code> flag to <code>jq</code>. As <code>jq --help</code> suggests:</p> <pre><code>-r    output raw strings, not JSON texts;\n</code></pre> <p>So:</p> <pre><code>#!/bin/bash\npage_html=$(curl -s $URL)\ntitle=$(echo $page_html | pup 'title' text{})\ndate_published=$(echo $page_html | pup 'script[type=\"application/ld+json\"] text{}' | jq -r '.datePublished')\necho $title\necho $date_published\n</code></pre> <p>Prints this:</p> <pre><code>URL=https://agardner.net/serverless-voting/ /tmp/scraper.sh\nServerless, Zero Database Voting System | Adam Gardner\n2020-06-14T00:00:00+00:00\n</code></pre>"},{"location":"which-opentelemetry-collector-distro/","title":"Everyone has an OpenTelemetry Distribution: Which one should I choose?","text":"<p>A lot of OpenTelemetry collector distributions exist. Many from the OpenTelemetry project, every vendor seemingly has their own and you can build YOUR own? So which should you choose?</p> <p>In this video I discuss the options available to you, what you should consider when choosing and perhaps when to switch. My aim is to help you make a more informed decision.</p>"},{"location":"why-blog-serverless/","title":"Why This Blog Went Serverless","text":"<p>I've recently moved this blog from Wordpress hosted on a DigitalOcean virtual machine to GitHub Pages, a \"serverless\" hosting platform. In this post, I aim to explain my reasoning.</p> <p>I've shared some tangible result in this blog post.</p> <p>First though, some facts that are pertinent to consider when reading this post:</p> <ul> <li>This is a personal site. It's not mission critical. It's not the end of the world if it's unavailable.</li> <li>I don't process transactions or capture any financial user information on this site.</li> <li>For practical purposes, most of the content can be considered static. It may change from time to time, but 99% of the content remains the same once published. There's very little need for a database as most content can be served perfectly well via static files.</li> <li>Running this site is not my day job. I need it to \"just work\".</li> </ul> <p>\"Serverless\" is one tool in your toolbox, not a panacea.</p>"},{"location":"why-blog-serverless/#considerations-choices","title":"Considerations &amp; Choices","text":"<p>Like any major architectural shift, the benefits and costs need to be carefully weighed. Although this site is small by most standards, there was still a fair amount to consider:</p>"},{"location":"why-blog-serverless/#lift-shift-vs-rewrite","title":"Lift &amp; Shift vs. Rewrite","text":"<p>GitHub Pages is powered by Jekyll. If it's good enough for GitHub, it's good enough for me. Jekyll is written in markdown &amp; Liquid templating format, so some conversion will be necessary from Wordpress to Jekyll.</p> <p>Jekyll offers an importer for most common blog / CMS platforms. You give the importer your database connection info, the importer does all the hardwork by translating the Wordpress into Jekyll ready content.</p>"},{"location":"why-blog-serverless/#choice-rewrite","title":"Choice: Rewrite","text":"<p>I chose not to utilise the import feature. Instead I took the opportunity to use a new blog template - one specifically designed for Jekyll.</p> <p>I also reworked all of the posts into Jekyll-native markdown format.</p> <p>Reasoning: A template specifically designed for Jekyll is more likely to be more compatible and lightweight. Markdown format is easy to read, store and is transferable, so if I decide to move away from GitHub Pages, all I need to do is copy my plain text markdown formatted files. No more database exports / imports.</p>"},{"location":"why-blog-serverless/#backups-maintenance","title":"Backups &amp; Maintenance","text":"<p>Using a \"serverless\" platform, there is no infrastructure for me to manage. Of course, there is a server somewhere, but that is GitHub's problem, not mine.</p> <p>This means I can focus on writing content &amp; not worry about updating infrastructure. In fact, I forgot to upgrade a Wordpress instance I use on a demo domain - it was promptly hacked &amp; used for essay writing posts.</p> <p>GitHub Pages relies on GitHub. In other words, it is a fully functional version control system, which also functions as a handy backup tool. I can version control and rollback to any previous version of my blog post, all hosted &amp; managed by GitHub.</p>"},{"location":"why-blog-serverless/#cost","title":"Cost","text":"<p>Being \"serverless\", means no servers to pay for. The GitHub Pages service is free. Other \"serverless\" hosting solutions such as AWS S3 are free or extremely low cost too.</p> <p>Result: My hosting costs just reduced to almost zero (aside from domain name renewal).</p>"},{"location":"why-blog-serverless/#scale","title":"Scale","text":"<p>Serverless solutions will scale with your traffic. That is all handled and managed by the provider. No more worrying about load balancers, number of VMs, scaling etc. All I do is provide my content and they figure out the rest.</p>"},{"location":"why-blog-serverless/#environmental-impact","title":"Environmental Impact","text":"<p>This one is harder to measure, but I like the idea that by reducing my VM usage, in a tiny way, I am reducing my environmental impact. This cannot be a bad thing. </p> <p>Imagine the amount of severely underutilised infrastructure components running in datacentres today. The environmental impact of a single VM is tiny, but multiply that many millions of times - it all adds up. As one example, processing a single Bitcoin transaction contributes the same CO2 as 750,000 VISA transactions!</p> <p>1 Bitcoin transaction equates to the same CO2 consumption as 750,000 VISA transactions.</p>"},{"location":"why-blog-serverless/#issues-concerns","title":"Issues / Concerns","text":"<p>Serverless delivery systems offer one significant downside: There is no server when means no way to run server side code or logic.</p> <p>Case in point: The contact form.</p>"},{"location":"why-blog-serverless/#migrating-the-contact-form","title":"Migrating the Contact Form","text":"<p>In the old Wordpress virtual machine setup, I had an HTML form which sent data to the webserver on the VM. PHP running on the VM processed this information and sent me an email with the contact form details. All very simple and standard.</p> <p>In the new world, I have no native capability to run any server side code. I can create the contact form, but that's all.</p>"},{"location":"why-blog-serverless/#possible-solutions","title":"Possible Solutions","text":"<p>There are two potential solutions that spring immediately to mind:</p> <p>1) Create a VM for contact form processing. 2) Use serverless functions to process the form.</p>"},{"location":"why-blog-serverless/#1-is-pointless-there-is-zero-value-in-creating-a-vm-only-to-process-the-contact-form-if-i-do-that-i-might-as-well-host-the-whole-site-on-that-vm-and-move-back-to-wordpress","title":"1 is pointless. There is zero value in creating a VM only to process the contact form. If I do that, I might as well host the whole site on that VM and move back to Wordpress.","text":""},{"location":"why-blog-serverless/#2-looks-promising-as-serverless-functions-only-run-when-theyre-actually-in-use-saves-money-and-the-environment","title":"2 looks promising as serverless functions only run when they're actually in use. Saves money and the environment.","text":"<p>I'll write a seperate post on how I redesigned the contact form to be serverless, but it really wasn't that difficult (once I got over the AWS learning curve).</p>"},{"location":"why-blog-serverless/#lessons-learned","title":"Lessons Learned","text":""},{"location":"why-blog-serverless/#consider-your-scenario","title":"Consider Your Scenario","text":"<p>As I said at the top of the article, serverless is not actually serverless and serverless is not a panacea. Serverless will not be the right fit for everyone.</p> <p>Take the time to fully evaluate your situation, and the effort involved in a move. Then you'll be in a position to make an informed decision.</p>"},{"location":"why-blog-serverless/#new-technologies","title":"New Technologies","text":"<p>Don't be afraid of new technologies. Generally speaking, newer technologies and methodologies offer a better or more efficient way of doing things. Don't be closed-minded. Don't ignore a potentially excellent solution, just because it is scary and takes a little bit of time to get your head around.</p> <p>Embrace new technology &amp; methodologies. There are good reasons why we're not all still flying in the Wright Brothers planes.</p>"},{"location":"why-blog-serverless/#take-time-practice","title":"Take Time &amp; Practice","text":"<p>Like everything in life, it takes time to figure things out. That is fine. Take your time, experiment, break things, take time to understand how and why things are working the way they are.</p> <p>The time spent practicing and understanding new tools will pay dividends later.</p>"},{"location":"why-blog-serverless/#benefits-are-worth-it","title":"Benefits are Worth It","text":"<p>When all is said and done, in my case, the benefits of the move have been worth it:</p> <ul> <li>The site structure is much more lightweight, cleaner and easier to maintain.</li> <li>I have a ready made backup &amp; restore solution via GitHub.</li> <li>I have slashed my response times (see: implications of going serverless blog post).</li> <li>I have vastly reduced my risk of being hacked or attacked - all infrastructure and DDOS protection is managed by GitHub. All blog posts are open source - there is really nothing to hack.</li> <li>I have embraced new tech so that I now get instant push notifications on my phone for contact form submissions.</li> <li>I am not paying for infrastructure.</li> </ul> <p>I hope this post has given you some pointers and thoughts about a potential move to serverless.</p> <p>As always, if you have any comments or suggestions, please do not hesitate to contact me.</p>"},{"location":"wordpress-serverless-architectural-overview/","title":"Wordpress to Serverless - An Architectural Overview","text":"<p>A post describing the architectural changes I had to make when moving from a Wordpress hosted blog to a Github Pages, serverless model.</p>"},{"location":"wordpress-serverless-architectural-overview/#the-old","title":"The Old","text":"<p>At a very high level, this is what my website used to look like. I had a Wordpress instance, running everything on a single VM. Whenever someone submitted the contact form, Wordpress would send me an email.</p> <p></p> <p>Unboxing \"Wordpress\" gives us this view. An apache webserver in front of PHP. PHP being responsible for the server-side processing and ultimately, sending the email outbound.</p> <p>There is also a MySQL database on the box, which stores all the Wordpress content (blog posts) and settings.</p> <p></p>"},{"location":"wordpress-serverless-architectural-overview/#distributed-highly-available-wordpress","title":"Distributed, Highly Available Wordpress","text":"<p>There are, of course, solutions for configuring Wordpress in a distributed and highly available manner. A quick search will give you many, many options. However, as already mentioned, I wanted to simplify the deployment model and wherever possible, remove any self hosted components.</p>"},{"location":"wordpress-serverless-architectural-overview/#the-new","title":"The New","text":"<p>In terms of what I know and control, the site architecture (particularly the contact form) now looks like this.</p> <p></p> <p>I commit code to GitHub. They generate the content (HTML pages). The contact form <code>POSTs</code> the data to AWS Lambda, via the AWS API Gateway. Lambda then processes the form and sends the notifications to one or more \"targets\".</p> <p>These \"notification targets\" can be anything at all really: log files, email, Slack, MS Teams, Trello, the list is endless...</p> <p>If you want to try out the contact form, it is here.</p>"},{"location":"wordpress-serverless-architectural-overview/#advantages-of-the-new-approach","title":"Advantages of the New Approach","text":"<p>Obviously I don't host any of this. GitHub handles the front end and AWS handles the Gateway and Lambda code.</p> <p>Another big advantage is that hosting is free. AWS Lambda provide 1 million requests and 400,000 GB seconds of compute time per month. Given that a contact form submission takes around 1 second, I'd need to exceed 400k contact form request per month to start getting charged. I am nowhere near that!</p>"},{"location":"wordpress-serverless-architectural-overview/#summary","title":"Summary","text":"<p>There was a learning curve to understand how the API Gateway and Lambda fit together (basically, AWS API Gateway is the required \"front door\" for any Lambda functions).</p> <p>There was also a learning curve to write the tiny piece of Python code that powers my Lambda function.</p> <p>That said, I am really happy with the move to a serverless model. Everything is offloaded and I am not responsible for any of the hosting, backups, redundancy, scaling - all topics I have covered in my other post.</p> <p>As per my other post, I am not advocating this for every site and situation, but it should definitely be on your shortlist to at least investigate.</p>"},{"location":"working-with-json-in-ansible/","title":"Working With JSON in Ansible","text":"<p>Reading JSON in Ansible is easy. Here are some examples...</p> <p>This is a JSON array of objects (hosts). Each host has a nested object called customProperties. Assume this is stored in a file called customProperties.json in the same directory as your playbook.</p> <p></p> <pre><code>[{\n  \"hostname\": \"host1\",\n  \"customProperties\": {\n    \"foo\": \"first\",\n    \"foo2\": \"second\"\n  }\n}, {\n   \"hostname\": \"host2\",\n   \"customProperties\": {\n     \"foo\": \"third\",\n     \"foo2\": \"fourth\"\n  }\n}]\n</code></pre>"},{"location":"working-with-json-in-ansible/#read-json-into-ansible","title":"Read JSON Into Ansible","text":"<p>Let\u2019s read this file into Ansible. Once done, it will be available as an Ansible variable called hostList (or whatever you define on line 6 of the playbook).</p> <p>{% raw %} <pre><code>---\n- name: Write vars\n  hosts: local\n\n  vars:\n    hostList: \"{{ lookup('file', 'customProperties.json') }}\"\n</code></pre> {% endraw %}</p>"},{"location":"working-with-json-in-ansible/#output-raw-json","title":"Output Raw JSON","text":"<p>Now extend the playbook to output the RAW JSON object. For this we\u2019ll use the debug module to print out to the screen.</p> <p>{% raw %} <pre><code>---\n- name: Write vars\n  hosts: local\n\n  vars:\n    hostList: \"{{ lookup('file', 'customProperties.json') }}\"\n\n  tasks:\n\n  - name: \"Output all hostList as-is\"\n    debug:\n      msg: \"{{ hostList }}\"\n</code></pre> {% endraw %}</p>"},{"location":"working-with-json-in-ansible/#print-all-host-names","title":"Print All Host Names","text":"<p>Using the Ansible <code>loops</code> keyword, we can iterate through each top level object (host) and print out any property, one at a time.</p> <p>Let\u2019s extend the playbook to loop through and print each <code>hostname</code> value.</p> <p>Notice the use of the <code>item</code> keyword. This is a special Ansible keyword which refers to the current item in the loop.</p> <p>{% raw %} <pre><code>---\n- name: Write vars\n  hosts: local\n\n  vars:\n    hostList: \"{{ lookup('file', 'customProperties.json') }}\"\n\n  tasks:\n\n  - name: \"Output all hostList as-is\"\n    debug:\n      msg: \"{{ hostList }}\"\n\n  - name: \"Print all hostnames\"\n    debug:\n      msg: \"{{ item.hostname }}\"\n    loop: \"{{ hostList }}\"\n</code></pre> {% endraw %}</p>"},{"location":"working-with-json-in-ansible/#print-nested-value","title":"Print Nested Value","text":"<p>Now for something more complex. Let\u2019s track down deeper into the JSON object to print the value of <code>foo2</code> for each host.</p> <p>{% raw %} <pre><code>---\n- name: Write vars\n  hosts: local\n\n  vars:\n    hostList: \"{{ lookup('file', 'customProperties.json') }}\"\n\n  tasks:\n\n  - name: \"Output all hostList as-is\"\n    debug:\n      msg: \"{{ hostList }}\"\n\n  - name: \"Print all hostnames\"\n    debug:\n      msg: \"{{ item.hostname }}\"\n    loop: \"{{ hostList }}\"\n\n  - name: \"Print 'foo2' for each host. Should be 'second' and 'fourth'.\"\n    debug:\n      msg: \"{{ item.customProperties.foo2 }}\"\n    loop: \"{{ hostList }}\"\n</code></pre> {% endraw %}</p>"},{"location":"working-with-json-in-ansible/#print-nested-value-when-condition-is-met","title":"Print Nested Value When Condition Is Met","text":"<p>Finally, let\u2019s choose to only print the <code>foo</code> value when the hostname is <code>host2</code>.</p> <p>{% raw %} <pre><code>---\n- name: Write vars\n  hosts: local\n\n  vars:\n    hostList: \"{{ lookup('file', 'customProperties.json') }}\"\n\n  tasks:\n\n  - name: \"Output all customProperties as-is\"\n    debug:\n      msg: \"{{ hostList }}\"\n\n  - name: \"Print all hostnames\"\n    debug:\n      msg: \"{{ item.hostname }}\"\n    loop: \"{{ hostList }}\"\n\n  - name: \"Print 'foo2' for each host. Should be 'second' and 'fourth'.\"\n    debug:\n      msg: \"{{ item.customProperties.foo2 }}\"\n    loop: \"{{ hostList }}\"\n\n  - name: \"Print 'foo' for host2. Should be 'third'.\"\n    debug:\n      msg: \"{{ item.customProperties.foo }}\"\n    loop: \"{{ hostList }}\"\n    when: \"item.hostname == 'host2'\"\n</code></pre> {% endraw %}</p> <p>The latest version of the playbook and JSON will always be available on Github.</p>"},{"location":"snippets/warning-legacy-dynatrace/","title":"Warning legacy dynatrace","text":"<p>Legacy Product</p> <p>This tutorial refers to the legacy application monitoring product from Dynatrace. Not the Dynatrace platform.</p>"},{"location":"archive/2025/","title":"2025","text":""},{"location":"archive/2024/","title":"2024","text":""},{"location":"archive/2023/","title":"2023","text":""},{"location":"archive/2022/","title":"2022","text":""},{"location":"archive/2021/","title":"2021","text":""},{"location":"archive/2020/","title":"2020","text":""},{"location":"archive/2019/","title":"2019","text":""},{"location":"archive/2018/","title":"2018","text":""},{"location":"archive/2017/","title":"2017","text":""},{"location":"category/opentelemetry/","title":"opentelemetry","text":""},{"location":"category/osquery/","title":"osquery","text":""},{"location":"category/kubernetes/","title":"kubernetes","text":""},{"location":"category/dns/","title":"dns","text":""},{"location":"category/otterize/","title":"otterize","text":""},{"location":"category/tracing/","title":"tracing","text":""},{"location":"category/ai/","title":"ai","text":""},{"location":"category/gemini/","title":"gemini","text":""},{"location":"category/collector/","title":"collector","text":""},{"location":"category/logs/","title":"logs","text":""},{"location":"category/multigitter/","title":"multigitter","text":""},{"location":"category/devops/","title":"devops","text":""},{"location":"category/csv/","title":"csv","text":""},{"location":"category/enrichment/","title":"enrichment","text":""},{"location":"category/prometheus/","title":"prometheus","text":""},{"location":"category/crewai/","title":"crewai","text":""},{"location":"category/traceloop/","title":"traceloop","text":""},{"location":"category/openllmetry/","title":"openllmetry","text":""},{"location":"category/weaver/","title":"weaver","text":""},{"location":"category/opencv/","title":"opencv","text":""},{"location":"category/yolo/","title":"yolo","text":""},{"location":"category/ultralytics/","title":"ultralytics","text":""},{"location":"category/json_exporter/","title":"json_exporter","text":""},{"location":"category/python/","title":"python","text":""},{"location":"category/flask/","title":"flask","text":""},{"location":"category/cncf/","title":"cncf","text":""},{"location":"category/oras/","title":"oras","text":""},{"location":"category/dockerhub/","title":"dockerhub","text":""},{"location":"category/logging/","title":"logging","text":""},{"location":"category/curl/","title":"curl","text":""},{"location":"category/fastapi/","title":"fastapi","text":""},{"location":"category/autoinstrumentation/","title":"autoinstrumentation","text":""},{"location":"category/redis/","title":"redis","text":""},{"location":"category/spans/","title":"spans","text":""},{"location":"category/traces/","title":"traces","text":""},{"location":"category/jaeger/","title":"jaeger","text":""},{"location":"category/span/","title":"span","text":""},{"location":"category/metadata/","title":"metadata","text":""},{"location":"category/instrumentation/","title":"instrumentation","text":""},{"location":"category/tutorial/","title":"tutorial","text":""},{"location":"category/tracepusher/","title":"tracepusher","text":""},{"location":"category/keptn/","title":"keptn","text":""},{"location":"category/metrics/","title":"metrics","text":""},{"location":"category/devto/","title":"dev.to","text":""},{"location":"category/helm/","title":"helm","text":""},{"location":"category/plugin/","title":"plugin","text":""},{"location":"category/development/","title":"development","text":""},{"location":"category/feature-flags/","title":"feature flags","text":""},{"location":"category/openfeature/","title":"openfeature","text":""},{"location":"category/kubecon/","title":"kubecon","text":""},{"location":"category/colocated-events/","title":"colocated events","text":""},{"location":"category/gitlab/","title":"gitlab","text":""},{"location":"category/fluent-bit/","title":"fluent bit","text":""},{"location":"category/fluentbit/","title":"fluentbit","text":""},{"location":"category/quick-tip/","title":"quick tip","text":""},{"location":"category/log-monitoring/","title":"log monitoring","text":""},{"location":"category/dive/","title":"dive","text":""},{"location":"category/containers/","title":"containers","text":""},{"location":"category/summary/","title":"summary","text":""},{"location":"category/continuous-delivery-foundation/","title":"continuous delivery foundation","text":""},{"location":"category/cdfoundation/","title":"cdfoundation","text":""},{"location":"category/pyrsia/","title":"pyrsia","text":""},{"location":"category/chatgpt/","title":"chatgpt","text":""},{"location":"category/nats/","title":"nats","text":""},{"location":"category/sql/","title":"sql","text":""},{"location":"category/sql-injection/","title":"sql injection","text":""},{"location":"category/security/","title":"security","text":""},{"location":"category/sre/","title":"sre","text":""},{"location":"category/service-level-providers/","title":"service level providers","text":""},{"location":"category/sli/","title":"sli","text":""},{"location":"category/slo/","title":"slo","text":""},{"location":"category/finops/","title":"finops","text":""},{"location":"category/infracost/","title":"infracost","text":""},{"location":"category/cloudevents/","title":"cloudevents","text":""},{"location":"category/aquasec-trivy/","title":"aquasec trivy","text":""},{"location":"category/quality-gates/","title":"quality gates","text":""},{"location":"category/dynatrace/","title":"dynatrace","text":""},{"location":"category/release-management/","title":"release management","text":""},{"location":"category/pipelines/","title":"pipelines","text":""},{"location":"category/open-source/","title":"open source","text":""},{"location":"category/webpage-scraper/","title":"webpage scraper","text":""},{"location":"category/webpage-parser/","title":"webpage parser","text":""},{"location":"category/linux/","title":"linux","text":""},{"location":"category/jq/","title":"jq","text":""},{"location":"category/pup/","title":"pup","text":""},{"location":"category/software-performance/","title":"software performance","text":""},{"location":"category/javascript/","title":"javascript","text":""},{"location":"category/serverless/","title":"serverless","text":""},{"location":"category/databaseless/","title":"databaseless","text":""},{"location":"category/voting/","title":"voting","text":""},{"location":"category/github-pages/","title":"github pages","text":""},{"location":"category/aws/","title":"aws","text":""},{"location":"category/lambda/","title":"lambda","text":""},{"location":"category/unleash/","title":"unleash","text":""},{"location":"category/feature-flag/","title":"feature flag","text":""},{"location":"category/self-healing/","title":"self healing","text":""},{"location":"category/automation/","title":"automation","text":""},{"location":"category/atlassian/","title":"atlassian","text":""},{"location":"category/bamboo/","title":"bamboo","text":""},{"location":"category/cicd/","title":"cicd","text":""},{"location":"category/shell-script/","title":"shell script","text":""},{"location":"category/osticket/","title":"osticket","text":""},{"location":"category/canary/","title":"canary","text":""},{"location":"category/istio/","title":"istio","text":""},{"location":"category/wordpress/","title":"wordpress","text":""},{"location":"category/architecture/","title":"architecture","text":""},{"location":"category/openssl/","title":"openssl","text":""},{"location":"category/web-performance/","title":"web performance","text":""},{"location":"category/github/","title":"github","text":""},{"location":"category/management-zones/","title":"management zones","text":""},{"location":"category/autonomous-monitoring/","title":"autonomous monitoring","text":""},{"location":"category/acm/","title":"acm","text":""},{"location":"category/applications/","title":"applications","text":""},{"location":"category/pitometer/","title":"pitometer","text":""},{"location":"category/pipeline/","title":"pipeline","text":""},{"location":"category/tagging/","title":"tagging","text":""},{"location":"category/service-naming/","title":"service naming","text":""},{"location":"category/process-group-naming/","title":"process group naming","text":""},{"location":"category/host-metadata/","title":"host metadata","text":""},{"location":"category/host-groups/","title":"host groups","text":""},{"location":"category/ansible/","title":"ansible","text":""},{"location":"category/json/","title":"json","text":""},{"location":"category/awx/","title":"awx","text":""},{"location":"category/centos/","title":"centos","text":""},{"location":"category/tutorials/","title":"tutorials","text":""},{"location":"category/vault/","title":"vault","text":""},{"location":"category/caching/","title":"caching","text":""},{"location":"category/geolocation/","title":"geolocation","text":""},{"location":"category/salt/","title":"salt","text":""},{"location":"category/saltstack/","title":"saltstack","text":""},{"location":"category/backup/","title":"backup","text":""},{"location":"category/restore/","title":"restore","text":""},{"location":"category/digitalocean/","title":"digitalocean","text":""},{"location":"category/oneagent/","title":"oneagent","text":""},{"location":"category/apache/","title":"apache","text":""},{"location":"category/reverse-proxy/","title":"reverse proxy","text":""},{"location":"category/synthetic/","title":"synthetic","text":""},{"location":"category/appmon/","title":"appmon","text":""},{"location":"category/web-dashboards/","title":"web dashboards","text":""},{"location":"category/service-level-agreement/","title":"service level agreement","text":""},{"location":"category/business-transactions/","title":"business transactions","text":""},{"location":"category/apache-solr/","title":"apache solr","text":""},{"location":"category/measures/","title":"measures","text":""},{"location":"category/batch-job/","title":"batch job","text":""},{"location":"category/monitoring/","title":"monitoring","text":""},{"location":"page/2/","title":"Home","text":""},{"location":"page/3/","title":"Home","text":""},{"location":"page/4/","title":"Home","text":""},{"location":"page/5/","title":"Home","text":""},{"location":"page/6/","title":"Home","text":""},{"location":"page/7/","title":"Home","text":""},{"location":"page/8/","title":"Home","text":""},{"location":"page/9/","title":"Home","text":""},{"location":"page/10/","title":"Home","text":""},{"location":"page/11/","title":"Home","text":""},{"location":"archive/2025/page/2/","title":"2025","text":""},{"location":"archive/2024/page/2/","title":"2024","text":""},{"location":"archive/2022/page/2/","title":"2022","text":""},{"location":"archive/2019/page/2/","title":"2019","text":""},{"location":"archive/2018/page/2/","title":"2018","text":""},{"location":"category/automation/page/2/","title":"automation","text":""},{"location":"category/dynatrace/page/2/","title":"dynatrace","text":""},{"location":"category/keptn/page/2/","title":"keptn","text":""},{"location":"category/opentelemetry/page/2/","title":"opentelemetry","text":""},{"location":"category/opentelemetry/page/3/","title":"opentelemetry","text":""}]}